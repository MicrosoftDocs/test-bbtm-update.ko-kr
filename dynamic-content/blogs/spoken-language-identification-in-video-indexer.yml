### YamlMime:Yaml
ms.openlocfilehash: bd8a083dc927f5b2497adfc40c1621058be0505d
ms.sourcegitcommit: d03bdc7fe5447adb6530886aab848b75fe8fa8ee
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 03/11/2022
ms.locfileid: "139900527"
Slug: spoken-language-identification-in-video-indexer
Title: Video Indexer의 음성 언어 식별
Summary: Video Indexer에 새로운 기능인 LID(음성 언어 식별)가 있다는 것을 공유하게 되어 기쁩니다!
Content: >-
  <p>Video Indexer에 새로운 기능인 LID(음성 언어 식별)가 있다는 것을 공유하게 되어 기쁩니다!</p>


  <p>고객의 일반적인 요청은 수동으로 언어를 제공하지 않고 비디오 또는 비디오 일괄 처리의 인덱싱을 사용하도록 설정하는 것이었습니다. 이는 일괄 업로드에 특히 중요합니다. 이를 지원하기 위해 Video Indexer에 자동 음성 언어 식별을 도입했습니다. 식별된 언어는 적절한 음성 텍스트 변환 모델을 호출하는 데 사용됩니다.</p>


  <p>LID는 오디오에 적용되는 최신 딥 Learning 기반으로 합니다. LID는 현재 영어, 중국어, 프랑스어, 독일어, 이탈리아어, 일본어, 스페인어, 러시아어 및 포르투갈어를 포함한 9개 언어를 지원합니다. 고품질에서 중간 수준의 녹음을 위해 높은 정확도로 작동합니다. 목록에 더 많은 언어를 추가하기 위해 노력하고 있으므로 계속 지켜봐 주시기 바랍니다.</p>


  <p>Video Indexer의 LID에 대해 자세히 알아보겠&rsquo;습니다.</p>


  <h2>Video Indexer에서 LID 사용</h2>


  <p>LID 기능을 사용하려면 두 가지 옵션이 있습니다. <a href="https://vi.microsoft.com/en-us/">포털을</a> 사용하는 경우 이제 비디오를 업로드할 때 언어 선택 콤보 상자에서 <strong>자동 검색</strong>을 선택할 수 있습니다.</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/87a063e4-1ea1-4578-8f41-ba3d9c0f90c8.png"><img alt="Fig2" border="0" height="449" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/73b4ce36-42d5-4a84-a87a-ae82b2540bfc.png" style="border: 0px currentcolor; border-image: none; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;" title="그림2" width="636"></a></p>


  <p><a href="https://api-portal.videoindexer.ai/docs/services/operations/operations/Upload-video?">API</a>를 사용하여 비디오를 업로드하는 경우 언어 매개 변수 값으로 <strong>자동</strong>을 사용합니다.</p>


  <p><strong>루트/</strong><strong>비디오/</strong><strong>인사이트</strong> 아래의 비디오 인덱스 JSON의 특성 sourceLanguage에는 검색된 언어 및 특성 sourceLanguageConfidence가 할당됩니다.</p>


  <p><strong><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/5f370558-49f3-418f-bd4b-ecf174445f5a.png"><img alt="Fig3" border="0" height="245" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/0eb1bdb6-853d-46b7-9103-dcc5927278da.png" style="border: 0px currentcolor; border-image: none; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;" title="그림 3" width="468"></a></strong></p>


  <p>또한 <a href="https://api-portal.videoindexer.ai/docs/services/operations/operations/get-video-artifact-download-url/">아티팩트 보조 API</a> 를 통한 자세한 응답( <strong>languageDetection</strong> 을 형식 매개 변수 값으로 설정)을 사용할 수 있습니다.</p>


  <p>아티팩트에서는 언어의 하위 집합 간에 신뢰도를 비교하거나 둘 이상의 가능성이 높은 언어를 식별하는 데 유용할 수 있는 언어별 신뢰도를 포함합니다. 또한 다국어 오디오를 실험하기 위해 검색된 언어 세그먼트를 제공합니다.</p>


  <h2>사용자에게 참고</h2>


  <ul>
   <li>LID 뒤에 있는 모델은 브로드캐스트 자료 및 팟캐스트, 강의, 자습서 등과 같은 엔터프라이즈 자료와 같은 명확한 녹음에서 가장 잘 작동합니다.</li>
   <li>이 모델은 시끄러운 녹음, 시끄러운 배경 음악, 수다 또는 에코가 있는 녹음, 낮은 품질의 녹음, 고도로 변형된 음향 조건 및 왜곡, 무거운 악센트로 인해 혼동될 수 있습니다.</li>
   <li>모델이 높은 신뢰도의 결과를 얻을 수 없는 경우 VI는 영어로 대체됩니다.</li>
  </ul>


  <h2>커튼 뒤</h2>


  <p>음성 언어 식별을 포함한 많은 인지 작업은 인간에게 쉽지만 컴퓨터에는 여전히 매우 어렵습니다. 이러한 유형의 작업에 접근하는 한 가지 방법은 인간의 뇌를 모방하는 것입니다. <strong>인공 신경망</strong>의 초기 아이디어는 <a href="https://en.wikipedia.org/wiki/Artificial_neuron">70여 년 전에 제안되었습니다</a>. 이 분야의 최첨단은 <a href="https://en.wikipedia.org/wiki/Deep_learning">딥 Learning(Deep Learning</a>)라고 불리며 음성 및 언어 이해, Computer Vision의 다양한 작업에 성공적으로 사용되고 있으며 <a href="https://www.nature.com/articles/nature21056.epdf?author_access_token=8oxIcYWf5UNrNpHsUHd2StRgN0jAjWel9jnR3ZoTv0NXpMHRAJy8Qn10ys2O4tuPakXos4UhQAFZ750CsBNMMsISFHIKinKDMKjShCpHIlYPYUHhNzkn6pSnOCt0Ftf6">피부암 진단</a>과 같은 일부 작업에서 인간을 능가하기도 합니다.</p>


  <p>VI에서는 음성 언어 식별을 위한 심층 Learning 기능을 활용합니다. 우리는 다양한 스피커에서 오는 다양한 음향 조건을 갖는 음성 예제의 거대한 숫자를 제시하여 네트워크를 학습.</p>


  <p>아래 그림은 네트워크에 대한 음성을 나타내는 방법을 보여줍니다. 이 표현은 음성을 <a href="https://en.wikipedia.org/wiki/Spectrogram">스펙트로그램</a>이라는 이미지로 바꿉니다. 스펙트로그램은 30초의 음성이 300,000픽셀이 쉽게 필요할 수 있으므로 음향이 얼마나 복잡한지 알 수 있습니다.</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/f4706a76-b38c-4a44-b5fc-4ee6aaac9367.png"><img alt="Fig1" border="0" height="311" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/f7b9401b-04f5-4199-991d-e84c31210aab.png" style="border: 0px currentcolor; border-image: none; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;" title="그림1" width="567"></a></p>


  <p><em>범례: 음성 샘플의 음성 시각화입니다. 위쪽: 녹음된 오디오의 <a href="https://en.wikipedia.org/wiki/Waveform">파형</a> 표현입니다. 아래쪽: 반사도 표현입니다.</em></p>


  <p>네트워크는 언어를 구분하는 스펙트로그램에서 패턴을 찾아 엄청난 양의 데이터에서 학습합니다. 좋은 패턴은 한 언어에 일반적인 패턴입니다. 딥 러닝은 이러한 패턴이 무엇인지에 대한 무작위 추측으로 시작합니다. 그런 다음 각 언어의 예제를 사용하면 올바른 방향으로 추측이 향상됩니다. 다음은 <a href="https://lisaloveslinguistics.wordpress.com/2011/03/12/flap-vs-trill-r-vs-%C9%BE/">스페인어의 롤링 &quot;R&quot; 이 스펙트로그램에서 어떻게 보이는지에 대한</a> 재미있는 예입니다.</p>


  <h2>결론</h2>


  <p>LID는 주요 언어를 알 수 없는 비디오를 인덱싱해야 하는 경우에 유용합니다. LID는 고급 심층 Learning 모델링을 기반으로 하며 고품질에서 중간 품질의 녹음에 가장 적합합니다. 더 많은 음향 환경을 포함하도록 LID를 확장하고 있습니다. <a href="https://vi.microsoft.com/">Video Indexer 웹 포털</a>에서 LID를 사용해 보세요. API에 대한 자세한 내용은 <a href="https://aka.ms/viapi">Video Indexer 개발자 포털</a>을 방문하세요. 우리는 LID와 함께 귀하의 경험을 듣고 기대하고 있습니다!</p>


  <p>질문이나 의견이 있으신가요? Microsoft는 여러분의 의견을 기다리고 있습니다! UserVoice를 사용하여 기능 또는 전자 메일 <a href="mailto:VISupport@Microsoft.com">VISupport@Microsoft.com</a>의 우선 순위를 지정합니다.</p>
