### YamlMime:Yaml
ms.openlocfilehash: 074e33c057e3a8b8ebc1189d5cf6c18e329924fb
ms.sourcegitcommit: d03bdc7fe5447adb6530886aab848b75fe8fa8ee
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 03/11/2022
ms.locfileid: "139913008"
Slug: creating-big-data-pipelines-using-azure-data-lake-and-azure-data-factory
Title: Azure Data Lake 및 Azure Data Factory를 사용하여 빅 데이터 파이프라인 만들기
Summary: 이번 주, Microsoft는 빅 데이터 처리 및 분석을 더 간단하고 접근성 있게 만드는 새롭고 확장된 Azure Data Lake의 공개 미리 보기를 발표했습니다.
Content: "<p>이번 주, Microsoft <a href=\"https://azure.microsoft.com/en-us/blog/azure-data-lake-reaches-public-preview/\">는</a> 빅 데이터 처리 및 분석을 더 간단하고 접근성 있게 만드는 새롭고 확장된 Azure Data Lake의 공개 미리 보기를 발표했습니다. 확장된 Azure Data Lake에는 Azure Data Lake Store, Azure Data Lake Analytics 및 Azure HDInsight가 포함됩니다.</p>\n\n<p>Azure Data Lake Store는 데이터 크기 조정으로 애플리케이션을 강제로 변경하지 않고도 크기, 유형 및 속도의 데이터를 쉽게 캡처할 수 있는 단일 리포지토리를 제공합니다. Azure Data Lake Analytics Apache YARN&nbsp;을 기반으로 하는 새로운 서비스이며 사용자 코드의 표현력과 SQL 이점을 통합하는 언어인 U-SQL 포함합니다. 이 서비스를 동적으로 확장하여 비즈니스 목표에 집중할 수 있도록 Azure Active Directory 통해 엔터프라이즈급 보안을 통해 모든 종류의 데이터에 대한 분석을 수행할 수 있습니다.</p>\n\n<p>10월 첫째 주에는 Azure HDInsight에 대한 기존 지원 외에도 Azure Data Lake 및 Azure Data Factory를 사용하여 빅 데이터 파이프라인(즉, 워크플로)을 만들고 운영할 수 있다고 <a href=\"https://azure.microsoft.com/en-us/blog/create-big-data-pipelines-using-azure-data-lake-store-analytics-azure-data-factory/\">발표</a> 했습니다. 오늘 새로 추가된 기능의 <strong>공개 미리 보기를</strong> 발표합니다. Azure Data Lake 및 Azure Data Factory 통합을 사용하면 다음을 수행할 수 있습니다.</p>\n\n<h2><font size=\"5\">데이터를 Azure Data Lake Store로 쉽게 이동</font></h2>\n\n<p>현재 Azure Data Factory는 다음 원본에서 Azure Data Lake Store로 데이터 이동을 지원합니다.</p>\n\n<ul>\n <li>Azure Blob</li>\n <li>Azure SQL Database</li>\n <li>Azure 테이블</li>\n <li>온-프레미스 SQL Server 데이터베이스</li>\n <li>Azure DocumentDB</li>\n <li>Azure SQL DW</li>\n <li>온-프레미스 파일 시스템</li>\n <li>온-프레미스 Oracle 데이터베이스</li>\n <li>온-프레미스 MYSQL 데이터베이스</li>\n <li>온-프레미스 DB2 데이터베이스</li>\n <li>온-프레미스 Teradata 데이터베이스</li>\n <li>온-프레미스 Sybase 데이터베이스</li>\n <li>온-프레미스 PostgreSQL 데이터베이스</li>\n <li>온-프레미스 HDFS</li>\n <li>일반 OData(출시 예정!)</li>\n <li>일반 ODBC(출시 예정!)</li>\n</ul>\n\n<p>Azure Data Lake Store에서 Azure Blob, Azure SQL Database, 온-프레미스 파일 시스템 등과 같은 여러 싱크로 데이터를 이동할 수도 있습니다. 아래 단계에 따라 Azure Blob Storage Azure Data Lake Store로 데이터를 이동합니다.</p>\n\n<p>참고: 아래 단계를 따르기 전에 유효한 Azure Data Lake Store 계정이 있어야 합니다. 계정이 없는&rsquo; 경우 <a href=\"https://azure.microsoft.com/en-us/documentation/articles/data-lake-store-get-started-portal/\">여기</a>를 클릭하여 새 계정을 만듭니다.</p>\n\n<h3>Azure Data Factory 만들기</h3>\n\n<p>Azure Portal에 로그인하고 Azure Data Factory로 이동합니다. 이름을 입력하고 구독, 리소스 그룹 이름 및 지역 이름을 선택합니다. 이름을&rsquo; AzureDataLakeStoreAnalyticsSample로 지정합니다.</p>\n\n<p><img alt=\"2015-10-25_22h22_06\" border=\"0\" height=\"869\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/fa29f0fd-5f04-492b-864f-d760326da94c.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-25_22h22_06\" width=\"1168\"></p>\n\n<p>만든 후 데이터 팩터리로 이동하고 작성기를 클릭하고 배포합니다.</p>\n\n<p><img alt=\"2015-10-25_22h37_40\" border=\"0\" height=\"663\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/9f970234-e70b-4336-af90-f8d797ec2315.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-25_22h37_40\" width=\"581\">&nbsp;</p>\n\n<h3>ADF 연결된 서비스 만들기</h3>\n\n<p>Azure Storage 연결된 서비스 만들기: 데이터를 이동하려는 Azure Blob Storage(원본)입니다.</p>\n\n<p>새 데이터 저장소 &ndash;&gt; Azure Storage 클릭합니다. AccountName&gt; 및 AccountKey&gt; 매개 변수의 &lt;값을 입력하고 &lt;배포를 누릅니다.</p>\n\n<p><img alt=\"2015-10-27_14h55_19\" border=\"0\" height=\"434\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/aba7e2d0-e2ed-45b8-88fb-39c069c9422b.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-27_14h55_19\" width=\"316\"></p>\n\n<p>Azure Data Lake Store 연결된 서비스 만들기: 데이터를 이동하려는 Azure Data Lake Storage(싱크 대상)입니다.</p>\n\n<p>새 데이터 저장소 -&gt; Azure Data Lake Store를 클릭합니다.</p>\n\n<p><img alt=\"2015-10-25_23h11_00\" border=\"0\" height=\"439\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/25ec0c3b-5f9b-42ae-b701-ea11b04d5461.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-25_23h11_00\" width=\"319\"></p>\n\n<p>Azure Data Lake Store 연결된 서비스에 대한 필수 매개 변수 입력</p>\n\n<p>DataLakeUri: 위 단계 또는 기존 항목을 사용하여 만들었습니다. 예를 들어 <a href=\"https://&lt;adlstoreaccountname&gt;.azuredatalakestore.net/webhdfs/v1\">https://&lt; adlstoreaccountname.azuredatalakestore.net/webhdfs/v1&gt;</a>. adlstoreaccountname을&gt; ADL Store 계정 이름으로 바꿉 &lt;있습니다.</p>\n\n<p>권한 부여: 이 매개 변수를 채우려면 권한 부여를 클릭합니다. 그러면 팝업이 열리고 자격 증명을 입력해야 합니다.</p>\n\n<p><img alt=\"2015-10-25_23h26_03\" border=\"0\" height=\"387\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/fedcaffb-9040-402a-aaef-e7abe79d8979.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-25_23h26_03\" width=\"380\"></p>\n\n<p>Azure Data Lake Store 계정이 다른 구독에 있고 데이터 팩터리와 다른 리소스 그룹 이름 아래에 있는 경우 다음 매개 변수도 입력해야 합니다.</p>\n\n<ul>\n <li>AccountName</li>\n <li>구독 ID</li>\n <li>ResourceGroupName</li>\n</ul>\n\n<p>배포를 클릭합니다. 이렇게 하면 Azure Data Lake Store 연결된 서비스가 만들어져야 합니다.</p>\n\n<p>참고: 배포를 누르기 전에 값을 지정하지 않는 경우 Json에서 선택 사항이라고 말하는 행을 삭제해야 합니다.</p>\n\n<h3>ADF 데이터 세트 만들기</h3>\n\n<p>Azure Blob Storage 원본 데이터 세트를 만듭니다.</p>\n\n<p>새 데이터 세트 &ndash;&gt; Azure Blob Storage를 클릭합니다.</p>\n\n<p><img alt=\"2015-10-26_07h58_55\" border=\"0\" height=\"437\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/310438c0-30bd-44c7-8aac-630836f2a0cc.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-26_07h58_55\" width=\"315\"></p>\n\n<p>이렇게 하면 모든 값을 채울 수 있는 Azure Blob Storage 데이터 세트에 대한 템플릿이 표시됩니다. 아래의 Azure Blob Storage 데이터 세트를 예로 확인하세요. 간단히 하기 위해 시간 기반 파티션에 대해 분할된 by 절을 사용하고 정적 폴더를 사용하지 않습니다. 아래 데이터 세트는 복사되는 데이터(SearchLog.tsv)가 azure Storage의 rawdatasample/data/folder에 있음을 지정합니다.</p>\n\n<pre class=\"prettyprint\">\n{\n    &quot;name&quot;: &quot;RawBlobDemoTable&quot;,\n    &quot;properties&quot;: {\n        &quot;published&quot;: false,\n        &quot;type&quot;: &quot;AzureBlob&quot;,\n        &quot;linkedServiceName&quot;: &quot;StorageLinkedService&quot;,\n        &quot;typeProperties&quot;: {\n            &quot;fileName&quot;: &quot;SearchLog.tsv&quot;,\n            &quot;folderPath&quot;: &quot;rawdatasample/data/&quot;,\n            &quot;format&quot;: {\n                &quot;type&quot;: &quot;TextFormat&quot;,\n                &quot;rowDelimiter&quot;: &quot;\\n&quot;,\n                &quot;columnDelimiter&quot;: &quot;\\t&quot;\n            }\n        },\n        &quot;availability&quot;: {\n            &quot;frequency&quot;: &quot;Day&quot;,\n            &quot;interval&quot;: 1,\n            &quot;style&quot;: &quot;StartOfInterval&quot;\n        },\n        &quot;external&quot;: true,\n        &quot;policy&quot;: {\n            &quot;validation&quot;: {\n                &quot;minimumSizeMB&quot;: 0.00001\n            }\n        }\n    }\n}</pre>\n\n<p>Azure Data Lake Store 대상 데이터 세트를 만듭니다.</p>\n\n<p>새 데이터 세트 -&gt; Azure Data Lake Store를 클릭합니다.</p>\n\n<p><img alt=\"2015-10-26_08h18_14\" border=\"0\" height=\"439\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/d2c509e0-440c-42a5-a3fa-b8993e9da751.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-26_08h18_14\" width=\"319\"></p>\n\n<p>그러면 모든 값을 채울 수 있는 Azure Data Lake Store 데이터 세트에 대한 템플릿이 제공됩니다. 예를 들어 아래 Azure Data Lake Store 데이터 세트를 참조하세요. 간단히 하기 위해 시간 기반 파티션에 대해 분할된 by 절을 사용하고 정적 폴더를 사용하지 않습니다. 아래 데이터 세트는 데이터 레이크의 datalake/input/폴더에 복사되는 데이터를 지정합니다.</p>\n\n<pre class=\"prettyprint\">\n{\n    &quot;name&quot;: &quot;DataLakeTable&quot;,\n    &quot;properties&quot;: {\n        &quot;published&quot;: false,\n        &quot;type&quot;: &quot;AzureDataLakeStore&quot;,\n        &quot;linkedServiceName&quot;: &quot;AzureDataLakeStoreLinkedService&quot;,\n        &quot;typeProperties&quot;: {\n            &quot;folderPath&quot;: &quot;datalake/input/&quot;,\n            &quot;fileName&quot;: &quot;SearchLog.tsv&quot;,\n            &quot;format&quot;: {\n                &quot;type&quot;: &quot;TextFormat&quot;,\n                &quot;rowDelimiter&quot;: &quot;\\n&quot;,\n                &quot;columnDelimiter&quot;: &quot;\\t&quot;\n            }\n        },\n        &quot;availability&quot;: {\n            &quot;frequency&quot;: &quot;Day&quot;,\n            &quot;interval&quot;: 1\n        }\n    }\n}</pre>\n\n<h3>ADF Pipelines 만들기</h3>\n\n<p>ADF 복사 파이프라인 만들기: 이 파이프라인은 Azure Blob Storage Azure Data Lake로 데이터를 복사합니다.</p>\n\n<p>새 파이프라인을 클릭하면 샘플 파이프라인 템플릿이 열립니다. 예를 들어 아래 파이프라인은 Azure Blob Storage에서 Azure Data Lake로 데이터를 복사합니다(위에서 만든 샘플 데이터 세트).</p>\n\n<p><u>파이프라인 정의</u>:</p>\n\n<pre class=\"prettyprint\">\n{\n    &quot;name&quot;: &quot;EgressBlobToDataLakePipeline&quot;,\n    &quot;properties&quot;: {\n        &quot;description&quot;: &quot;Egress data from blob to azure data lake&quot;,\n        &quot;activities&quot;: [\n            {\n                &quot;type&quot;: &quot;Copy&quot;,\n                &quot;typeProperties&quot;: {\n                    &quot;source&quot;: {\n                        &quot;type&quot;: &quot;BlobSource&quot;,\n                        &quot;treatEmptyAsNull&quot;: true\n                    },\n                    &quot;sink&quot;: {\n                        &quot;type&quot;: &quot;AzureDataLakeStoreSink&quot;,\n                        &quot;writeBatchSize&quot;: 0,\n                        &quot;writeBatchTimeout&quot;: &quot;00:00:00&quot;\n                    }\n                },\n                &quot;inputs&quot;: [\n                    {\n                        &quot;name&quot;: &quot;RawBlobDemoTable&quot;\n                    }\n                ],\n                &quot;outputs&quot;: [\n                    {\n                        &quot;name&quot;: &quot;DataLakeTable&quot;\n                    }\n                ],\n                &quot;policy&quot;: {\n                    &quot;timeout&quot;: &quot;10:00:00&quot;,\n                    &quot;concurrency&quot;: 1,\n                    &quot;executionPriorityOrder&quot;: &quot;NewestFirst&quot;,\n                    &quot;retry&quot;: 1\n                },\n                &quot;scheduler&quot;: {\n                    &quot;frequency&quot;: &quot;Day&quot;,\n                    &quot;interval&quot;: 1\n                },\n                &quot;name&quot;: &quot;EgressDataLake&quot;,\n                &quot;description&quot;: &quot;Move data from blob to azure data lake&quot;\n            }\n        ],\n        &quot;start&quot;: &quot;2015-08-08T00:00:00Z&quot;,\n        &quot;end&quot;: &quot;2015-08-08T01:00:00Z&quot;,\n        &quot;isPaused&quot;: false\n    }\n}</pre>\n\n<h3>ADF Pipelines 모니터링</h3>\n\n<p>위에서 만든 ADF 복사 파이프라인은 데이터 세트에 일일 빈도가 있고 파이프라인 정의의 시작이 2015년 8월 8일로 설정되므로 실행이 시작됩니다. 따라서 파이프라인은 해당 날짜에만 실행되고 복사 작업을 한 번만 수행합니다. ADF 파이프라인 예약에 대해 자세히 알아보려면 <a href=\"https://azure.microsoft.com/en-us/documentation/articles/data-factory-scheduling-and-execution/\">여기</a> 를 클릭하세요.</p>\n\n<p>ADF 다이어그램 뷰로 이동하여 데이터 팩터리의 운영 계보를 확인합니다. Blob Storage에서 Azure Data Lake Store로 데이터를 이동하기 위한 파이프라인과 함께 Azure Blob Storage 및 Azure Data Lake Store 데이터 세트를 볼 수 있습니다.</p>\n\n<p><img alt=\"2015-10-27_15h04_32\" border=\"0\" height=\"355\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/e81c7887-4b4b-4b7b-834d-555cd85bf012.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-27_15h04_32\" width=\"763\"></p>\n\n<p>다이어그램 보기에서 DataLakeTable을 클릭하여 해당 활동 실행 및 상태를 확인합니다.</p>\n\n<p><img alt=\"2015-10-27_14h51_37\" border=\"0\" height=\"449\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/6b68c345-8a97-4904-8a7e-0accf804243a.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-27_14h51_37\" width=\"586\"></p>\n\n<p>ADF의 EgressBlobToDataLakePipeline(위의 스크린샷 참조)의 복사 작업이 Azure Blob Storage Azure Data Lake Store로 3.08KB 데이터를 성공적으로 실행하고 복사한 것을 볼 수 있습니다. Microsoft Azure 포털에 로그인하고 Azure Data Lake Data Explorer를 사용하여 Azure Data Lake Store에 복사된 데이터를 시각화할 수도 있습니다.</p>\n\n<p><img alt=\"2015-10-27_14h58_37\" border=\"0\" height=\"264\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/4057daff-7511-4076-a66e-75b4be12f232.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-27_14h58_37\" width=\"729\">&nbsp;</p>\n\n<p>Azure Data Factory 데이터 이동 활동에 대해 자세히 알아보려면 <a href=\"https://azure.microsoft.com/en-us/documentation/articles/data-factory-data-movement-activities/\">여기</a> 를 클릭하세요. ADF에서 AzureDataLakeStore 커넥터를 사용하는 방법에 대한 자세한 설명서는 여기에서 찾을 수 <a href=\"https://azure.microsoft.com/en-us/documentation/articles/data-factory-azure-datalake-connector/\">있습니다</a>.</p>\n\n<h2><font size=\"5\">Azure Data Lake Analytics 서비스의 처리 단계로 U-SQL 스크립트를 실행하는 E2E 빅 데이터 ADF 파이프라인 만들기</font></h2>\n\n<p>여러 산업 분야(소매, 금융, 게임)에 대한 매우 일반적인 사용 사례는 로그 처리입니다.</p>\n\n<p>참고: 아래 단계를 따르기 전에 유효한 Azure Data Lake Analytics 계정이 있어야 합니다. 계정이 없는&rsquo; 경우 <a href=\"https://azure.microsoft.com/en-us/documentation/articles/data-lake-analytics-get-started-portal/\">여기</a>를 클릭하여 새 계정을 만듭니다.</p>\n\n<p>이 시나리오에서는 이전 단계에서 Azure Data Lake Store 계정에 복사된 로그를 사용하고 처리 단계 중 하나로 Azure Data Lake Analytics U-SQL 스크립트를 실행하여 로그를 처리하는 ADF 파이프라인을 만듭니다. U-SQL 스크립트는 다운스트림 프로세스에서 사용할 수 있는 지역별 이벤트를 계산합니다.</p>\n\n<p>위의 시나리오에서 만든 데이터 팩터리(AzureDataLakeStoreAnalyticsSample)를 다시 사용하여 Azure Blob Storage Azure Data Lake Store로 데이터를 복사합니다.</p>\n\n<h3>ADF 연결된 서비스 만들기</h3>\n\n<p>Azure Data Lake Analytics 연결된 서비스를 만듭니다. 로그 처리를 수행하는 U-SQL 스크립트를 실행하는 Azure Data Lake Analytics 계정입니다.</p>\n\n<p>새 컴퓨팅 &ndash;&gt; Azure Data Lake Analytics 클릭합니다.</p>\n\n<p><img alt=\"2015-10-26_14h55_59\" border=\"0\" height=\"307\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/e2f7acc0-101e-4feb-9664-4b46a06d15ce.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-26_14h55_59\" width=\"315\">&nbsp;</p>\n\n<p>Azure Data Lake Analytics 연결된 서비스에 대한 필수 매개 변수 입력</p>\n\n<ul>\n <li>AccountName: 위 단계 또는 기존 계정을 사용하여 만든 계정</li>\n <li>권한 부여: 이 매개 변수를 채우려면 권한 부여를 클릭합니다. 그러면 팝업이 열리고 자격 증명을 입력해야&#39;.</li>\n</ul>\n\n<p><img alt=\"2015-10-26_15h01_38\" border=\"0\" height=\"323\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/bc515b52-e29d-4054-9f49-cdcd0c7cda3c.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-26_15h01_38\" width=\"430\">&nbsp;</p>\n\n<p>Azure Data Lake Analytics 계정이 다른 구독에 있고 다른 리소스 그룹 이름 아래에 있는 경우 선택적 매개 변수를 입력합니다.</p>\n\n<ul>\n <li>구독 ID</li>\n <li>ResourceGroupName</li>\n</ul>\n\n<p>배포를 클릭합니다. 이렇게 하면 Azure Data Lake Analytics 연결된 서비스가 만들어져야 합니다.</p>\n\n<p>참고: 배포를 누르기 전에 값을 지정하지 않는 경우 JSON에서 선택 사항이라는 행을 삭제해야 합니다.</p>\n\n<p>Azure Data Lake Store 연결된 서비스 만들기: 데이터를 이동하려는 Azure Data Lake Storage(싱크 대상)입니다.</p>\n\n<p>참고: 위의 복사 시나리오를 계속 진행하는 경우 이 연결된 서비스를 이미 만들었을 것입니다.</p>\n\n<p>새 데이터 저장소 -&gt; Azure Data Lake Store를 클릭합니다.</p>\n\n<p><img alt=\"2015-10-25_23h11_00\" border=\"0\" height=\"439\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/985e7079-5f07-478e-b596-03e9e95b6b7f.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-25_23h11_00\" width=\"319\"></p>\n\n<p>Azure Data Lake Store 연결된 서비스에 대한 필수 매개 변수 입력</p>\n\n<p>DataLakeUri: 위 단계 또는 기존 항목을 사용하여 만든 예제: <a href=\"https://&lt;adlstoreaccountname&gt;.azuredatalakestore.net/webhdfs/v1\">https://&lt; adlstoreaccountname.azuredatalakestore.net/webhdfs/v1&gt;</a>. adlstoreaccountname을&gt; ADL Store 계정 이름으로 바꿉 &lt;있습니다.</p>\n\n<p>권한 부여: 이 매개 변수를 채우려면 권한 부여를 클릭합니다. 그러면 팝업이 열리고 자격 증명을 입력해야 합니다.</p>\n\n<p><img alt=\"2015-10-25_23h26_03\" border=\"0\" height=\"387\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/c8fb55ad-7d83-46ed-b4e7-077015a70275.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-25_23h26_03\" width=\"380\"></p>\n\n<p>Azure Data Lake Store 계정이 다른 구독에 있고 데이터 팩터리와 다른 리소스 그룹 이름 아래에 있는 경우 다음 매개 변수도 입력해야 합니다.</p>\n\n<ul>\n <li>AccountName</li>\n <li>구독 ID</li>\n <li>ResourceGroupName</li>\n</ul>\n\n<p>배포를 클릭합니다. 이렇게 하면 Azure Data Lake Store 연결된 서비스가 만들어져야 합니다.</p>\n\n<p>참고: 배포를 누르기 전에 값을 지정하지 않는 경우 JSON에서 선택 사항이라는 행을 삭제해야 합니다.</p>\n\n<h3>ADF 데이터 세트 만들기</h3>\n\n<p>Azure Data Lake Store 원본 데이터 세트를 만듭니다.</p>\n\n<p>참고: 위의 복사 시나리오를 계속 진행하면서 이 시나리오를 수행하는 경우 이 데이터 세트를 이미 만들었을 것입니다.</p>\n\n<p>새 데이터 세트 -&gt; Azure Data Lake Store를 클릭합니다.</p>\n\n<p><img alt=\"2015-10-26_08h18_14\" border=\"0\" height=\"439\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/cac7b58f-f532-4084-883a-131bc284032b.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-26_08h18_14\" width=\"319\"></p>\n\n<p>그러면 Azure Data Lake Store 데이터 세트에 대한 템플릿이 제공됩니다. 모든 값을 입력할 수 있습니다.</p>\n\n<p>예를 들어 아래 Azure Data Lake Store 데이터 세트를 살펴보세요. 간단히 하기 위해 시간 기반 파티션에 분할된 절을 사용하고 정적 폴더를 사용하지 않습니다. 아래 데이터 세트는 데이터 레이크의 datalake/input/폴더에 복사되는 데이터를 지정합니다.</p>\n\n<pre class=\"prettyprint\">\n{\n    &quot;name&quot;: &quot;DataLakeTable&quot;,\n    &quot;properties&quot;: {\n        &quot;published&quot;: false,\n        &quot;type&quot;: &quot;AzureDataLakeStore&quot;,\n        &quot;linkedServiceName&quot;: &quot;AzureDataLakeStoreLinkedService&quot;,\n        &quot;typeProperties&quot;: {\n            &quot;folderPath&quot;: &quot;datalake/input/&quot;,\n            &quot;fileName&quot;: &quot;SearchLog.tsv&quot;,\n            &quot;format&quot;: {\n                &quot;type&quot;: &quot;TextFormat&quot;,\n                &quot;rowDelimiter&quot;: &quot;\\n&quot;,\n                &quot;columnDelimiter&quot;: &quot;\\t&quot;\n            }\n        },\n        &quot;availability&quot;: {\n            &quot;frequency&quot;: &quot;Day&quot;,\n            &quot;interval&quot;: 1\n        }\n    }\n}</pre>\n\n<p>Azure Data Lake Storedestination&rsquo; 데이터 세트를 만듭니다.</p>\n\n<p>새 데이터 세트 -&gt; Azure Data Lake Store를 클릭합니다.</p>\n\n<p>예를 들어 아래의 EventsByEnGbRegionTable 데이터 세트 정의를 참조하세요. 이 데이터 세트에 해당하는 데이터는 En-gb&rsquo; 로캘 및 날짜 &ldquo;&lt; 2012/02/19&rdquo;에 대한 모든 이벤트를 가져오기 위해 &lsquo;AzureDataLakeAnalytics U-SQL 스크립트를 실행한 후에 생성됩니다.</p>\n\n<pre class=\"prettyprint\">\n{\n    &quot;name&quot;: &quot;EventsByEnGbRegionTable&quot;,\n    &quot;properties&quot;: {\n        &quot;published&quot;: false,\n        &quot;type&quot;: &quot;AzureDataLakeStore&quot;,\n        &quot;linkedServiceName&quot;: &quot;AzureDataLakeStoreLinkedService&quot;,\n        &quot;typeProperties&quot;: {\n            &quot;folderPath&quot;: &quot;datalake/output/&quot;\n        },\n        &quot;availability&quot;: {\n            &quot;frequency&quot;: &quot;Day&quot;,\n            &quot;interval&quot;: 1\n        }\n    }\n}</pre>\n\n<h3>ADF Pipelines 만들기</h3>\n\n<p>ADF AzureDataLakeAnalytics 파이프라인 만들기: 이 파이프라인은 처리를 수행할 U-SQL 작업을 실행합니다.</p>\n\n<p>새 파이프라인을 클릭하면 샘플 파이프라인 템플릿이 열립니다.</p>\n\n<p><img alt=\"2015-10-26_17h24_21\" border=\"0\" height=\"501\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/d99477d6-179b-49a3-97db-c13c9d2f80d7.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-26_17h24_21\" width=\"663\"></p>\n\n<p>새 파이프라인을 클릭한 후 활동 추가를 클릭하고 DataLakeAnalyticsU-SQL 작업에 대한 템플릿을 추가할 수도 있습니다.</p>\n\n<p><img alt=\"2015-10-26_17h26_34\" border=\"0\" height=\"500\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/212f31b3-205e-4ad4-96ce-3d9ae0677b90.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-26_17h26_34\" width=\"598\"></p>\n\n<p>예를 들어 아래 파이프라인은 ADLA U-SQL 작업을 실행하여 en-gb&rsquo; 로캘 및 날짜 &ldquo;&lt; 2012/02/19&rdquo;에 대한 &lsquo;모든 이벤트를 가져옵니다.</p>\n\n<p><u>파이프라인 정의</u>:</p>\n\n<pre class=\"prettyprint\">\n{\n    &quot;name&quot;: &quot;ComputeEventsByEnGbRegionPipeline&quot;,\n    &quot;properties&quot;: {\n        &quot;description&quot;: &quot;This is a pipeline to compute events for en-gb locale and date less than 2012/02/19.&quot;,\n        &quot;activities&quot;: [\n            {\n                &quot;type&quot;: &quot;DataLakeAnalyticsU-SQL&quot;,\n                &quot;typeProperties&quot;: {\n                    &quot;scriptPath&quot;: &quot;scripts\\\\kona\\\\SearchLogProcessing.txt&quot;,\n                    &quot;scriptLinkedService&quot;: &quot;StorageLinkedService&quot;,\n                    &quot;degreeOfParallelism&quot;: 3,\n                    &quot;priority&quot;: 100,\n                    &quot;parameters&quot;: {\n                        &quot;in&quot;: &quot;/datalake/input/SearchLog.tsv&quot;,\n                        &quot;out&quot;: &quot;/datalake/output/Result.tsv&quot;\n                    }\n                },\n                &quot;inputs&quot;: [\n                    {\n                        &quot;name&quot;: &quot;DataLakeTable&quot;\n                    }\n                ],\n                &quot;outputs&quot;: [\n                    {\n                        &quot;name&quot;: &quot;EventsByEnGbRegionTable&quot;\n                    }\n                ],\n                &quot;policy&quot;: {\n                    &quot;timeout&quot;: &quot;06:00:00&quot;,\n                    &quot;concurrency&quot;: 1,\n                    &quot;executionPriorityOrder&quot;: &quot;NewestFirst&quot;,\n                    &quot;retry&quot;: 1\n                },\n                &quot;scheduler&quot;: {\n                    &quot;frequency&quot;: &quot;Day&quot;,\n                    &quot;interval&quot;: 1\n                },\n                &quot;name&quot;: &quot;EventsByRegion&quot;,\n                &quot;linkedServiceName&quot;: &quot;AzureDataLakeAnalyticsLinkedService&quot;\n            }\n        ],\n        &quot;start&quot;: &quot;2015-08-08T00:00:00Z&quot;,\n        &quot;end&quot;: &quot;2015-08-08T01:00:00Z&quot;,\n        &quot;isPaused&quot;: false\n    }\n}</pre>\n\n<p>위의 파이프라인에서 실행되는 U-SQL 스크립트는 배포된 StorageLinkedService에 &lsquo;해당하는 Azure Blob Storage 계정의 스크립트/kona&rsquo; 폴더에 있습니다.</p>\n\n<p><u>SearchLogProcessing.txt 스크립트 정의:</u></p>\n\n<pre class=\"prettyprint\">\n@searchlog =\n    EXTRACT UserId          int,\n            Start           DateTime,\n            Region          string,\n            Query           string,\n            Duration        int?,\n            Urls            string,\n            ClickedUrls     string\n    FROM @in\n    USING Extractors.Tsv(nullEscape:&quot;#NULL#&quot;);\n\n@rs1 =\n    SELECT Start, Region, Duration\n    FROM @searchlog\nWHERE Region == &quot;en-gb&quot;;\n\n@rs1 =\n    SELECT Start, Region, Duration\n    FROM @rs1\n    WHERE Start &lt;= DateTime.Parse(&quot;2012/02/19&quot;);\n\nOUTPUT @rs1   \n    TO @out\n      USING Outputters.Tsv(quoting:false, dateTimeFormat:null);\n</pre>\n\n<p>위의 U-SQL 스크립트의 값 @in 과 @out 매개 변수는 매개 변수 섹션을 사용하여 ADF에 의해 동적으로 전달됩니다. 파이프라인 정의에서 위의 매개 변수 섹션을 참조하세요.</p>\n\n<p>다른 속성 viz. degreeOfParallelism, 우선 순위 등은 물론 Azure Data Lake Analytics 서비스에서 실행되는 작업에 대한 파이프라인 정의에서도 지정할 수 있습니다.</p>\n\n<h3>ADF Pipelines 모니터링</h3>\n\n<p>위의 ADF 복사 파이프라인은 데이터 세트가 일별 빈도를 가지며, 파이프라인 정의의 끝은 2015년 8월 8일로 설정되므로 실행이 시작됩니다. 따라서 파이프라인은 해당 날짜에만 실행되고 U-SQL 스크립트를 한 번만 실행합니다. ADF 파이프라인 예약에 대해 자세히 알아보려면 <a href=\"https://azure.microsoft.com/en-us/documentation/articles/data-factory-scheduling-and-execution/\">여기</a> 를 클릭하십시오.</p>\n\n<p>ADF 다이어그램 보기로 이동하여 데이터 팩터리의 작동 계보를 확인합니다. 두 개의 파이프라인과 해당 데이터 세트 viz가 표시됩니다. EgressBlobToDataLakePipeline(Azure Blob Storage에서 Azure Data Lake Store로 데이터 복사) 및 ComputeEventsByEnGbRegionPipeline(en-gb&rsquo; 로캘 및 날짜 &ldquo;&lt; 2012/02/19&rdquo;에 대한 &lsquo;모든 이벤트 가져오기).</p>\n\n<p><img alt=\"2015-10-27_14h50_04\" border=\"0\" height=\"525\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/611bb838-06be-4ab9-9286-35769f5faab6.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-27_14h50_04\" width=\"1250\"></p>\n\n<p>다이어그램 보기에서 EventsByEnGbRegionTable을 클릭하여 해당 활동 실행 및 해당 상태를 확인합니다.</p>\n\n<p><img alt=\"2015-10-27_15h01_09\" border=\"0\" height=\"614\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/ff7293a5-7d2e-4342-a390-5bec17e16ec9.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-27_15h01_09\" width=\"583\"></p>\n\n<p>ADF의 ComputeEventsByEnGbRegionPipeline에서 U-SQL 작업이 성공적으로 실행되고 AzureDataLakeStore 계정에 Results.tsv 파일(/datalake/output/Result.tsv)이 생성되었음을 확인할 수 있습니다. Result.tsv에는 en-gb&rsquo; 로캘 및 날짜 &ldquo;&lt; 2012/02/19&rdquo;에 대한 &lsquo;모든 이벤트가 포함됩니다. Microsoft Azure 포털에 로그인하고 Azure Data Lake Data Explorer를 사용하여 Azure Data Lake Store에서 위의 처리 단계의 일부로 생성된 Result.tsv 파일을 시각화할 수 있습니다.</p>\n\n<p><a href=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/f1fdee9c-20e2-461e-9d47-dc9a0fc259c8.png\"><img alt=\"2015-10-27_15h02_42\" border=\"0\" height=\"272\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/0531d6b8-5a56-409b-97a6-2b529cea2926.png\" style=\"border-width: 0px; padding-top: 0px; padding-right: 0px; padding-left: 0px; display: inline; background-image: none;\" title=\"2015-10-27_15h02_42\" width=\"754\"></a>&nbsp;</p>\n\n<p>Azure Data Factory에서 AzureDataLakeAnalyticsU-SQL 활동에&nbsp; 대한 자세한 설명서를 <a href=\"https://azure.microsoft.com/en-us/documentation/articles/data-factory-usql-activity/\">찾을 수 있습니다</a>.</p>\n\n<p>요약하자면, 위의 단계에 따라 Azure Data Factory를 사용하여 데이터를 Azure Data Lake Store로 이동할 수 있는 E2E 빅 데이터 파이프라인을 빌드할 수 있었습니다. 또한 Azure Data Lake Analytics U-SQL 스크립트를 처리 단계 중 하나로 실행하고 필요에 따라 동적으로 확장할 수 있었습니다.</p>\n\n<p>빅 데이터 처리 및 분석 워크플로를 운영할 수 있는 솔루션에 계속 투자할 것입니다. Microsoft 클라우드 플랫폼 팀의 Microsoft Azure Data Lake에 대해 자세히 알아보려면 <a href=\"https://azure.microsoft.com/en-us/solutions/data-lake/\">여기</a>를 클릭하세요. Azure Data Factory를 사용해 보려면 <a href=\"https://azure.microsoft.com/en-us/documentation/services/data-factory/\">여기</a> 를 방문하여 데이터 팩터리를 사용하여 파이프라인을 쉽고 빠르게 빌드하여 시작하세요. 기능 요청이 있거나 데이터 팩터리에 대한 피드백을 제공하려는 경우 <a href=\"https://feedback.azure.com/forums/270578-azure-data-factory\">Azure Data Factory 포럼</a>을 방문하세요.</p>"
