### YamlMime:Yaml
ms.openlocfilehash: 925b8ff6fd3c1af5f966d171fb18451fefaa59f7
ms.sourcegitcommit: d03bdc7fe5447adb6530886aab848b75fe8fa8ee
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 03/11/2022
ms.locfileid: "139908175"
Slug: processing-trillions-of-events-per-day-with-apache-kafka-on-azure
Title: Azure에서 Apache Kafka를 사용하여 매일 엄청난 수의 이벤트 처리
Summary: 'n 현재 시대에 기업은 초당 엄청난 양의 데이터를 생성합니다. 비즈니스 인텔리전스, 사용자 분석 또는 운영 인텔리전스를 위한 것인지 여부 스트리밍 데이터를 수집하고 분석하려면 이 데이터를 원본에서 관심 있는 여러 소비자로 이동해야 합니다. '
Content: >-
  <p><em>이 블로그는 Nitin Kumar, Siphon 팀, AI Platform의 감독 하에 벤치마크, 최적화 및 성능 튜닝 실험을 공동으로 수행한 Noor Abani 및 Negin Raoof 소프트웨어 엔지니어가 공동 저술했습니다. </em></p>


  <p><em>HDInsight 팀의 드루브 골과 우마 마헤스와리 안바자간에게 진심으로 감사드립니다.</em></p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/1266d43c-ccdb-43d9-a747-d3fe550d3d2e.png"><img alt="image" border="0" height="420" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/d63561bf-7dc7-4317-bc17-c7f0c310e8bc.png" style="border: 0px currentcolor; border-image: none; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;" title="이미지" width="746"></a></p>


  <p>&nbsp;</p>


  <p align="center"><em>그림 1: 다양한 시나리오에 대한 생산자 처리량입니다. 10개 broker Kafka 클러스터에서 2GBps를 달성했습니다. </em></p>


  <p>현재 시대에 기업은 초당 엄청난 양의 데이터를 생성합니다. 비즈니스 인텔리전스, 사용자 분석 또는 운영 인텔리전스를 위한 것인지 여부 스트리밍 데이터를 수집하고 분석하려면 이 데이터를 원본에서 관심 있는 여러 소비자로 이동해야 합니다. <a href="https://kafka.apache.org/" target="_blank">Apache Kafka</a> 는 확장성이 뛰어나고 안정적이며 빠른 데이터 수집 및 스트리밍 도구 역할을 하는 분산 복제 메시징 서비스 플랫폼입니다. Microsoft에서는 Apache Kafka를 근 실시간 데이터 전송 서비스의 주요 구성 요소로 사용하여 초당 최대 3,000만 개의 이벤트를 처리합니다.</p>


  <p>이 게시물에서는 세계에서&rsquo; 가장 큰 Kafka 배포 중 하나를 실행한 경험과 학습을 공유합니다. 기본 인프라 고려 사항 외에도 메시지 처리량, 대기 시간 및 내구성에 영향을 주는 몇 가지 조정 가능한 Kafka broker 및 클라이언트 구성에 대해 설명합니다. 수백 개의 실험을 실행한 후 다양한 프로덕션 사용 사례에 대한 최대 사용률을 달성하는 데 필요한 Kafka 구성을 표준화했습니다. 최상의 성능을 위해 Kafka 클러스터를 조정하는 방법을 보여 줍니다.</p>


  <p>성능에는 두 개의 직각 차원 &ndash; 처리량과 대기 시간이 있습니다. 이 환경에서 고객 성능 요구 사항은 아래 다이어그램의 세 가지 범주 A, B 및 C에 속합니다. 범주 A 고객은 높은 처리량(~1.5GBps)이 필요하며 대기 시간(&lt; 250ms)에 관대합니다. 이러한 시나리오 중 하나는 보안 및 침입 감지 애플리케이션과 같은 거의 실시간 프로세스에 대한 원격 분석 데이터 수집입니다. 범주 B 고객은 온라인 맞춤법 및 문법 검사와 같은 실시간 처리를 위해 매우 엄격한 대기 시간 요구 사항(&lt; 10ms)을 가지고 있습니다. 마지막으로, 범주 C 고객은 높은 처리량과 짧은 대기 시간(~100ms)이 모두 필요하지만 서비스 가용성 모니터링 애플리케이션과 같은 낮은 데이터 안정성을 허용할 수 있습니다.</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/8a78560f-ab71-4108-ad05-93cddbddc2e0.png"><img alt="This graph shows the maximum throughput we achieved in each case" border="0" height="554" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/3522f0f9-3763-40c9-ab9f-71d7bc042d73.png" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" title="이 그래프는 각 사례에서 달성한 최대 처리량을 보여줍니다." width="731"></a></p>


  <p>위의 그래프는 각 사례에서 달성한 최대 처리량을 보여줍니다. 안정성은 성능과 상충되는 또 다른 요구 사항입니다. Kafka는 데이터를 복제하고 구성 가능한 승인 설정을 제공하여 안정성을 제공합니다. 이러한 보장에 따른 성능 영향을 정량화합니다.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p>


  <p>Microsoft의 목표는 프로덕션 Kafka 클러스터를 실행하려는 모든 사용자가 각 구성의 효과를 이해하고, 관련된 장단점 평가, 사용 사례에 맞게 적절하게 조정하고, 최상의 성능을 얻을 수 있도록 하는 것입니다.</p>


  <h2>Siphon 및 Azure HDInsight</h2>


  <p>O365, Bing, Skype, SharePoint 온라인 등과 같은 기업에서 매일 3조 개의 이벤트를 수집 및 처리할 수 있는 거의 실시간 게시 구독 시스템을 준수하고 비용 효율적으로 빌드하기 위해 <a href="https://azure.microsoft.com/en-us/blog/siphon-streaming-data-ingestion-with-apache-kafka/">Siphon</a>이라는 스트리밍 플랫폼을 만들었습니다. Siphon은 <a href="https://azure.microsoft.com/en-us/services/hdinsight/">HDInsight의 Apache Kafka</a> 를 핵심 구성 요소로 사용하여 Azure 클라우드의 내부 Microsoft 고객을 위해 빌드됩니다. 하드웨어 구매, 비트 설치 및 튜닝 및 모니터링을 통해 Kafka 클러스터를 설정하고 운영하는 것은 매우 어려운 일입니다. Azure HDInsight는 Azure에서 Apache Kafka 클러스터를 프로비전하고 배포하는 비용 효율적인 VM 기반 <a href="https://azure.microsoft.com/en-us/pricing/details/hdinsight/">가격 책정 모델을</a> 사용하는 관리형 서비스입니다. HDInsight는 Kafka 가동 시간에 99.9%의 SLA를 사용하여 일상적인 유지 관리 및 패치를 수행하는 동안 브로커가 정상 상태를 유지하도록 보장합니다. 또한 <a href="https://docs.microsoft.com/en-us/azure/hdinsight/domain-joined/apache-domain-joined-run-kafka">역할 기반 액세스 제어</a> 및 <a href="https://azure.microsoft.com/en-us/blog/bring-your-own-keys-for-apache-kafka-on-hdinsight/">BYOK(Bring Your Own Key</a>) 암호화와 같은 엔터프라이즈 보안 기능도 있습니다.</p>


  <h2>벤치마크 설정</h2>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/0cab15e4-bc37-4b20-8811-ad8d1d206298.png"><img alt="Benchmark setup" border="0" height="282" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/b956ef49-d4ec-43bf-be1d-227bf0fc3148.png" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" title="벤치마크 설정" width="1006"></a></p>


  <h2>트래픽 생성기</h2>


  <p>일반적으로 시스템 및 Kafka 클러스터를 스트레스 테스트하기 위해 클러스터&rsquo; 프런트 엔드에 임의 바이트의 메시지 일괄 처리를 지속적으로 생성하는 애플리케이션을 개발했습니다. 이 애플리케이션은 100개의 스레드를 회전하여 5ms 간격으로 각 토픽에 1KB 임의 데이터 1,000개의 메시지를 보냅니다. 달리 명시적으로 언급하지 않는 한 표준 애플리케이션 구성입니다.</p>


  <h2>이벤트 서버 설정</h2>


  <p>Event Server는 Kafka 생산자 및 소비자 API를 구현하는 프런트 엔드 웹 서버로 사용됩니다. 클러스터에 여러 이벤트 서버를 프로비전하여 부하를 분산하고 수천 대의 클라이언트 컴퓨터에서 Kafka broker로 전송되는 생성 요청을 관리합니다. 각 Event Server 컴퓨터가 임의로 선택된 파티션&rsquo; 리더에 연결하여 고정 시간 간격 후에 다시 설정되는 파티션 선호도를 구현하여 브로커에 대한 TCP 연결 수를 최소화하도록 Event Server를 최적화했습니다. 각 Event Server 애플리케이션은 <a href="https://azure.microsoft.com/en-us/blog/f-series-vm-size/">Azure Standard F8s Linux VM</a>의 확장 집합에 있는 Docker 컨테이너에서 실행되며 최대 Java 힙 크기가 9GB로 설정된 7개의 CPU와 12GB의 메모리가 할당됩니다. 스트레스 도구에서 생성된 대량의 트래픽을 처리하기 위해 이러한 이벤트 서버의 인스턴스 20개 실행</p>


  <p>또한 이벤트 서버는 여러 슬라이딩 큐를 사용하여 클라이언트의 미해결 요청 수를 제어합니다. 새 요청은 이벤트 서버 인스턴스의 여러 큐 중 하나에 큐에 대기한 다음 여러 병렬 Kafka 생산자 스레드에서 처리됩니다. 각 스레드는 하나의 생산자를 인스턴스화합니다. 슬라이딩 큐 수는 스레드 풀 크기로 제어됩니다. 서로 다른 스레드 풀 크기에 대한 생산자 성능을 테스트할 때 스레드를 너무 많이 추가하면 처리 오버헤드가 발생하고 <a href="https://kafka.apache.org/documentation/#monitoring">Kafka 요청 큐 시간 및 로컬 처리 시간이</a> 증가할 수 있습니다. Kafka 전송 대기 시간이 두 배로 증가했음에도 불구하고 5개 이상의 스레드를 추가해도 수신 처리량이 크게 증가하지는 않았습니다. 따라서 이벤트 서버 인스턴스당 5개의 Kafka 생산자 스레드를 선택했습니다.</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/ececd185-beac-4b1f-928d-b2ae9030abb5.png"><img alt="Kafka producer threads" border="0" height="674" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/a93596ae-e3b7-489a-9472-03ed3aeff0ba.png" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" title="Kafka 생산자 스레드" width="1001"></a></p>


  <h2>Kafka Broker 하드웨어</h2>


  <p>실험에 <a href="https://kafka.apache.org/11/documentation.html">Kafka 버전 1.1</a> 을 사용했습니다. 테스트에 사용되는 Kafka 브로커는 <a href="https://azure.microsoft.com/en-us/pricing/details/virtual-machines/linux/">Azure Standard D4 V2 Linux VM입니다</a>. 코어 8개와 RAM 28GB가 각각 있는 10개의 브로커를 사용했습니다. 이 설정으로 높은 CPU 사용률이 발생하지 않았습니다. 반면 디스크 수는 처리량에 직접적인 영향을 미쳤습니다. 처음에는 각 Kafka Broker에 10 <a href="https://docs.microsoft.com/en-us/azure/virtual-machines/windows/managed-disks-overview">개의 Azure Managed Disks</a> 를 연결하여 시작했습니다. 기본적으로 Managed Disks는 데이터 복사본 3개가 단일 지역 내에 유지되는 LRS( <a href="https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy-lrs">로컬 중복 스토리지</a> )를 지원합니다. 이렇게 하면 LRS 스토리지 계정에 대한 쓰기 요청이 데이터가 모든 복사본에 기록된 후에만 성공적으로 반환되므로 또 다른 수준의 내구성이 도입됩니다. 각 복사는 스토리지 배율 단위 내에서 별도의 <a href="https://docs.microsoft.com/en-us/azure/hdinsight/kafka/apache-kafka-high-availability">장애 도메인 및 업데이트 도메인에</a> 상주합니다. 즉, 3배 복제 요소 Kafka 구성과 함께 기본적으로 9배 복제를 보장합니다.</p>


  <h2>소비자 및 Kafka 커넥트 설정</h2>


  <p>벤치마크에서는 <a href="https://kafka.apache.org/documentation/#connect">Kafka 커넥트</a> 커넥터 서비스로 사용하여 Kafka의 데이터를 사용했습니다. Kafka 커넥트 신뢰할 수 있고 확장 가능한 방식으로 Kafka 메시지를 생성하고 소비하기 위한 기본 제공 도구입니다. 실험을 위해 Kafka의 메시지를 사용하고 삭제한 다음 오프셋을 커밋하는 Null 싱크 커넥터를 실행했습니다. 이를 통해 생산자 및 소비자 처리량을 모두 측정하는 동시에 특정 대상으로 데이터를 전송하여 발생할 수 있는 병목 현상을 제거할 수 있었습니다. 이 설정에서는 <a href="https://azure.microsoft.com/en-us/blog/f-series-vm-size/">Azure Standard F8s Linux VM</a> 노드의 20개 인스턴스에서 Kafka 커넥트 Docker 컨테이너를 실행했습니다. 각 컨테이너에는 최대 Java 힙 크기가 7GB인 8개의 CPU와 10GB 메모리가 할당됩니다.</p>


  <h2>결과</h2>


  <h3>생산자 구성</h3>


  <p>성능 및 내구성에 가장 큰 영향을 미치는 것으로 확인된 주요 <a href="https://kafka.apache.org/documentation/#producerconfigs" target="_blank">생산자 구성</a> 은 다음과 같습니다.</p>


  <ul>
   <li>Batch.size</li>
   <li>Ack</li>
   <li>Compression.type</li>
   <li>Max.request.size</li>
   <li>Linger.ms</li>
   <li>Buffer.memory</li>
  </ul>


  <h3>Batch 크기</h3>


  <p>각 Kafka 생산자는 단일 파티션에 대한 레코드를 일괄 처리하여 파티션 리더에게 발급된 네트워크 및 IO 요청을 최적화합니다. 따라서 일괄 처리 크기를 늘리면 처리량이 증가할 수 있습니다. 가벼운 부하에서 생산자가 일괄 처리가 준비될 때까지 대기하므로 Kafka 전송 대기 시간이 증가할 수 있습니다. 이러한 실험의 경우 생산자를 대량의 요청으로 처리하므로 최대 512KB의 일괄 처리 크기까지 증가된 대기 시간을 관찰하지 않습니다&rsquo;. 그 외에 처리량이 감소하고 대기 시간이 증가하기 시작했습니다. 즉, 부하가 512KB 생산자 일괄 처리를 충분히 빠르게 채우기에 충분했습니다. 그러나 생산자는 더 큰 일괄 처리를 채우는 데 시간이 더 오래 걸렸습니다. <strong>따라서 부하가 많은 경우 처리량 및 대기 시간을 개선하기 위해 일괄 처리 크기를 늘리는 것이 좋습니다.</strong></p>


  <p><strong><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/7c21e04c-15eb-41cf-b6ec-630a5cd50401.png"><img alt="Kafka batch size" border="0" height="584" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/1cbae8d7-4b6f-4d82-bc42-ce56fa6abcee.png" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" title="Kafka 일괄 처리 크기" width="889"></a></strong></p>


  <p><em>Linger.ms</em> 설정은 일괄 처리를 제어합니다. 일괄 처리가 가득 차 있지 않더라도 생산자가 일괄 처리를 보내기 전에 기다리는 시간을 상한으로 지정합니다. 부하가 낮은 시나리오에서는 대기 시간을 희생하여 처리량을 향상시킵니다. 지속적인 높은 처리량에서 Kafka를 테스트했기 때문에 이 설정의 이점을 얻지 못했습니다&rsquo;.</p>


  <p>더 큰 일괄 처리를 지원하기 위해 조정한 또 다른 구성은 버퍼링에 생산자가 사용할 수 있는 메모리 양을 제어하는 <em>buffer.memory</em>였습니다. 이 설정을 1GB로 늘렸습니다.</p>


  <h2>생산자 필수 acks</h2>


  <p>생산자 필수 acks 구성은 쓰기 요청이 완료된 것으로 간주되기 전에 파티션 리더가 요구하는 승인 수를 결정합니다. 이 설정은 데이터 안정성에 영향을 미치며 값 0, 1 또는 -1(즉 &ldquo;, 모두&rdquo;)을 사용합니다.</p>


  <p>가장 높은 안정성을 달성하기 위해 acks = 모든 설정은 리더가 메시지를 승인하기 위해 모든 ISR(동기화 중인 복제본)을 대기하도록 보장합니다. 이 경우 동기화 중인 복제본 수가 구성된 min.insync.replicas보다 작으면 요청이 실패합니다. 예를 들어 min.insync.replicas를 1로 설정하면 해당 파티션에 사용할 수 있는 ISR이 <strong>하나 이상</strong> 있는 경우 리더가 요청을 성공적으로 승인합니다. 스펙트럼의 다른 쪽 끝에서 acks = 0을 설정하면 요청이 생산자가 보내는 즉시 완료된 것으로 간주됩니다. acks = 1을 설정하면 리더가 메시지를 수신했음을 보장합니다.</p>


  <p>이 테스트에서는 이 세 값 간의 구성을 변경했습니다. 결과는 안정성 보장과 대기 시간 사이에 발생하는 직관적인 절충을 확인합니다. <strong>ack = -1은 데이터 손실에 대해 더 강력한 보장을 제공하지만 대기 시간이 더 높고 처리량이 낮습니다.</strong></p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/42388b7b-c1d4-43ed-9064-4be2d924ab2b.png"><img alt="While ack provides stronger guarantees against data loss, it results in higher latency and lower throughput." border="0" height="640" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/5aae2be0-22db-4543-a7c5-55f14258d757.png" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" title="ack는 데이터 손실에 대해 더 강력한 보장을 제공하지만 대기 시간이 더 짧고 처리량이 낮아질 수 있습니다." width="954"></a></p>


  <h2>압축</h2>


  <p>Kafka 생산자는 메시지를 브로커에 보내기 전에 압축하도록 구성할 수 있습니다. <em>Compression.type</em> 설정은 사용할 압축 코덱을 지정합니다. 지원되는 압축 코덱은 &ldquo;gzip,&rdquo; &ldquo;snappy&rdquo; 및 &ldquo;lz4입니다.&rdquo; 압축은 도움이 되며 디스크 용량에 제한이 있는 경우 고려해야 합니다.</p>


  <p>일반적으로 사용되는 두 압축 코덱인 gzip&rdquo;과 &ldquo;snappy&rdquo; <strong>&ldquo;중 gzip&rdquo;은 압축 비율이 높아 CPU 부하가 높은 디스크 사용량이 감소하는 반면&ldquo;, snappy&rdquo;는 CPU 오버헤드가 적은 압축을 줄입니다. </strong> &ldquo; gzip&rdquo;이 데이터를 snappy보다 &ldquo;5배 더 압축할 수 있으므로 &ldquo;브로커 디스크 또는 생산자 CPU 제한 사항에 따라 사용할 코덱을 결정할 수 있습니다.&rdquo;</p>


  <p>이전 Kafka 생산자(Scala 클라이언트)를 사용하여 최신 Kafka 버전으로 보내면 메시지 형식 구조(<a href="https://kafka.apache.org/documentation/#messageset" target="_blank">매직 바이트</a>)의 비호환성이 생성되어 브로커가 작성하기 전에 압축을 풉니다. 이렇게 하면 이 추가 작업으로 인해 메시지 배달 및 CPU 오버헤드(이 경우 거의 10%)에 대기 시간이 추가됩니다. 최신 Kafka 버전을 사용하는 경우 Java 생산자 클라이언트를 사용하는 것이 좋습니다.</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/590c32f7-6a45-4187-90b0-1f7d8350dc25.png"><img alt="Throughput versus latency" border="0" height="657" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/68fabc5a-70f5-41e2-8500-1f7024e235f5.png" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" title="처리량 및 대기 시간" width="970"></a></p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/b6d9bbd7-d11a-4875-a3d5-08f784cab09e.png"><img alt="CPU Utilization" border="0" height="626" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/df7f408a-9c6b-49f8-806a-15e809c2766f.png" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" title="CPU 사용률" width="906"></a></p>


  <h2>Broker 구성</h2>


  <h3>디스크 수</h3>


  <p>스토리지 디스크에는 IOPS(초당 입력/출력 작업) 및 초당 읽기/쓰기 바이트가 제한되어 있습니다. 새 파티션을 만들 때 Kafka는 기존 파티션이 가장 적은 디스크에 각각의 새 파티션을 저장하여 사용 가능한 다른 디스크와 균형을 맞춥니다. 그럼에도 불구하고 각 디스크에서 수백 개의 파티션 복제본을 처리할 때 Kafka는 사용 가능한 디스크 처리량을 쉽게 포화시킬 수 있습니다.</p>


  <p>클러스터에서 <a href="https://azure.microsoft.com/en-us/pricing/details/managed-disks/" target="_blank">Azure 표준 S30 HDD 디스크를</a> 사용했습니다. 실험에서는 Kafka가 디스크당 여러 동시 I/O 작업을 수행하는 동안 디스크당 평균 38.5MBps 처리량을 관찰했습니다. 전체 쓰기 처리량에는 Kafka 수집 및 복제 요청이 모두 포함됩니다.</p>


  <p>브로커당 10개, 12개 및 16개의 연결된 디스크를 테스트하여 생산자 처리량에 미치는 영향을 연구했습니다. <strong>결과는 연결된 디스크 수가 증가하는 처리량의 상관 관계를 보여 줍니다.</strong> 하나의 VM에 연결할 수 있는 디스크 수(최대 16개 디스크)로 제한되었습니다. 따라서 디스크를 더 추가하려면 추가 VM이 필요하므로 비용이 증가합니다. 다음 실험에서는 브로커당 16개의 표준 HDD를 계속 적용하기로 결정했습니다. 이 실험은 디스크 수의 영향을 관찰하기 위한 것이며 처리량을 최적화하기 위해 수행된 다른 구성 튜닝을 포함하지 않았습니다. 따라서 이 섹션에 언급된 처리량은 이 게시물의 다른 위치에 표시된 값보다 낮습니다.</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/54e21a17-aad9-4559-804f-98620557fe9c.png"><img alt="Number of throughput" border="0" height="646" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/4e7eb7e1-db87-4f80-b2e7-2422820674d9.png" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" title="처리량 수" width="948"></a></p>


  <h3>토픽 및 파티션 수</h3>


  <p>각 Kafka 파티션은 시스템의 로그 파일이며 생산자 스레드는 동시에 여러 로그에 쓸 수 있습니다.</p>


  <p>마찬가지로, 각 소비자 스레드는 하나의 파티션에서 메시지를 읽기 때문에 여러 파티션에서 소비하는 것도 병렬로 처리됩니다. 이 실험에서는 파티션 밀도(즉, 복제본을 포함하지 않는 브로커당 파티션 수)가 성능에 미치는 영향을 정량화합니다. 파티션 밀도를 높이면 메타데이터 작업과 관련된 오버헤드와 파티션 리더와 해당 팔로워 간의 파티션 요청/응답당 오버헤드가 추가됩니다. 데이터 흐름이 없는 경우에도 파티션 복제본은 여전히 리더에서 데이터를 가져오므로 네트워크를 통한 전송 및 수신 요청에 대한 추가 처리가 발생합니다. 따라서 CPU를 보다 효율적으로 활용하기 위해 I/O, 네트워크 및 복제본 페처 스레드 수를 늘렸습니다. CPU가 완전히 활용되면 스레드 풀 크기를 늘리면 처리량이 향상되지 않을 수 있습니다. <a href="https://kafka.apache.org/documentation/#monitoring">Kafka 메트릭</a>을 사용하여 네트워크 및 I/O 프로세서 유휴 시간을 모니터링할 수 있습니다.</p>


  <p>또한 요청 및 응답 큐 시간에 대한 <a href="https://kafka.apache.org/documentation/#monitoring">Kafka 메트릭</a> 을 관찰하면 Kafka 스레드 풀의 크기를 조정할 수 있습니다. 더 많은 I/O 및 네트워크 스레드를 할당하면 요청 및 응답 큐 대기 시간을 모두 줄일 수 있습니다. 요청 로컬 대기 시간이 높을수록 디스크가 I/O 요청을 충분히 빠르게 처리할 수 없음&rsquo;을 나타냅니다. 주요 Kafka 구성은 아래 목록에 요약되어 있습니다.</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/59e2fb4a-d251-407d-a205-68918cbf3979.png"><img alt="10" border="0" height="324" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/359191c7-edeb-4cd1-8ec0-5a56cc729b11.png" style="border: 0px currentcolor; border-image: none; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;" title="10" width="727"></a></p>


  <p><strong>Kafka는 broker당 수천 개의 파티션을 처리할 수 있습니다</strong>. 토픽당 파티션 100개, 즉 브로커당 총 200개의 파티션(토픽 20개 및 브로커 10개)에서 가장 높은 처리량을 달성했습니다. 더 높은 파티션 밀도에 대해 표시되는 처리량 감소는 디스크가 처리해야 하는 추가 I/O 요청의 오버헤드로 인해 발생한 높은 대기 시간에 해당합니다.</p>


  <p>또한 파티션 밀도가 증가하면 토픽을 사용할 수 없게 될 수 있습니다. 이러한 경우 Kafka는 각 브로커가 더 많은 파티션을 저장하고 리더가 되어야 합니다. 이러한 브로커의 부정한 종료의 경우, 새로운 지도자를 선출하는 데 몇 초가 걸릴 수 있습니다, 크게 성능에 영향을 미치는.</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/36441b94-9d55-4880-8c6e-c9a0677aa9cc.png"><img alt="Partition density" border="0" height="621" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/ee6e9b6a-c7f6-4a27-a4d1-b2a805cd2c42.png" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" title="파티션 밀도" width="941"></a></p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/5c6748de-cf43-4ebc-8b05-8397833b6e94.png"><img alt="CPU Utilization versus Partition density" border="0" height="615" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/07305da0-7f1c-4a0c-a7a7-f3fcef9a5754.png" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" title="CPU 사용률과 파티션 밀도 비교" width="947"></a></p>


  <h3>복제본 수</h3>


  <p>복제는 서비스 안정성을 제공하는 토픽 수준 구성입니다. Siphon에서는 일반적으로 프로덕션 환경에서 3배 복제를 사용하여 최대 2개의 브로커를 동시에 사용할 수 없는 상황에서 데이터를 보호합니다. 그러나 가용성보다 높은 처리량과 짧은 대기 시간을 달성하는 것이 더 중요한 경우 복제 인수를 더 낮은 값으로 설정할 수 있습니다.</p>


  <p>복제 계수가 높을수록 파티션 리더와 팔로워 간에 추가 요청이 발생합니다. 따라서 <strong>복제 비율이 높을수록</strong> 추가 요청을 처리하기 위해 디스크와 CPU가 더 많이 사용되어<strong> 쓰기 대기 시간이 증가하고 처리량이 감소합니다.</strong></p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/61013bbe-2905-4965-ab07-c7c838d44717.png"><img alt="Producer throughput and replication" border="0" height="597" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/97b4747e-188d-47cd-adc5-2623f24ad841.png" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" title="생산자 처리량 및 복제" width="967"></a></p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/bc081b88-2ce0-472b-8c52-cd782ccc1de6.png"><img alt="Kafka send latency versus replication" border="0" height="616" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/977a09e1-3b1a-432c-a94c-96e5166b1478.png" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" title="Kafka 전송 대기 시간 및 복제" width="939"></a></p>


  <h3>메시지 크기</h3>


  <p>Kafka는 대량의 데이터를 매우 효율적으로 이동할 수 있습니다. 그러나 Kafka는 QPS(초당 쿼리 수) 및 메시지 크기 측면에서 수신 볼륨에 따라 대기 시간을 변경할 수 있습니다. 메시지 크기의 효과를 연구하기 위해 메시지 크기를 1KB에서 1.5MB로 테스트했습니다. 이 실험 중에 부하가 일정하게 유지되었습니다. 메시지 크기에 관계없이 ~1.5GBps의 일정한 처리량과 ~150ms의 대기 시간을 관찰했습니다. 1.5MB보다 큰 메시지의 경우 이 동작이 변경될 수 있습니다.</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/2868b75e-b085-4090-9341-50fa20dead53.png"><img alt="image" border="0" height="197" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/8236366d-e137-485c-86f2-a0bfca1753d6.png" style="border: 0px currentcolor; border-image: none; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;" title="이미지" width="770"></a></p>


  <h2>결론</h2>


  <p>생산자, 브로커 및 소비자를 구성하도록 조정할 수 있는 수백 개의 <a href="https://kafka.apache.org/documentation/">Kafka 구성</a> 이 있습니다. 이 블로그에서는 성능에 영향을 주는 것으로 확인된 주요 구성을 정확히 파악했습니다. 이러한 매개 변수를 처리량, 대기 시간 및 CPU 사용률과 같은 성능 메트릭에 튜닝하는 효과를 보여 줍니다. 파티션 밀도, 버퍼 크기, 네트워크 및 IO 스레드와 같은 적절한 구성을 통해 브로커 10개와 브로커당 디스크 16개와 함께 약 2GBps를 달성한 것으로 나타났습니다. 또한 복제 인자 및 복제본 승인과 같은 구성을 사용하여 안정성과 처리량 간에 발생하는 장단분도 정량화했습니다.</p>
