### YamlMime:Yaml
ms.openlocfilehash: 006bc8b2d5018fe115e2c1bc18411293b39a195e
ms.sourcegitcommit: d03bdc7fe5447adb6530886aab848b75fe8fa8ee
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 03/11/2022
ms.locfileid: "139908804"
Slug: onnx-runtime-integration-with-nvidia-tensorrt-in-preview
Title: 미리 보기에서 NVIDIA TensorRT와 ONNX 런타임 통합
Summary: '오늘 ONNX 런타임에서 NVIDIA TensorRT 실행 공급자의 미리 보기를 오픈 소스로 제공하게 되어 기쁩니다. 이 릴리스에서는 개발자가 프레임워크 선택에 관계없이 업계 최고의 GPU 가속을 쉽게 활용할 수 있도록 하여 개방형 및 상호 운용 가능한 AI를 향해 한 걸음 더 나아가고 있습니다. '
Content: >-
  <p>오늘 <a href="https://azure.microsoft.com/en-us/blog/onnx-runtime-is-now-open-source/">ONNX 런타임</a>에서 NVIDIA TensorRT 실행 공급자의 미리 보기를 오픈 소스로 제공하게 되어 기쁩니다. 이 릴리스에서는 개발자가 프레임워크 선택에 관계없이 업계 최고의 GPU 가속을 쉽게 활용할 수 있도록 하여 개방형 및 상호 운용 가능한 AI를 향해 한 걸음 더 나아가고 있습니다. 이제 개발자는 ONNX 런타임을 통해 TensorRT의 기능을 활용하여 PyTorch, <a href="https://developer.nvidia.com/tensorrt">TensorFlow</a> 및 기타 인기 있는 프레임워크에서 내보내거나 변환할 수 있는 ONNX 모델의 추론을 가속화할 수 있습니다.</p>


  <p>Microsoft와 NVIDIA는 TensorRT 실행 공급자를 ONNX 런타임과 통합하기 위해 긴밀히 협력했으며 <a href="https://github.com/onnx/models">모델 동물원</a>의 모든 ONNX 모델에 대한 지원의 유효성을 검사했습니다. TensorRT 실행 공급자를 사용하면 ONNX 런타임이 일반 GPU 가속에 비해 동일한 하드웨어에서 더 나은 추론 성능을 제공합니다. Bing MultiMedia 서비스의 내부 워크로드에서 TensorRT 실행 공급자를 사용하여 성능이 최대 2배 향상되었습니다.</p>


  <h2>작동 방식</h2>


  <p>ONNX 런타임을 TensorRT 실행 공급자와 함께 사용하면 그래프를 구문 분석하고 지원되는 하드웨어에서 TensorRT 스택에서 실행할 특정 노드를 할당하여 딥 러닝 모델의 추론을 가속화합니다. TensorRT 실행 공급자는 ONNX 하위 그래프를 처리하고 NVIDIA 하드웨어에서 실행하기 위해 플랫폼에 미리 설치된 TensorRT 라이브러리와 인터페이스합니다. 이를 통해 개발자는 다양한 버전의 하드웨어에서 ONNX 모델을 실행하고 다양한 하드웨어 구성을 유연하게 대상으로 지정할 수 있는 애플리케이션을 빌드할 수 있습니다. 이 아키텍처는 심층 신경망의 실행을 최적화하는 데 필수적인 하드웨어 특정 라이브러리의 세부 정보를 추상화합니다.</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/36f70acc-f94f-4c42-82c8-98b7e1e22533.png"><img alt="Infographic showing input data and output result using the ONNX model" border="0" height="483" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/67b71a1a-0165-453c-bc2b-9fe997c603c3.png" style="border: 0px currentcolor; border-image: none; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;" title="ONNX 모델을 사용하여 입력 데이터 및 출력 결과를 보여 주는 인포그래픽" width="1024"></a></p>


  <h2>TensorRT 실행 공급자를 사용하는 방법</h2>


  <p>TensorRT 실행 공급자와 함께 ONNX 런타임은 Opset 버전 9와 함께 ONNX 사양 v1.2 이상을 지원합니다. TensorRT 최적화 모델은 Azure의 NVIDIA GPU에서 구동되는 모든 N 시리즈 VM에 배포할 수 있습니다.</p>


  <p>TensorRT를 사용하려면 먼저 TensorRT 실행 공급자를 사용하여 ONNX 런타임을 <a href="https://github.com/Microsoft/onnxruntime/blob/master/BUILD.md">빌드</a> 해야 합니다(build.sh 도구에서 플래그 사용<code>&nbsp;--use_tensorrt --tensorrt_home &lt;path to </code><code>location</code><code> for TensorRT libraries in your local machine&gt;</code> ). 그런 다음 ONNX 런타임 API를 통해 유추 세션을 시작하여 TensorRT를 활용할 수 있습니다. ONNX 런타임은 성능을 최대화하기 위해 TensorRT에서 실행하기 위해 적절한 하위 그래프의 우선 순위를 자동으로 지정합니다.</p>


  <pre>

  InferenceSession session_object{so};

  session_object.RegisterExecutionProvider(std::make_unique&lt;::onnxruntime::TensorrtExecutionProvider&gt;());

  status = session_object.Load(model_file_name);</pre>


  <p>자세한 지침은 <a href="https://aka.ms/trt-onnxrt">GitHub</a> 사용할 수 있습니다. 또한 표준 테스트 컬렉션은 리포지토리의 onnx_test_runner 유틸리티를 통해 사용할 수 있으므로 TensorRT 실행 공급자를 사용하여 ONNX 런타임 빌드를 확인할 수 있습니다.</p>


  <h2>ONNX 및 ONNX 런타임이란?</h2>


  <p><a href="https://onnx.ai/">ONNX</a> 는 Microsoft가 Facebook 및 AWS와 공동 개발한 딥 러닝 및 기존 기계 학습 모델을 위한 개방형 형식입니다. ONNX를 사용하면 ONNX 런타임을 사용하여 여러 하드웨어 플랫폼에서 실행할 수 있는 공통 형식으로 모델을 나타낼 수 있습니다. 이를 통해 개발자는 작업에 적합한 프레임워크를 자유롭게 선택할 수 있을 뿐만 아니라 선택한 하드웨어를 사용하여 다양한 플랫폼에서 모델을 효율적으로 실행할 수 있습니다.</p>


  <p><a href="https://github.com/microsoft/onnxruntime">ONNX 런타임은 ONNX-ML</a> 프로필을 포함하여 ONNX 1.2 이상을 완전히 지원하는 최초의 공개적으로 사용 가능한 유추 엔진입니다. ONNX 런타임은 TensorRT와 같은 하드웨어 가속기를 실행 공급자로 &ldquo;연결할 수 있는 <a href="https://github.com/Microsoft/onnxruntime/blob/master/docs/HighLevelDesign.md">확장 가능한 아키텍처</a>를 갖춘 경량 모듈식입니다.&rdquo; 이러한 실행 공급자는 짧은 대기 시간 및 고효율 신경망 계산의 잠금을 해제합니다. 현재 ONNX 런타임은 Bing, Office 등에서 수십억 명의 사용자에게 서비스를 제공하는 핵심 시나리오를 제공합니다.</p>


  <h2>개방적이고 상호 운용 가능한 AI를 향한 또 다른 단계</h2>


  <p>ONNX 런타임용 TensorRT 실행 공급자의 미리 보기는 AI를 위한 개방적이고 상호 운용 가능한 에코시스템을 만들기 위한 또 다른 이정표입니다. 이를 통해 프로덕션 모델에 대한 대기 시간 요구 사항이 계속 증가하는 세계에서 AI 혁신을 보다 쉽게 추진할 수 있기를 바랍니다. ONNX 런타임은 지속적으로 발전하고 개선되고 있으며, 여러분의 피드백과 기여를 기대합니다!  </p>


  <p>클라우드 및 에지에서 가속 추론에 ONNX를 사용하는 방법에 대해 자세히 알아보려면 NVIDIA GTC에서 <a href="https://gputechconf2019.smarteventscloud.com/connect/search.ww#loadSearch-searchPhrase=ONNX&amp;searchType=session&amp;tc=0&amp;sortBy=dayTime&amp;p=">ONNX 세션을</a> 확인하세요. ONNX 런타임에 대한 피드백 또는 질문이 있나요? GitHub <a href="https://github.com/Microsoft/onnxruntime/issues">문제를</a> 제출하고 <a href="https://twitter.com/onnxruntime">Twitter</a>에서 팔로우하세요.&nbsp;</p>
