### YamlMime:Yaml
ms.openlocfilehash: 78876e9614f9df791d300c4ebe358014d9c83f79
ms.sourcegitcommit: d03bdc7fe5447adb6530886aab848b75fe8fa8ee
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 03/11/2022
ms.locfileid: "139907876"
Slug: bing-delivers-its-largest-improvement-in-search-experience-using-azure-gpus
Title: Bing Azure GPU를 사용하여 검색 환경의 가장 큰 개선을 제공합니다.
Summary: 지난 몇 년 동안 딥 러닝은 Bing 검색 스택에서 널리 채택되었으며 방대한 수의 지능형 기능을 지원합니다.
Content: >-
  <p>지난 몇 년 동안 딥 러닝은 Bing 검색 스택에서 널리 채택되었으며 방대한 수의 지능형 기능을 지원합니다. 자연어 모델을 사용하여 사용자&rsquo; 검색 의도 및 관련 웹 페이지에 대한 핵심 검색 알고리즘&rsquo; 이해를 개선하여 Bing 사용자에게 가장 관련성이 큰 검색 결과를 제공할 수 있도록 합니다. Microsoft는 텍스트 설명 또는 요약 메타데이터가 없는 경우에도&rsquo; 수십억 개의 이미지 검색 가능성을 향상시키기 위해 딥 러닝 컴퓨터 비전 기술을 사용합니다. 기계 기반 읽기 이해 모델을 활용하여 사용자가 가진 특정 질문에 직접 답하는 더 큰 텍스트 본문 내에서 캡션을 검색합니다. 이러한 모든 향상된 기능은 웹 검색 쿼리에 대한 보다 관련성이 높고 상황에 맞는 결과를 제공합니다.</p>


  <p>최근 변환기(변환기, <a href="https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Farxiv.org%2Fabs%2F1810.04805&amp;data=02%7C01%7Cv-carjen%40microsoft.com%7C60eee5593b424638a57908d76bc14484%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637096350133971160&amp;sdata=5g6BsXkAcxjMTsG%2FbxTgUN4hqLbl2nmfcE9z%2B5LaM%2F8%3D&amp;reserved=0">BERT</a>의 양방향 인코더 표현에 의해 대중화됨)라는 모델 유형으로 자연어 이해에 돌파구가 있었습니다. 단어를 순서대로 개별적으로 처리한 이전의 DNN(심층 신경망) 아키텍처와 달리 변환기는 문장에서 각 단어와 그 주변의 모든 단어 사이의 컨텍스트와 관계를 이해합니다. 올해 4월부터 대형 변압기 모델을 사용하여 지난 1년 동안 Bing 고객에게 가장 큰 품질 개선을 제공했습니다. 예를 들어 쿼리&quot;에서 뇌진탕&quot;을 악화시킬 수 있는 항목에서 악화&quot;라는 단어&quot;는 사용자가 원인이나 증상이 아니라 뇌진탕 후 수행할 작업에 대해 알아보고자 했음을 나타냅니다. 이러한 모델을 통해 제공되는 검색은 이제 사용자 의도를 이해하고 더 유용한 결과를 제공할 수 있습니다. 더 중요한 것은 이러한 모델이 모든 Bing 검색 쿼리에 전역적으로 적용되어 Bing 결과가 더 관련성이 높고 지능적이된다는 점입니다.</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/f32c5d70-d3c7-4c1d-ad12-35fa5f1fdb4e.jpg"><img alt="rankBERTblogbeforeafter" border="0" height="292" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/4ae2fecf-0ff2-4765-98ae-7172103207c4.jpg" style="border: 0px currentcolor; border-image: none; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;" title="rankBERTblogbeforeafter" width="1024"></a></p>


  <h2>웹 검색 규모의 딥 러닝은 엄청나게 비용이 많이 들 수 있습니다.</h2>


  <p>Bing 고객은 매우 빠른 검색 환경과 모든 밀리초의 대기 시간을 기대합니다.&nbsp; 변환기 기반 모델은 최대 수십억 개의 매개 변수로 미리 학습되며, 이는 이전 네트워크 아키텍처에 비해 매개 변수 크기 및 계산 요구 사항이 상당한 증가입니다. 20개의 CPU 코어에서 대기 시간을 제공하는 증류된 3계층 BERT 모델은 처음에 유추당 77ms로 벤치마킹되었습니다. 그러나 이러한 모델은 웹 검색에 전원을 공급하기 위해 초당 수백만 개 이상의 쿼리와 코드 조각을 실행해야 하므로 유추당 77ms조차도 웹 검색 규모에서 엄청나게 비용이 많이 들기 때문에 수만 대의 서버가 단 하나의 검색 개선 사항만 제공해야 했습니다.</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/0784edf6-9167-4a1d-a271-213f22dc62fe.png"><img alt="Bert Model Optimization" border="0" height="530" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/e1500ba4-178c-4118-a6f6-21f114879013.png" style="border: 0px currentcolor; border-image: none; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;" title="Bert 모델 최적화" width="1024"></a></p>


  <h2>Azure Virtual Machine GPU를 활용하여 800배 유추 처리량 달성</h2>


  <p>변환기와 이전 DNN 아키텍처 간의 주요 차이점 중 하나는 순차적 처리 대신 대규모 병렬 컴퓨팅에 의존한다는 것입니다. GPU(그래픽 처리 장치) 아키텍처가 높은 처리량 병렬 컴퓨팅을 위해 설계되었다는 점을 감안할 때 GPU 가속기가 내장된 AzureS&rsquo; N 시리즈 VM(Virtual Machines)은 이러한 변환기 모델을 가속화하기 위한 자연스러운 적합성이었습니다. 우리는 주로 낮은 비용과 지역 가용성 때문에 <a href="https://azure.microsoft.com/en-us/pricing/details/virtual-machines/windows/#n-series">NV6 Virtual Machine</a> 으로 시작하기로 결정했습니다.&nbsp; GPU를 사용하여 해당 VM에서 3계층 BERT 모델을 실행하는 것만으로도 20ms(약 3배 개선)의 대기 시간이 관찰되었습니다. 서비스 효율성을 더욱 개선하기 위해 NVIDIA와 협력하여 GPU 아키텍처를 최대한 활용하고 포함, 변환기 및 출력 계층 다시 쓰기를 포함하여 TensorRT C++ API 및 CUDA 또는 CUBLAS 라이브러리를 사용하여 전체 모델을 다시 구현했습니다.&nbsp; NVIDIA는 또한 softmax, GELU, 정규화 및 감소를 포함한 효율적인 CUDA 변환기 플러그 인을 제공했습니다.</p>


  <p>동일한 Azure NV6 Virtual Machine에서 TensorRT 최적화 GPU 모델을 벤치마킹했으며 GPU 가속이 없는 모델에 비해 9ms에서 4개의 유추, 8배 대기 시간 속도 향상 및 34배 처리량 향상을 제공할 수 있었습니다. 그런 다음, NC6s_v3 Virtual Machine에서 정밀도가 혼합된 Tensor Cores를 활용하여 성능을 더욱 최적화하고, 6ms에서 64개의 유추(CPU에 비해 최대 800배 처리량 향상)의 일괄 처리 크기를 벤치마킹했습니다.</p>


  <h2>Azures&rsquo; 글로벌 규모를 사용하여 전 세계적으로 Bing 검색 환경 변환</h2>


  <p>이러한 GPU 최적화를 통해 4개 지역에서 2,000개 이상의 Azure GPU Virtual Machines를 사용하여 전 세계적으로 초당 100만 개 이상의 BERT 유추를 제공할 수 있었습니다. Azure N 시리즈 GPU VM은 특히 딥 러닝 모델이 계속 복잡해짐에 따라 고가용성, 민첩성 및 상당한 비용 절감으로 Bing 혁신적 AI 워크로드 및 제품 품질 향상을 가능하게 하는 데 매우 중요합니다. Bing 같은 대규모 조직에서도 기본 제공 GPU 가속을 통해 Azure에서 N 시리즈 가상 머신을 사용하여 AI 워크로드를 가속화할 수 있습니다. GPU 없이 이러한 종류의 글로벌 규모 AI 추론을 제공하려면 CPU 기반 VM 수가 기하급수적으로 더 많이 필요했을 것이고, 결국에는 비용이 많이 듭니다.&nbsp; 또한 Azure는 고객에게 여러 유형의 GPU에 즉시 배포할 수 있는 민첩성을 제공하며, 온-프레미스에서 GPU를 설치하는 데 몇 달이 걸렸을 것입니다.&nbsp; N 시리즈 Virtual Machines는 오늘날 전 세계적으로 사용할 수 있는 Bing 검색을 개선하기 위해 고급 딥 러닝 모델을 최적화하고 제공하는 기능에 필수적입니다.</p>


  <h2>N 시리즈 일반 공급</h2>


  <p>Azure는 <a href="https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-gpu" target="_blank">NC, ND 및 NV 시리즈</a> 제품 라인에 걸쳐 Virtual Machine 기능의 전체 포트폴리오를 제공합니다. 이러한 Virtual Machines는 GPU 가속이 일반적인 애플리케이션 시나리오(예: 계산 집약적, 그래픽 집약적 및 시각화 워크로드)를 위해 설계되었습니다.</p>


  <ul>
      <li>NC 시리즈 Virtual Machines는 계산 집약적이고 네트워크 집약적인 애플리케이션에 최적화되어 있습니다.</li>
      <li>ND 시리즈 Virtual Machines는 딥 러닝을 위한 학습 및 추론 시나리오에 최적화되어 있습니다.</li>
      <li>NV 시리즈 Virtual Machines는 시각화, 스트리밍, 게임, 인코딩 및 VDI 시나리오에 최적화되어 있습니다.</li>
  </ul>


  <p>ND 및 NV 시리즈 Virtual Machines에 대한 최신 제품 추가는 <a href="https://azure.microsoft.com/en-us/blog/new-azure-hpc-and-partner-offerings-at-supercomputing-19/" target="_blank">Supercomputing19 블로그</a> 를 참조하세요.</p>


  <h2>자세한 정보</h2>


  <p>Supercomputing19에 참여하여 Azure GPU를 활용하여 Bing 최적화 여정에 대해 자세히 알아보세요.</p>
