### YamlMime:Yaml
ms.openlocfilehash: dbdadd80d91c4aaeac1dc0dbf2a58abb723df47b
ms.sourcegitcommit: d03bdc7fe5447adb6530886aab848b75fe8fa8ee
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 03/11/2022
ms.locfileid: "139906214"
Slug: azure-introduces-new-capabilities-for-live-video-analytics
Title: Azure는 라이브 비디오 분석을 위한 새로운 기능을 도입했습니다.
Summary: 2020년 6월에는 인텔리전트 에지에서 인텔리전트 클라우드로 실시간 분석을 사용하여 비디오를 캡처하고 처리하는 워크플로를 빌드할 수 있는 획기적인 새로운 기능 집합인 Live Video Analytics 플랫폼의 미리 보기를 Azure Media Services 발표했습니다.
Content: >-
  <p>2020년 6월에는 인텔리전트 에지에서 인텔리전트 클라우드로 실시간 분석을 사용하여 비디오를 캡처하고 처리하는 워크플로를 빌드할 수 있는 <a href="https://azure.microsoft.com/blog/introducing-live-video-analytics-on-iot-edge-now-in-preview/" target="_blank">Azure Media Services Live Video Analytics</a> 플랫폼&mdash;의 획기적인 새로운 기능 집합의 미리 보기를 발표했습니다. Microsoft는 IoT Edge의 Live Video Analytics를 미리 보기로 사용하여 업계 전반의 고객들을 지속적으로 만나 조직에 긍정적인 결과를 제공합니다. 지난 주 Microsoft Ignite에서 소셜 분산, 공장 현장 안전, 보안 경계 모니터링 등을 포함하는 추가 시나리오의 잠금을 해제하는 새로운 기능, 파트너 통합 및 참조 앱을 발표했습니다. 이러한 시나리오를 사용하도록 설정하는 새로운 제품 기능은 다음과 같습니다.</p>


  <ul>
      <li><strong>Cognitive Services용 Azure Computer Vision의 공간 분석:</strong> 물리적 도메인의 사용자와 이동 간의 공간 관계를 고려한 향상된 비디오 분석입니다.</li>
      <li><strong>Intel OpenVINO 모델 서버 통합:</strong> Intel CPU(Atom, Core, Xeon), FPGA 및 VPU에서 실행되는 최적화된 미리 학습된 모델을 사용하여 OpenVINO 도구 키트로 구동되는 복잡하고 성능이 뛰어난 라이브 비디오 분석 솔루션을 빌드합니다.</li>
      <li><strong>NVIDIA DeepStream 통합:</strong> NVIDIA GPU의 기능을 Azure 서비스와 결합하는 하드웨어 가속 하이브리드 비디오 분석 앱을 지원합니다.</li>
      <li><strong>Arm64 지원:</strong> 저전력, 낮은 공간 Linux Arm64 디바이스에서 라이브 비디오 분석 솔루션을 개발하고 배포합니다.</li>
      <li><strong>Azure IoT Central Custom Vision 템플릿:</strong> 코딩 없이 몇 분에서 몇 시간 안에 풍부한 사용자 지정 비전 애플리케이션을 빌드합니다.</li>
      <li><strong>Cognitive Services Custom Vision 통합을 사용한 높은 프레임 속도 유추:</strong> 공장 환경에 대한 6가지 유용한 시나리오를 지원하는 제조 산업 참조 앱에 설명되어 있습니다.</li>
  </ul>


  <h2>비디오 AI를 더 쉽게 사용할 수 있도록 만들기</h2>


  <p>사용 가능한 다양한 CPU 아키텍처(x86-64, Arm 등) 및 하드웨어 가속 옵션(Intel Movidius VPU, iGPU, FPGA, NVIDIA GPU)과 사용자 지정 AI를 빌드하기 위한 데이터 과학 전문가의 부족이 고려되어 기존의 비디오 분석 솔루션을 함께 사용하면 상당한 시간, 노력 및 복잡성이 수반됩니다.</p>


  <p>이 공지는&rsquo; 인텔, NVIDIA 및 Arm을 비롯한 널리 사용되는 칩 아키텍처에 대한 지원, NVIDIA DeepStream 및 Intel OpenVINO와 같은 하드웨어 최적화 AI 프레임워크와의 통합, Microsofts&rsquo; AI 에코시스템&mdash; 전반의 보완 기술과 긴밀한 통합을 통해 모든 사용자에게&mdash; 비디오 분석을 보다 접근 가능하고 유용하게 만드는 우리의 사명을 더욱 강화합니다. 공간 분석 및 Cognitive Services Custom Vision용 Computer Vision뿐만 아니라 Azure IoT Central Custom Vision 템플릿 및 제조 현장 참조 애플리케이션을 통한 향상된 개발 환경</p>


  <h2>공간 분석을 위한 Computer Vision을 사용하여 라이브 비디오 분석</h2>


  <p>Azure Cognitive Service의 일부인 Computer Vision의 공간 분석 기능을 IoT Edge의 Live Video Analytics와 함께 사용하여 물리적 환경에서 사용자와 이동 간의 공간 관계를 더 잘 이해할 수 있습니다. Weve&rsquo;는 카메라&rsquo; 시야 내의 지정된 영역에 있는 사람을 계산하거나, 사람이 지정된 선이나 영역을 교차하는 경우 또는 사람들이 거리 규칙을 위반하는 시기를 추적할 수 있는 새로운 작업을 추가했습니다.</p>


  <p>Live Video Analytics 모듈은 RTSP(실시간 스트리밍 프로토콜) 카메라에서 라이브 비디오를 캡처하고 AI 처리를 위한 공간 분석 모듈을 호출합니다. 이러한 모듈은 로컬 또는 Azure Blob Storage에 대한 비디오 분석 및 클립 녹화를 사용하도록 구성할 수 있습니다.</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/70553d6b-5ec6-4245-bdf6-0d49dfd7a757.jpg"><img alt="An architecture diagram showing how Computer Vision Spatial Analysis and Live Video Analytics can be combined to build computer vision solutions that understand spatial relationships in physical environments" border="0" height="676" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/49710bf3-888e-481e-9699-f0083f0a0840.jpg" style="border: 0px currentcolor; border-image: none; display: inline; background-image: none;" title="" width="911"></a></p>


  <p>Azure IoT Hub에서 에지 디바이스에 Live Video Analytics 및 공간 분석 모듈을 쉽게 배포할 수 있습니다. 권장되는 에지 디바이스는 <a href="https://docs.microsoft.com/azure/databox-online/azure-stack-edge-gpu-overview" target="_blank">NVIDIA T4 Tensor Core GPU를 사용하는 Azure Stack Edge</a>입니다. 이 설명서에서 <a href="https://aka.ms/lva-spatial-analysis" target="_blank">Computer Vision for Spatial Analysis를 사용하여 라이브 비디오를 분석하는</a> 방법에 대해 자세히 알아볼 수 있습니다.</p>


  <h2>Intels&rsquo; OpenVINO 모델 서버를 사용하는 Live Video Analytics</h2>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/50336828-cf8a-4928-b0e5-2f182b305587.jpg"><img alt="An architecture diagram showing how Live Video Analytics can be combined with Intel’s OpenVINO Model Server and your own business logic to build custom vision apps that are optimized to run on a wide range of Intel processors." border="0" height="293" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/0beec9ee-6287-48ae-a4ad-a239e6457c4f.jpg" style="border: 0px currentcolor; border-image: none; display: inline; background-image: none;" title="" width="1024"></a></p>


  <p>IoT Edge의 Live Video Analytics 모듈을 <a href="https://aka.ms/lva-intel-ovms" target="_blank">Intel의 OpenVINO Model Server(OVMS) &ndash; AI 확장</a> 과 페어링하여 복잡하고 성능이 뛰어난 라이브 비디오 분석 솔루션을 빌드할 수 있습니다. OpenVINO Model Serveris&nbsp;는 Intel에서 실행되는 컴퓨터 비전 워크로드에 최적화된 <a href="https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit.html" target="_blank">OpenVINO 도구 키트</a>&rsquo;로 구동되는 유추 서버입니다. <a href="https://github.com/openvinotoolkit/model_server/tree/master/extras/ams_wrapper" target="_blank">확장으로</a>, HTTP 지원 및 샘플은 유추 서버와 Live Video Analytics 모듈 간에 <a href="https://docs.microsoft.com/azure/media-services/live-video-analytics-edge/use-intel-openvino-tutorial" target="_blank">비디오 프레임과 유추 결과를 쉽게 교환</a> 할 수 있도록 OVMS에 추가되어 OpenVINO 도구 키트에서 지원하는 모든 개체 검색, 분류 또는 분할 모델을 실행할 수 있습니다.</p>


  <p>유추 서버 모듈을 사용자 지정하여 <a href="https://github.com/openvinotoolkit/open_model_zoo" target="_blank">Open Model Zoo</a> 리포지토리에서 최적화된 미리 학습된 모델을 사용하고, CPU(Atom, Core, Xeon), FPGA(필드 프로그래밍 가능 게이트 배열) 및 사용 사례에 가장 적합한 VPU(비전 처리 장치)를 포함하여 애플리케이션을 변경하지 않고도 Intel 하드웨어에서 지원하는 다양한 가속 메커니즘 중에서 선택할 수 있습니다. 또한 <a href="https://www.intel.com/content/www/us/en/internet-of-things/ai-in-production/develop.html" target="_blank">개발자 키트</a> 또는 <a href="https://www.intel.com/content/www/us/en/internet-of-things/ai-in-production/scale.html" target="_blank">시장 준비</a> 솔루션과 같은 다양한 사용 사례별 Intel 기반 솔루션 중에서 선택하고 쉽게 연결할 수 있는 Live Video Analytics 플랫폼을 규모에 통합할 수 있습니다.</p>


  <p style="margin-left: 40px;"><em>&ldquo;Azure Live Video Analytics용 OpenVINO 모델 서버를 확장하여 에지에서 AI의 힘을 발휘하게 되어 기쁩니다. 이 확장은 모듈식 분석 플랫폼을 통해 복잡한 비디오 솔루션을 개발하는 프로세스를 간소화합니다. 개발자는 클라우드 애플리케이션에 대한 에지를 신속하게 빌드하고 풍부한 에코시스템을 통해 Intels&rsquo;의 광범위한 컴퓨팅 및 AI 가속기 플랫폼에 배포할 수 있습니다.&rdquo;&mdash;</em> Adam Burns, VP, Edge AI 개발자 도구, 사물 인터넷 그룹, Intel</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/5744f58e-b73f-46ce-b846-39a18171781e.jpg"><img align="left" alt="#1509 3" border="0" height="121" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/1028a828-30db-415a-a4ed-b97746321bd3.jpg" style="border:0px currentcolor; display:inline; background-image:none; float:right" title="#1509 3" width="121"></a></p>


  <p>&nbsp;</p>


  <p>&nbsp;</p>


  <p>&nbsp;</p>


  <h2>NVIDIAs&rsquo; DeepStream SDK를 사용하는 Live Video Analytics</h2>


  <p>Live Video Analytics 및 NVIDIA DeepStream SDK를 사용하여 NVIDIA GPU(그래픽 처리 장치)의 기능을 Azure Media Services, Azure Storage, Azure IoT 등의 Azure 클라우드 서비스와 결합하는 하드웨어 가속 AI 비디오 분석 앱을 빌드할 수 있습니다. 수천 개의 위치에 걸쳐 확장할 수 있고 클라우드를 통해 해당 위치의 에지 디바이스에서 비디오 워크플로를 관리할 수 있는 정교한 실시간 앱을 빌드할 수 있습니다. <a href="https://github.com/Azure/live-video-analytics/tree/master/utilities/video-analysis/deepstream" target="_blank">GitHub 관련 샘플을 탐색할</a> 수 있습니다.</p>


  <p>Live Video Analytics를 사용하여 에지 및 클라우드에 걸쳐 있는 비디오 워크플로를 빌드한 다음, DeepStream SDK를 결합하여 선택한 AI를 사용하여 비디오에서 인사이트를 추출하는 파이프라인을 빌드할 수 있습니다.</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/1df3ed02-2c2c-4395-a344-f23c96495dd4.jpg"><img alt="An architectural flow diagram that illustrate show you can use LVA to build video workflows that span the edge and cloud, and then combine DeepStream SDK to build pipelines to extract insights from video using the AI of your choice. " border="0" height="536" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/2612dd29-62da-451c-8bac-c31729507091.jpg" style="border: 0px currentcolor; border-image: none; display: inline; background-image: none;" title="" width="856"></a></p>


  <p>위의 다이어그램에서는 AI 이벤트에 의해 트리거되는 비디오 클립을 기록하여 클라우드에서 Azure Media Services 방법을 보여 줍니다. 샘플은 두 플랫폼의 강력한 디자인과 개방성에 대한 증거입니다.</p>


  <p style="margin-left: 40px;"><em>&ldquo;NVIDIA 컴퓨팅 스택에서 제공하는 NVIDIA DeepStream SDK와 Live Video Analytics의 강력한 조합은 세계적 수준의 비디오 분석 개발 및 배포를 가속화하는 데 도움이 됩니다. Microsoft와의 파트너십을 통해 모든 산업 및 사용 사례에서 에지에서 클라우드로 AI 지원 비디오 분석을 채택할 것입니다.&rdquo;&mdash;</em> Deepu Talla, NVIDIA 에지 컴퓨팅 부사장 겸 총괄 매니저</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/d4467cb5-e869-4679-bc2e-07978d337fa2.png"><img alt="#1509 5" border="0" height="83" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/e511e5f7-2aaa-4bfd-b22a-487828607353.png" style="border:0px currentcolor; display:inline; background-image:none; float:right" title="#1509 5" width="240"></a></p>


  <p>&nbsp;</p>


  <h2>&nbsp;</h2>


  <h2>Live Video Analytics는 이제 Arm에서 실행됩니다.</h2>


  <p>이제 <a href="https://docs.microsoft.com/azure/media-services/live-video-analytics-edge/deploy-iot-edge-device" target="_blank">Linux Arm64v8 디바이스의 IoT Edge에서 Live Video Analytics</a>를 실행하여 <a href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-nano/" target="_blank">NVIDIA&reg; Jetson&trade;</a> 시리즈와 같은 낮은 전력 소비, 낮은 공간 디바이스를 사용할 수 있습니다.</p>


  <h2>IoT Central Video Analytics 템플릿을 사용하여 신속하게 솔루션 개발</h2>


  <p>새로운 <a href="https://apps.azureiotcentral.com/build/new/video-analytics-om" target="_blank">IoT Central 비디오 분석 템플릿</a>은 카메라와 Azure 클라우드 서비스 간의 게이트웨이 역할을 하도록 Azure IoT Edge 디바이스의 설정을 간소화합니다. Azure Live Video Analytics 비디오 유추 파이프라인과 Intel의 OpenVINO Model Serveran&mdash; AI 유추 서버를 통합하여 고객이 코드 없이 몇 시간 안에 완전히 작동하는 엔드투엔드 솔루션을 빌드할 수 있도록 합니다. &rsquo;Azure Media Services 파이프라인과 완전히 통합되어 클라우드에서 분석된 비디오를 캡처, 녹화 및 재생할 수 있습니다.</p>


  <p>템플릿은 IoT Central 게이트웨이, IoT Edge의 Live Video Analytics, Intel OpenVINO 모델 서버 및 에지 디바이스에 ONVIF 모듈과 같은 IoT Edge 모듈을 설치합니다. 이러한 모듈은 IoT Central 애플리케이션이 디바이스를 구성 및 관리하고, 카메라에서 라이브 비디오 스트림을 수집하고, 차량 또는 사람 감지와 같은 AI 모델을 쉽게 적용하는 데 도움이 됩니다. 클라우드에서 동시에 라이브 비디오 피드의 관련 부분을 기록하고 스트리밍하는 Azure Media Services 및 Azure Storage. 시작하는 방법에 대한 전체 개요 및 지침은 <a href="https://aka.ms/iotshow/232/youtube" target="_blank">IoT Show 에피소드</a> 및 <a href="https://aka.ms/iotshow/VisionAIInIoTCentral" target="_blank">관련 블로그 게시물을</a> 참조하세요.</p>


  <h2>Live Video Analytics에서 Cognitive Services Custom Vision 모델 통합</h2>


  <p>많은 조직에서는 이미 비디오 데이터를 캡처하기 위해 많은 수의 카메라를 배포했지만 스트림에 대한 의미 있는 분석을 수행하지는 않습니다. Live Video Analytics가 등장하면서 라이브 비디오 피드에 기본 이미지 분류 및 개체 검색 알고리즘을 적용하면 진정한 유용한 인사이트를 확보하고 비즈니스를 더 안전하고, 더 안전하고, 효율적이고, 궁극적으로 더 수익성 있게 만들 수 있습니다. 잠재적인 시나리오는 다음과 같습니다.</p>


  <ul>
      <li>산업/제조 공장의 직원이 안전과 현지 규정 준수를 보장하기 위해 하드 모자를 쓰고 있는지 감지합니다.</li>
      <li>컨베이어 벨트에서 제품 계산 또는 결함이 있는 제품 감지</li>
      <li>온-프레미스에서 원치 않는 개체(사람, 차량 등)가 있는지 감지하고 보안에 알립니다.</li>
      <li>소매점 선반 또는 공장 부품 선반에서 재고가 부족하거나 재고가 부족한 제품을 감지합니다.</li>
  </ul>


  <p>이러한 작업을 수행하기 위해 AI 모델을 처음부터 개발하고 대규모로 배포하여 에지의 라이브 비디오 스트림에서 작업하려면 사소한 작업이 수반됩니다. 확장 가능하고 신뢰할 수 있는 방식으로 수행하면 훨씬 더 어렵고 비용이 많이 듭니다. Cognitive <a href="https://azure.microsoft.com/services/cognitive-services/custom-vision-service/" target="_blank">Services Custom Vision</a>과 <a href="https://azure.microsoft.com/services/media-services/live-video-analytics/" target="_blank">IoT Edge의 Live Video Analytics</a><a href="https://docs.microsoft.com/azure/media-services/live-video-analytics-edge/custom-vision-tutorial" target="_blank">를 통합</a>하면 몇 분에서 몇 시간 안에 이러한 모든 시나리오에 대한 작업 솔루션을 구현할 수 있습니다.</p>


  <p>먼저 Custom Vision 서비스에 미리 레이블이 지정된 이미지를 업로드하여 컴퓨터 비전 모델을 빌드하고 학습하는 것으로 시작합니다. 따라서 데이터 과학, 기계 학습 또는 AI에 대한 사전 지식이 필요하지 않습니다&rsquo;. 그런 다음 Live Video Analytics를 사용하여 학습된 사용자 지정 모델을 에지의 컨테이너로 배포하고 비용 효율적인 방식으로 여러 카메라 스트림을 분석할 수 있습니다.</p>


  <h2>Live Video Analytics 기반 제조 현장 참조 앱</h2>


  <p>Azure Stack 팀과 협력하여 데이터 과학 지식 없이도 비전 모델을 쉽게 학습하고 배포할 수 있는 턴키 애플리케이션인 <a href="https://aka.ms/LVAwithASE-ManufacturingAI" target="_blank">Factory.AI</a> 솔루션을 발전시켰습니다. 이 솔루션에는 개체 계산, 직원 안전, 결함 감지, 기계 정렬 오류, 도구 감지 및 부품 확인 기능이 포함됩니다. 이러한 모든 시나리오는 Azure Stack Edge 디바이스에서 실행되는 Live Video Analytics의 통합을 통해 제공됩니다.</p>


  <p>또한 Factory.AI 솔루션을 통해 고객은 <a href="https://docs.microsoft.com/azure/cognitive-services/custom-vision-service/quickstarts/image-classification?pivots=programming-language-csharp" target="_blank">Custom Vision SDK</a>를 사용하여 고유한 사용자 지정 <a href="https://docs.microsoft.com/azure/cognitive-services/custom-vision-service/custom-vision-onnx-windows-ml" target="_blank">ONNX 모델을</a> 학습하고 배포할 수 있습니다. 사용자 지정 모델이 에지에 배포되면 참조 앱은 높은 프레임 속도의 정확한 추론을 위해 <a href="https://docs.microsoft.com/azure/media-services/live-video-analytics-edge/analyze-live-video-use-your-grpc-model-quickstart?pivots=programming-language-csharp" target="_blank">Live Video Analytics의 gRPC</a> 를 활용합니다. <a href="https://myignite.microsoft.com/sessions/3fc1dd73-1979-4631-a536-8f693e988dfd" target="_blank">Microsoft Ignite에서</a> 또는 <a href="https://github.com/Azure-Samples/azure-intelligent-edge-patterns/tree/master/factory-ai-vision" target="_blank">Azure 지능형 에지 패턴 페이지를</a> 방문하여 제조 참조 앱에 대해 자세히 알아볼 수 있습니다.</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/1fefde35-a95d-442f-ac9f-b92c8b7de9ef.jpg"><img alt="An architectural flow diagram illustrating how to configure the Factory.ai, powered by the integration of LVA running on Azure Stack Edge devices." border="0" height="659" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/00a9ea66-52f5-44df-bf25-fcf8825c559f.jpg" style="border: 0px currentcolor; border-image: none; display: inline; background-image: none;" title="" width="1024"></a></p>


  <h2>오늘 시작</h2>


  <p>끝으로, Wed&rsquo;는 이미 IoT Edge의 Live Video Analytics 미리 보기에 참여하고 있는 모든 분들께 감사드립니다. 클라우드와 에지 모두에서 비디오 분석을 통해 성공을 거둘 수 있도록 엔지니어링 팀에 지속적인 피드백을 제공해 주셔서 감사합니다. 이 기술을 접하는 분들을 위해, Wed&rsquo;는 다음과 같은 유용한 리소스로 오늘 시작하는 것이 좋습니다.</p>


  <ul>
      <li>Live Video Analytics <a href="https://azure.microsoft.com/resources/videos/live-video-analytics/" target="_blank">소개 비디오를</a> 시청하세요.</li>
      <li><a href="https://azure.microsoft.com/services/media-services/live-video-analytics/" target="_blank">제품 세부 정보 페이지에서</a> 자세한 정보를 찾습니다.</li>
      <li><a href="https://azure.microsoft.com/en-us/resources/videos/live-video-analytics-demo/" target="_blank">Live Video Analytics 데모를</a> 시청하세요.</li>
      <li>Azure 무료 평가판 계정으로 지금 <a href="https://docs.microsoft.com/azure/media-services/live-video-analytics-edge/overview" target="_blank">새로운 Live Video Analytics 기능을</a> 사용해 보세요.</li>
      <li><a href="https://techcommunity.microsoft.com/t5/azure-media-services/bg-p/AzureMediaServices" target="_blank">Media Services Tech Community</a> 등록하고 엔지니어링 팀에서 향후 새로운 기능에 대해 직접 듣고 피드백을 제공하고 로드맵 요청에 대해 논의합니다.</li>
      <li><a href="https://azuremarketplace.microsoft.com/marketplace/apps/azure-media-services.live-video-analytics-edge?tab=Overview" target="_blank">Azure Marketplace</a>에서 IoT Edge의 Live Video Analytics를 다운로드합니다.</li>
      <li><a href="https://github.com/Azure-Samples/live-video-analytics-iot-edge-csharp" target="_blank">C#</a> 및 <a href="https://github.com/Azure-Samples/live-video-analytics-iot-edge-python" target="_blank">Python</a> 코드 샘플을 빠르게 시작합니다.</li>
      <li><a href="https://docs.microsoft.com/azure/media-services/live-video-analytics-edge/overview" target="_blank">제품 설명서를 검토합니다</a>.</li>
      <li>Live Video Analytics 오픈 소스 프로젝트에 대한 <a href="https://github.com/Azure/live-video-analytics" target="_blank">GitHub 리포지토리</a>를 검색합니다.</li>
      <li>질문이 있는 경우 문의하세요 <a href="mailto:amshelp@microsoft.com">amshelp@microsoft.com</a> .</li>
  </ul>


  <hr>

  <p>Intel, Intel 로고, Atom, Core, Xeon 및 OpenVINO는 Intel Corporation 또는 그 자회사의 등록 상표입니다.</p>


  <p>NVIDIA 및 NVIDIA 로고는 미국 및/또는 기타 국가에서 NVIDIA Corporation의 등록 상표 또는 상표입니다. 다른 회사 및 제품 이름은 연결된 각 회사의 상표일 수 있습니다.</p>
