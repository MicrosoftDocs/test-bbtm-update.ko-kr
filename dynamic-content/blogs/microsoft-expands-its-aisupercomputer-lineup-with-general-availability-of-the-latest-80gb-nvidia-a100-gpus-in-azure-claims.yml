### YamlMime:Yaml
ms.openlocfilehash: ec5681062454fa2074fd60bf79c59bfbf4f715d2
ms.sourcegitcommit: d03bdc7fe5447adb6530886aab848b75fe8fa8ee
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 03/11/2022
ms.locfileid: "139907714"
Slug: microsoft-expands-its-aisupercomputer-lineup-with-general-availability-of-the-latest-80gb-nvidia-a100-gpus-in-azure-claims
Title: Microsoft는 Azure에서 최신 80GB NVIDIA A100 GPU의 일반 공급으로 AI 슈퍼컴퓨터 라인업을 확장하며 TOP500 슈퍼컴퓨터 목록에서 4위를 차지했습니다.
Summary: 오늘 Microsoft는 NVIDIA A100 Tensor Core 80GB GPU를 갖춘 NDm A100 v4 시리즈인 Azure의 새로운 VM(가상 머신) 시리즈의 일반 공급이 발표되었습니다. 이렇게 하면 퍼블릭 클라우드에서 Azure 리더십 수준의 AI 슈퍼컴퓨팅 확장성이 확장되고, 원래 ND A100 v4 인스턴스의 6월 일반 공급에 기반하며, 상위 500개 슈퍼컴퓨팅 목록에서 4개의 공식 위치를 주장하는 Azure ND A100 v4 VM을 사용하여 다른 퍼블릭 클라우드를 먼저 추가합니다.
Content: >-
  <p>오늘 Microsoft는 NVIDIA A100 Tensor Core 80GB GPU를 갖춘 NDm A100 v4 시리즈인 Azure의 새로운 VM(가상 머신) 시리즈의 일반 공급이 발표되었습니다. 이렇게 하면 퍼블릭 클라우드에서 Azure 리더십 수준의 AI 슈퍼컴퓨팅 확장성이 확장되고, <a href="https://azure.microsoft.com/en-us/blog/azure-announces-general-availability-of-scaleup-scaleout-nvidia-a100-gpu-instances-claims-title-of-fastest-public-cloud-super/" target="_blank">원래 ND A100 v4 인스턴스의 6월 일반 공급</a>에 기반하며, 상위 500개 슈퍼컴퓨팅 목록에서 4개의 공식 위치를 주장하는 Azure ND A100 v4 VM을 사용하여 다른 퍼블릭 클라우드를 먼저 추가합니다. 이 중요 시점은 NVIDIA Quantum InfiniBand 네트워킹을 사용하여 클래스를 선도하는 설계로, In-Network 컴퓨팅, 각 GPU에 대해 200GB/s 및 GPUDirect RDMA 및 모든 새로운 PCIe Gen 4.0 기반 아키텍처를 제공합니다.</p>


  <p>우리는 대규모 AI 모델의 시대에 살고 있으며, 대규모 컴퓨팅에 대한 수요는 계속 증가하고 있습니다. <a href="https://docs.microsoft.com/en-us/azure/virtual-machines/nda100-v4-series" target="_blank">원래 ND A100 v4 시리즈는 각각 40GB</a>의 HBM2 메모리가 장착된 NVIDIA A100 Tensor Core GPU를 갖추고 있으며, 새로운 NDm A100 v4 시리즈는 80GB로 두 배로 증가하며, 오늘날&rsquo; 대부분의 데이터 집약적 워크로드에서 GPU 메모리 대역폭이 30% 증가합니다. 또한 가상 머신에서 사용할 수 있는 RAM은 VM당 1,900GB로 증가하여 큰 데이터 세트를 사용하는 고객이 새로운 데이터 관리 기술, 더 빠른 검사점 지정 등을 지원하기 위해 메모리 용량의 비례 증가를 모델링할 수 있습니다.</p>


  <p>메모리가 높은 NDm A100 v4 시리즈는 모든 기업이 경쟁 우위로 사용할 수 있는 기회를 만들어 대중에게 AI-Supercomputer 힘을 제공합니다. 최첨단 AI 고객은 대규모 프로덕션 AI 및 기계 학습 워크로드를 위해 40GB ND A100 v4 VM과 80GB NDm A100 v4 VM을 모두 사용하고 있으며, 연구 및 제품을 위한 OpenAI, 선도적인 AI 연구를 위한 메타, 포괄적인 AI 기반 음성 지원 솔루션에 대한 뉘앙스, 대규모 인지 과학 모델 학습을 위한 수많은 Microsoft 내부 팀 등 뛰어난 성능과 확장성을 보고 있습니다.  그리고 더 많은.</p>


  <p style="margin-left: 40px;"><em>&ldquo;일부 연구 모델은 수십 또는 수백 개의 NVIDIA GPU를 사용하여 최적으로 학습할 수 있으며, Azures&rsquo; ND A100 v4 제품은 대규모 AI 모델의 증가하는 학습 요구를 해결하는 데 도움이 됩니다. 최신 학습 기술에는 강력한 가속기뿐만 아니라 이들 간의 통신 패브릭이 필요하며, 각 NVIDIA A100 GPU 간의 GPUDirect RDMA를 사용한 NVIDIA Quantum InfiniBand 200GB/s 네트워킹의 Azure&rsquo; 구현을 통해 PyTorch와 이미 잘 알고 있는 통신 라이브러리&rsquo;를 수정하지 않고 사용할 수 있습니다.&rdquo;&mdash;</em> Myle Ott, Meta AI Research 연구 엔지니어</p>


  <p style="margin-left: 40px;"><em>&ldquo;대화형 AI의 혁신 속도는 부분적으로 실험적 처리량과 소요 시간에 의해 제어됩니다. ND A100 v4를 사용하면 NDv2&nbsp; 와 비교해 반 시간 동안 실험을 완료할 수 있을 뿐만 아니라 실험당 PAYG 비용을 크게 절감할 수 있습니다. 이것은 우리의 드래곤 앰비언트 eXperience 기술의 발전을위한 중요한 가속이 될 것입니다.&rdquo;&mdash;</em> 폴 보질라, 부사장, 뉘앙스 커뮤니케이션의 중앙 연구&nbsp;</p>


  <p style="margin-left: 40px;"><em>&quot;우리는 최근에 발표된 <a href="https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/" target="_blank">MT-NLG 530B</a>와 같은 대규모 AI 모델의 시대에 살고 있습니다. 이 크기의 최첨단 튜링 모델을 학습하는 것은 기본 학습 인프라에 전례 없는 과제를 제시하는 동시에 가속, 네트워킹, 안정성 및 가용성에 대한 막대를 크게 높였습니다. <a href="https://www.nvidia.com/en-us/on-demand/session/supercomputing2020-sc2019/" target="_blank">NVIDIA Selene 슈퍼컴퓨팅</a> 인프라와의 공동 연구 노력과 마찬가지로 80GB의 높은 대역폭 메모리를 사용하는 Azure NDm A100 v4는 최대 매개 변수 수를 늘리고 필요한 노드 수를 줄이는 등 모델을 확장하는 데 많은 기존 제한을 제거할 수 있습니다. 성능과 민첩성은 AI 발전 경쟁에서 Azure 고객에게 심각한 경쟁 우위를 제공할 수 있습니다.&quot;&mdash;</em> Microsoft Turing</p>


  <p>데이터 집약적 GPU 컴퓨팅 워크로드를 위한 새로운 고메모리 NDm A100 v4는 최신 확장 및 스케일 아웃 GPU 가속기 기술을 퍼블릭 클라우드에 신속하게 채택하고 전달하겠다는 Microsoft&rsquo;의 의지를 재확인합니다.</p>


  <p>새 Azure NDm A100 v4 플랫폼을 사용하여 빌드, 분석 및 검색할 항목을&rsquo; 확인할 수&rsquo; 있습니다.</p>


  <p>&nbsp;</p>


  <table border="1" cellpadding="2" cellspacing="0">
      <tbody>
          <tr>
              <td valign="top">
              <blockquote dir="ltr" style="margin-right: 0px;">
              <p><strong>크기</strong></p>
              </blockquote>
              </td>
              <td valign="top"><strong>물리적 CPU 코어</strong></td>
              <td valign="top"><strong>호스트 메모리(GB)</strong></td>
              <td valign="top"><strong>GPU</strong></td>
              <td valign="top"><strong>로컬 NVMe 임시 디스크</strong></td>
              <td valign="top"><strong>NVIDIA Quantum InfiniBand 네트워크</strong></td>
              <td valign="top"><strong>Azure 네트워크</strong></td>
          </tr>
          <tr>
              <td valign="top">
              <p align="left">Standard_ND96amsr</p>

              <p align="left">_A100_v4</p>
              </td>
              <td valign="top">96</td>
              <td valign="top">1,900GB</td>
              <td valign="top">8 x 80GB NVIDIA A100</td>
              <td valign="top">6,400GB</td>
              <td valign="top"> 200GB/s</td>
              <td valign="top">40Gbps</td>
          </tr>
      </tbody>
  </table>


  <h2>자세한 정보</h2>


  <ul>
      <li>고객에게 &quot;<a href="https://azure.microsoft.com/en-us/blog/bringing-ai-supercomputing-to-customers/" target="_blank">AI 슈퍼컴퓨팅을 제공하는</a> 블로그 &quot; 에서 자세히 알아보세요.</li>
      <li>Azure 보기: girish Bablani, Microsoft CVP, NVIDIA의 데이터 센터 비즈니스 부사장 및 GM의 이안 벅(Ian Buck)이 <a href="https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s32779/" target="_blank">High-Ambition AI 및 HPC를 통해 전 세계에 힘을 실어주</a>세요.</li>
      <li><a href="https://docs.microsoft.com/en-us/azure/virtual-machines/sizes-hpc" target="_blank">Azure HPC</a> 가상 머신의 고성능 컴퓨팅 가상 머신 크기에 대한 자세한 내용을 확인하세요.</li>
      <li><a href="https://aka.ms/aiatscale/" target="_blank">Microsoft AI at Scale</a>.</li>
      <li>최신 <a href="https://techcommunity.microsoft.com/t5/azure-compute/microsoft-announces-new-ndm-a100-v4-public-ai-supercomputers-and/ba-p/2966848" target="_blank">TOP500 결과를</a> 참조하세요.</li>
  </ul>
