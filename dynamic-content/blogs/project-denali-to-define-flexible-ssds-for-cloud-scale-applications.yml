### YamlMime:Yaml
ms.openlocfilehash: 3fef1751fe1aed176403398e992c14dfea8dd85f
ms.sourcegitcommit: d03bdc7fe5447adb6530886aab848b75fe8fa8ee
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 03/11/2022
ms.locfileid: "139900963"
Slug: project-denali-to-define-flexible-ssds-for-cloud-scale-applications
Title: Project Denali를 사용하여 클라우드 규모 애플리케이션에 대한 유연한 SSD 정의
Summary: 지난 9월, SNIA의 Storage 개발자 컨퍼런스에서 Project Denali SSD의 프로토타입을 발표했습니다. Project Denali 드라이브는 ...의 워크로드에 최적화하는 데 필요한 유연성을 제공합니다.
Content: >-
  <p>지난 9월, SNIAs&rsquo; Storage 개발자&rsquo; 컨퍼런스에서 Project 데날리 SSD의 프로토타입을 발표했습니다. Project Denali 드라이브는 다양한 클라우드 애플리케이션의 워크로드에 최적화하는 데 필요한 유연성, NAND 플래시 메모리 및 애플리케이션 디자인의 신속한 혁신과 보조를 맞추기 위한 단순성, 클라우드에서 매우 일반적인 다중 테넌트 하드웨어에 필요한 규모를 제공합니다.&nbsp;</p>


  <p>이번 달에는&rsquo; OCP(Open Compute Project) Us Summit 2018에 참석하여 데날리 드라이브를 Project 인터페이스를 정의하는 사양의 공식화를 시작합니다. 이 사양이 적용되면 하드웨어 공급업체와 클라우드 공급자 모두 최종 제품을 빌드하고 릴리스할 수 있습니다. 사양은 NAND를 관리하고 데이터 배치를 관리하는 역할을 구분하는 새 추상화가 정의됩니다. 전자는 NAND에 가까운 하드웨어 &ndash; 와 모든 새로운 세대의 NAND와 함께 자신을 재창조하는 제품에 남아있을 것입니다. NAND 관리 알고리즘에서 분리된 후자는 혁신에 대한 자체 일정을 따를 수 있으며&rsquo; NAND 세대만 추적하는 제품 주기에 의해 도입된 버그가 발생하기 쉽습니다.&nbsp;</p>


  <p>필요한 리팩터링된 알고리즘을 사용하는 사양은 드라이브 펌웨어와 호스트 소프트웨어 모두에서 혁신을 새롭게 촉진합니다. Project Denali 아키텍처&rsquo;를 개발하는 동안 다음 네 가지 목표에 집중했습니다.</p>


  <ol>
   <li>혁신 민첩성을 위한 유연한 아키텍처: 워크로드별 최적화, 클라우드 서비스 구성 요소로 관리되는 FTL</li>
   <li>새로운 NAND 세대의 신속한 지원: NAND는 무어&rsquo;스 법을 따릅니다. SSD: 사전 조건까지의 시간, 수백 개의 워크로드</li>
   <li>대규모 공유 디바이스에서 Azure(&gt;600개 서비스), Bing, Exchange, O365 등 광범위한 애플리케이션을 지원합니다. 드라이브당 최대 수백 명의 사용자</li>
   <li>확장하려면 다중 공급업체 지원 &amp; 공급망 다양성이 필요합니다. Azure는 전 세계 38개 지역에서 다른 클라우드 공급자보다 더 많이 운영됩니다.</li>
  </ol>


  <h2>증가하는 격차 해결</h2>


  <p>Project Denali 아키텍처의 기본 드라이버 중 하나는 하드웨어와 애플리케이션 간의 쓰기 및 회수 크기가 일치하지 않는 것입니다. 클라우드 규모 애플리케이션은 컴퓨터의 테넌트 수를 확장하는 경향이 있지만 하드웨어는 아키텍처 매개 변수의 크기를 확장합니다. 서버의 코어 수가 증가함에 따라 단일 컴퓨터는 더 많은 VM을 지원할 수 있습니다. 스토리지 서버가 용량을 늘리면 일반적으로 각각을 백 엔드로 사용하는 테넌트 수가 증가합니다. 몇 가지 주목할 만한 예외가 있지만 클라우드 하드웨어가 이러한 다중 테넌트 디자인을 효율적으로 처리할 수 있는 충분한 유연성을 제공해야 합니다.&nbsp;&nbsp;&nbsp;</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/b003cffa-ca02-4a79-9d57-0acfdd2b5ad6.png"><img align="right" alt="Address Map" border="0" height="220" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/4ef33dca-2fb9-4cdb-ba92-5f9863db8b38.png" style="margin: 20px 0px 0px; border: 0px currentcolor; border-image: none; float: right; display: inline; background-image: none;" title="주소 MapSSD" width="274"></a> 캐싱은&rsquo; 이러한 나누기를 더욱 증가합니다. SSD&rsquo; 병렬 처리를 최대한 활용하기 위해 컨트롤러는 현재 드라이브에서 약 4MB의 모든 평면에서 하나의 플래시 페이지를 채우기에 충분한 쓰기를 수집합니다. 데이터가 여러 애플리케이션에서 가져온 경우 드라이브는 예측하거나 제어하기 어려운 방식으로 데이터를 혼합합니다. 디바이스가 GC(가비지 수집)를 수행할 때 문제가 됩니다. 이 때까지 일부 애플리케이션은 데이터를 해제하거나 다른 애플리케이션보다 빠른 속도로 업데이트할 수 있습니다. 효율적인 GC를 위해 단일 블록에서 데이터를 해제하거나 업데이트하는 것이 이상적입니다. 그러나 캐싱이 유효 블록 크기를 1GB로 가져오는 경우&rsquo; 호스트가 다음 테넌트에 대한 쓰기를 서비스하기 전에 한 테넌트에 대한 드라이브에 충분한 데이터를 발급할 가능성은 매우 낮습니다.</p>


  <p>이 캐싱 디자인은 단일 워크로드( 순차 쓰기)에 대한 최적화로 볼 수도 있습니다. &rsquo;SSD 제조업체가 이 워크로드 &ndash; 최적화를 HDD에 대상으로 지정하는 것은 놀라운 일이 아니며, SSD의 순차적 쓰기는 임의 쓰기 워크로드(OP가 7%인 완전 드라이브)보다 지속적으로 4-5배 더 빠릅니다. 그러나 실제 클라우드는<a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/3fd85d0d-fdbe-4e93-9e0a-de1380627293.png"><img align="left" alt="Die Capacity" border="0" height="149" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/b2b7099f-632e-439b-8a61-16f301af6805.png" style="margin: 11px 0px 0px; border: 0px currentcolor; border-image: none; float: left; display: inline; background-image: none;" title="Die Capacitypplications" width="268"></a>는 전체 드라이브에서 순차적인 쓰기로 거의 구성되지 않습니다. 훨씬 더 일반적인 것은 스토리지의 TB당 수백 개의 순차적 쓰기 스트림으로 구성된 워크로드입니다.&nbsp;&nbsp;</p>


  <p>관의 마지막 손톱은 시간이 지남에 따라 플래시 블록 크기가 어떻게 변하는지에 있습니다. NAND 플래시의 밀도가 발전하면 일반적으로 블록의 크기가 증가합니다. 이러한 이유로 호스트가 향후 디자인에서 네이티브 블록 크기에 데이터를 배치할 수 있는 액세스 권한을 갖는 것이 훨씬 더 중요합니다.</p>


  <h2>클라우드 하드웨어를 위한 유연한 아키텍처</h2>


  <p>위에서 설명한 문제를 해결하기 위해 SSD&rsquo; 플래시 변환 계층 및 클라우드 애플리케이션에서 알고리즘을 자세히 살펴보고 다음 두 가지 범주를 찾았습니다.</p>


  <p><strong>로그 관리자는</strong> 임의 쓰기를 받고 이러한 쓰기를 하나 이상의 순차 쓰기 스트림으로 변환하고 스토리지 스택의 다음 계층으로 내보냅니다. 이렇게 하려면 로그 관리자는 일반적으로 주소 맵을 유지 관리하며 가비지 수집을 수행해야 합니다.&nbsp;</p>


  <p><strong>미디어 관리자는</strong> 특정 세대의 미디어를 위해 작성되며 오류가 발생하는 위치 및 시기 패턴 및 데이터 보존 시간과 같은 미디어의 물리적 속성에 따라 ECC, 읽기 재시도 및 데이터 새로 고침과 같은 알고리즘을 구현합니다.&nbsp;</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/c89839e5-6b9c-4217-bce8-5ce2432b9559.png"><img align="right" alt="SSD" border="0" height="233" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/52b46c80-76d0-4039-a377-c3ad39ae1ca6.png" style="margin: 0px 0px 0px 10px; border-image: none; float: right; display: inline; background-image: none;" title="SSDThese" width="299"></a> 두 구성 요소는 여러 위치에 표시됩니다. 표준 SSD는 둘 다 혼합되고, 초기 오픈 채널 아키텍처는 호스트가 둘 다 구현해야 합니다. 대부분의 시스템에는 SanDisk가 내 로그에 로그를 쌓지 않도록&rsquo; 경고한 시나리오가 서로 위에 &ndash; 쌓여 있는 로그 관리자가 2~3명 이상 있습니다. Project Denali에서는 드라이브에 미디어 관리자를 배치하고 호스트에 완전히 캡슐화된 단일 로그 관리자를 사용하여 로그 관리자와 미디어 관리자 간에 명확한 나누기를 제안합니다.&nbsp;</p>


  <h2>성공적인 프로토타입</h2>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/ddbf8cd0-7818-4950-b5b0-6d4298111bce.jpg"><img align="left" alt="Prototype" border="0" height="211" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/1cc5ac1f-3d37-409a-aa5d-8bb5bf3abd92.jpg" style="margin: 7px 16px 0px 0px; border-image: none; float: left; display: inline; background-image: none;" title="프로토타입" width="119"></a> 개발의 첫 번째 단계에서는 CNEX와 협업하여 프로토타입 시스템을 빌드했습니다. 인터페이스 변경으로 스토리지 스택의 여러 계층에서 최적화할 수 있는 기회가 열리지만 AzureS OS에서 펌웨어와 가장 낮은 수준의 디바이스 드라이버&rsquo;라는 두 가지 구성 요소만 수정했습니다.&nbsp; 이를 통해 아이디어를 빠르게 평가할 수 있고, 레거시 애플리케이션에 대한 인프라를 제공하며, 향후 최적화를 위한 시스템을 설정합니다.&nbsp;</p>


  <p>사전 최적화 결과가 예상보다 좋았습니다. 일반적으로 드라이브에 있는 메모리, 쓰기 증폭 및 CPU 오버헤드는 예상대로 호스트로 이동했으며 시스템&rsquo; 처리량 및 대기 시간은 표준 SSD보다 약간 더 좋았습니다.&nbsp;</p>


  <p>우리는 Denali 그룹을 통해 앞으로 몇 달 안에 Denali 사양을 마무리하고 올해 말에 이 사양을 광범위하게 사용할 수 있도록 할 계획입니다. 플래시 변환 계층을 리팩터링하면 스토리지 스택 전반에서 혁신을 촉진할 수 있으며, 새 인터페이스에 대해 자세히 알아보도록 초대합니다.&nbsp;</p>
