### YamlMime:Yaml
ms.openlocfilehash: 8953ebdd6d446a3d2fdabb931f6b9e69845c6b28
ms.sourcegitcommit: d03bdc7fe5447adb6530886aab848b75fe8fa8ee
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 03/11/2022
ms.locfileid: "139908855"
Slug: microsoft-makes-it-easier-to-build-popular-language-representation-model-bert-at-large-scale
Title: Microsoft를 사용하면 널리 사용되는 언어 표현 모델 BERT를 대규모로 쉽게 빌드할 수 있습니다.
Summary: 오늘 우리는 고객이 자신의 조직에 대한 BERT 대형 모델의 학습 사용자 지정 버전의 힘을 잠금을 해제 할 수 있도록, Azure Machine Learning 작동 코드를 포함하여 Bing 팀에 의해 만들어진 BERT (변압기에서 양방향 인코더 표현)를 미리 학습하는 우리의 조리법의 오픈 소싱을 발표하고있다. 이를 통해 개발자와 데이터 과학자는 BERT를 넘어 고유한 범용 언어 표현을 빌드할 수 있습니다.
Content: >-
  <p><em>이 게시물은 Rangan Majumder, 그룹 프로그램 관리자, Bing 및 막심 루키야노프, 수석 프로그램 관리자, Azure Machine Learning 의해 공동 저술됩니다.</em></p>


  <p>오늘 우리는 고객이 자신의 데이터를 사용하여 BERT 대형 모델의 학습 사용자 지정 버전의 힘을 잠금을 해제 할 수 있도록, <a href="https://azure.microsoft.com/en-us/services/machine-learning-service/">Azure Machine Learning</a> 작동하는 코드를 포함하여 Bing 팀이 구축 한 BERT (변압기에서 양방향 인코더 표현)를 미리 학습하는 우리의 조리법의 오픈 소싱을 발표하고있다. 이를 통해 개발자와 데이터 과학자는 BERT를 넘어 고유한 범용 언어 표현을 빌드할 수 있습니다.</p>


  <p>자연어 처리 영역은 지난 몇 년 동안 BERT 중 하나와 함께 혁신의 놀라운 금액을 보았다. Google AI 언어 연구에서 만든 언어 표현인 <a href="https://arxiv.org/abs/1810.04805"> BERT</a>는 언어의 복잡성을 포착하는 기능을 크게 발전시켰고 텍스트 분류, 추출 및 질문 답변과 같은 많은 자연어 응용 프로그램에 대한 예술의 상태를 향상시켰습니다. 이 새로운 언어 표현을 만들면 개발자와 데이터 과학자가 BERT를 디딤돌로 사용하여 특수 언어 작업을 해결하고 자연어 처리 시스템을 처음부터 빌드할 때보다 훨씬 더 나은 결과를 얻을 수 있습니다.</p>


  <p>BERT의 광범위한 적용 가능성은 대부분의 개발자와 데이터 과학자가 처음부터 새 데이터를 사용하여 새 버전을 빌드하는 대신 미리 학습된 BERT 변형을 사용할 수 있음을 의미합니다. 도메인&rsquo; 데이터가 원래 모델&rsquo; 데이터와 유사한 경우 이 솔루션은 합리적인 솔루션이지만 새 문제 공간으로 넘어갈 때 동급 최고의 정확도를 제공하지 않습니다. 예를 들어 의료 노트 분석을 위한 모델을 학습하려면 의료 도메인에 대한 깊은 이해가 필요하며, 경력 권장 사항은 일자리와 후보자에 대한 대규모 텍스트 모음의 인사이트에 따라 달라지며, 법적 문서 처리를 위해서는 법적 도메인 데이터에 대한 교육이 필요합니다. 이러한 경우 NLP(자연어 처리) 알고리즘의 정확도를 최대화하려면 미세 조정을 넘어 BERT 모델을 미리 학습해야 합니다.</p>


  <p>또한 BERTs&rsquo; 정확도 이상으로 언어 표현을 발전시키기 위해 사용자는 모델 아키텍처, 학습 데이터, 비용 함수, 작업 및 최적화 루틴을 변경해야 합니다. 이러한 모든 변경 내용은 큰 매개 변수 및 학습 데이터 크기에서 탐색해야 합니다. BERT-large의 경우 3억 4천만 개의 매개 변수가 있고 25억 개 이상의 Wikipedia 및 8억 개의 BookCorpus 단어를 학습시켰기 때문에 상당히 상당할 수 있습니다. 딥 러닝 기반 NLP 모델을 학습하는 데 사용되는 가장 일반적인 하드웨어인 GPU(그래픽 처리 장치)를 통해 이를 지원하려면 기계 학습 엔지니어가 이러한 대규모 모델을 학습하기 위한 분산 학습 지원이 필요합니다. 그러나 이러한 분산 환경을 구성하는 복잡성과 취약성으로 인해 전문가 조정조차도 학습된 모델의 열등한 결과로 끝날 수 있습니다.</p>


  <p>이러한 문제를 해결하기 위해 Microsoft는 Azure에서 BERT 대형 모델의 사용자 지정 버전을 학습하기 위한 첫 번째 종류의 엔드투엔드 레시피를 오픈 소싱하고 있습니다. 전반적으로 이것은 안정적이고 예측 가능한 레시피로, 개발자와 데이터 과학자가 스스로 탐색을 시도할 수 있는 최적의 레시피입니다.</p>


  <p><em>&ldquo;BERT 미세 조정은 Bing 검색 관련성에 중요한 다양한 작업의 품질을 개선하는 데 정말 도움이되었다,&rdquo; Rangan Majumder말한다, Bing 그룹 프로그램 관리자, 누가이 작품의 공개 소싱을 주도.&nbsp; &ldquo; 그러나 기본 데이터가 원래 corpus BERT와 다른 일부 작업이 미리 학습되었고 작업 및 모델 아키텍처 수정을 실험하려고 했습니다.&nbsp; 이러한 탐사를 가능하게 하기 위해 과학자와 연구원 팀은 GPU에서 BERT를 미리 학습시키는 방법을 해결하기 위해 열심히 노력했습니다. 그런 다음 향상된 표현을 빌드하여 BERT를 통해 내부 작업에 대한 정확도를 훨씬 높일 수 있습니다.&nbsp; 우리는 커뮤니티가 우리의 경험을 복제하고 그들의 요구를 충족시키는 새로운 방향으로 확장 할 수 있도록 Bing 작업을 오픈 소스로 열게되어 기쁩니다.&rdquo;</em></p>


  <p><em>&ldquo;GPU의 원래 BERT 릴리스와 동일한 품질로 수렴하는 교육을 받는 것은 사소한 것이 아니었다고&rdquo; Bing 응용 과학 관리자 인 Saurabh Tiwary는 말합니다.&nbsp; &ldquo; BERT를 미리 학습하려면 대규모 계산 및 메모리가 필요합니다. 즉, 계산을 여러 GPU에 분산해야 했습니다. 그러나 최종 결과 모델의 수렴 및 품질 측면에서 예측 가능한 동작으로 비용 효율적이고 효율적인 방식으로 이 작업을 수행하는 것은 매우 어려웠습니다. 우리는&rsquo; 다른 사람들이 우리의 노력의 혜택을 누릴 수 있도록 분산 교육 프로세스를 단순화하기 위해 한 작업을 발표했습니다.&rdquo;</em></p>


  <h2>결과</h2>


  <p>코드를 테스트하기 위해 표준 데이터 세트에서 BERT 대용량 모델을 학습하고 표 1에 표시된 대로 GLUE 작업 집합에 대한 원래 용지의 결과를 재현했습니다. 필요한 컴퓨팅을 예측하기 위해 Azure ML 클러스터의 8xND40_v2 노드(총 64 NVidia V100 GPU)에서 6일 동안 학습을 실행하여 테이블에 나열된 정확도에 도달했습니다. 표시되는 실제 숫자는 데이터 세트 및 업스트림 작업에 사용할 BERT 모델 검사점 선택에 따라 달라집니다.</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/63711425-5239-457a-9f91-db16a492f2b3.gif"><img alt="clip_image002" border="0" height="229" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/18d3c1c7-ae4a-4585-889c-3fb721c77ef3.gif" style="border: 0px currentcolor; margin-right: auto; margin-left: auto; display: block; background-image: none;" title="clip_image002" width="912"></a></p>


  <p align="center"><em>표1. GLUE 개발 집합 결과입니다. Google BERT 결과는 개발 집합에 게시된 BERT 모델을 사용하여 평가됩니다. &ldquo;평균&rdquo; 열은 테이블 결과에 대한 단순 평균입니다. F1 점수는 QQP 및 MRPC에 대해 보고되고, 스피어맨 상관 관계는 STS-B에 대해 보고되며, 정확도 점수는 다른 작업에 대해 보고됩니다. 데이터 세트 크기가 작은 작업에 대한 결과는 상당한 변형을 가지며 결과를 재현하기 위해 여러 번의 미세 조정 실행이 필요할 수 있습니다.</em></p>


  <p align="left">이 코드는 <a href="https://github.com/microsoft/AzureML-BERT">Azure Machine Learning BERT GitHub 리포지토리</a>의 오픈 소스에서 사용할 수 있습니다. 리포지토리에 포함된 내용은 다음과 같습니다.</p>


  <ul>
      <li><a href="https://github.com/huggingface/pytorch-pretrained-BERT">포옹 얼굴 리포지토리</a>에서 BERT 모델의 PyTorch 구현.</li>
      <li>원시 및 미리 처리된 영어 Wikipedia 데이터 세트입니다.</li>
      <li>데이터 준비 스크립트.</li>
      <li>그라데이션 누적 및 혼합 정밀도와 같은 최적화 기술을 구현합니다.</li>
      <li>모델의 사전 학습을 시작하는 <a href="https://azure.microsoft.com/en-us/services/machine-learning-service/">Azure Machine Learning 서비스</a> Jupyter Notebook입니다.</li>
      <li>미세 조정 실험에 사용할 수 있는 미리 학습된 모델 집합입니다.</li>
      <li>미세 조정 실험을 수행하기 위한 Notebook의 예제 코드입니다.</li>
  </ul>


  <p>개발자와 데이터 과학자는 간단한 &ldquo;모두 실행&rdquo; 명령을 사용하여 <a href="https://azure.microsoft.com/en-us/services/machine-learning-service/">Azure Machine Learning 서비스에서</a> 제공된 Jupyter Notebook을 사용하여 자체 BERT 모델을 학습시킬 수 있습니다. 코드, 데이터, 스크립트 및 도구는 다른 모든 학습 환경에서도 실행할 수 있습니다.</p>


  <h2>요약</h2>


  <p>우리는 우리 앞에 연구원의 놀라운 일을 활용하지 않고 이러한 결과를 달성 할 수 없었다, 우리는 지역 사회가 우리의 일을 가지고 더 갈 수 있기를 바랍니다. 질문이나 피드백이 있는 경우 <a href="https://github.com/microsoft/AzureML-BERT">GitHub 리포지토리</a>로 이동하여 더 나은 방법을 알려주세요.</p>


  <p><a href="https://azure.microsoft.com/en-us/services/machine-learning-service/">Azure Machine Learning</a> 기계 학습 모델의 빌드, 학습 및 배포를 간소화하는 데 어떻게 도움이 되는지 알아봅니다. <a href="https://azure.microsoft.com/en-us/free/services/machine-learning/">오늘 무료로 시작하세요</a>.</p>
