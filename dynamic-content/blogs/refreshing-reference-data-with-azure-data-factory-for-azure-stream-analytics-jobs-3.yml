### YamlMime:Yaml
ms.openlocfilehash: f5d7a991da6eebd5d03bfbb532caae0310228fb1
ms.sourcegitcommit: d03bdc7fe5447adb6530886aab848b75fe8fa8ee
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 03/11/2022
ms.locfileid: "139912522"
Slug: refreshing-reference-data-with-azure-data-factory-for-azure-stream-analytics-jobs-3
Title: Azure Stream Analytics 작업을 위해 Azure Data Factory를 사용하여 참조 데이터 새로 고침
Summary: Azure Data Factory를 활용하여 다양한 데이터 저장소에서 참조 데이터를 가져와서 일정에 따라 새로 고쳐서 스트림 분석 작업에 대한 입력으로 제공합니다.
Content: >-
  <p align="justify">들어오는 이벤트 스트림(예: 디바이스, 느린 변경 <a href="https://azure.microsoft.com/en-us/documentation/articles/stream-analytics-connect-data-event-inputs/#reference-data-inputs"><u>&ldquo;참조 데이터&rdquo;</u></a> (예: 디바이스 프로필 또는&nbsp; 커스터 프로필 정보&nbsp;)을 사용하여 스트림 분석 작업의 일부로 쿼리에 조인하려는 경우가 <a href="https://azure.microsoft.com/en-us/documentation/services/stream-analytics/">많습니다</a> . 이렇게 하면 스트림 작업에서 생성된 인사이트에 대한 향상된 보고서를 만들 수 있습니다. 이 게시물 및 함께 제공되는 샘플에서는 Azure Data Factory를 활용하여 다양한 데이터 저장소에서 참조 데이터를 가져오고, 일정에 따라 새로 고치고, 스트림 분석 작업에 대한 입력으로 제공하는 방법을 보여 줍니다.</p>


  <h2 align="justify">Azure Stream Analytics에서 참조 데이터 작업</h2>


  <p align="justify">Stream Analytics는 Azure Blob Storage에 저장된 참조 데이터를 작업에 대한 <a href="https://azure.microsoft.com/en-us/documentation/articles/stream-analytics-connect-data-event-inputs/">입력&rdquo; 중&ldquo;</a> 하나로 사용할 수 있습니다. 참조 데이터를 새로 고칠 수 있도록 설정하려면 사용자가 경로 패턴 내에서 {날짜} 및 {시간} 토큰을 사용하여 입력 구성에 Blob 목록을 지정해야 합니다. 작업은 UTC 표준 시간대를 사용하여 Blob 이름으로 인코딩된 날짜 및 시간에 기반한 해당 Blob을 로드합니다.</p>


  <p align="left">예를 들어 작업에서 /referencedata/{date}/{time}/customertable.csv 같은 경로 패턴으로 포털에 구성된 참조 입력이 있는 경우 날짜 형식은 YYYY/MM/DD&rdquo;이고 &ldquo;시간 형식은 HH/mm&rdquo;입니다&ldquo;. 작업은 2015년 7월 26일 UTC 표준 시간대에 오전 8시 30분에 /referencedata/2015/07/26/08/30/customertable.csv 파일을 선택합니다.</p>


  <p align="justify">이를 위해서는 고객이 다음과 같은&nbsp; 문제를 해결해야 합니다.</p>


  <ol>
   <li>
   <div align="justify">참조 데이터가 Azure Blob 이외의 데이터 저장소에 있는 경우 Azure Blob으로 이동해야 합니다.</div>
   </li>
   <li>
   <div align="justify">참조 데이터는 비교적 자주 변경되지 않지만 여전히 변경됩니다. Azure Blob에서 올바른 경로 및 데이터 시간 정보를 사용하여 참조 데이터를 선택&nbsp; 및 삭제하도록 정기적으로 새로 고침 일정을 지정하려고 합니다.</div>
   </li>
  </ol>


  <h2 align="justify">Azure Data Factory를 사용하여 다양한 데이터 저장소에서 참조 데이터 새로 고침</h2>


  <p align="justify"><a href="https://azure.microsoft.com/en-us/documentation/services/data-factory/">Azure Data Factory</a> 는 위에서 언급한 과제에 대한 완벽한 솔루션입니다. Azure Data Factory는 데이터의 이동 및 변환을 오케스트레이션하고 자동화하는 클라우드 기반 데이터 통합 서비스입니다.&nbsp; &nbsp;많은 <a href="https://azure.microsoft.com/en-us/documentation/articles/data-factory-data-movement-activities/">수의 클라우드 기반 및 온-프레미스 데이터 저장소에 연결하고</a> 사용자가 지정한 일정에&nbsp; 따라 데이터를 쉽게 이동할 수 있도록 지원합니다.</p>


  <p align="justify">&#39;예제에서 전리품을 보자 ...</p>


  <p align="justify">TheSteam&nbsp; <a href="https://azure.microsoft.com/en-us/documentation/articles/stream-analytics-get-started/">Analytics 시작 가이드</a>는 통화 레코드 데이터가 대규모 스트리밍 방식으로 처리되고 SIM 카드 사기(같은 ID에서 동시에 지리적으로 다른 위치에서 오는 여러 통화)를 분석하는 통신 회사에 대한 시나리오를 보여 줍니다. 이 시나리오에 대한 스트림 분석 작업은 하나의 입력을 사용합니다. 스트리밍 호출은 EventHub를 통해 들어오는 데이터를 기록합니다. 이제 고객(customerInfo 테이블)에 대한 정보(예: 이름, 연락처 정보)를 사용하여 다른 입력, 참조 데이터를 추가하려고 했습니다. 이를 통해 사기 행위의 영향을 받는 고객을 식별하기 위해 사기성 호출을 감지하는 스트리밍 쿼리에서 customertInfo 테이블에 대한 조인을 추가할 수 있습니다. 또한 customerInfo 테이블이 Azure SQL 데이터베이스에서 유지 관리되고 새 고객이 추가되고 연락처 정보가 변경될 때 하루 동안 여러 번 업데이트할 수 있다고 가정합니다.</p>


  <p align="justify">아래 다이어그램에서는 Azure Data Factory 및 Stream Analytics를 함께 활용하여 참조 데이터를 사용하여 위에서 언급한 쿼리를 실행하고 일정에 따라 참조 데이터에 대한 새로 고침을 설정하는 고급 솔루션 아키텍처를 보여 줍니다.</p>


  <p align="justify">&nbsp;</p>


  <p align="justify"><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/cebce617-25a1-46f0-89f0-ead26500d170.png"><img alt="referencedatarefreshdiagram6" border="0" height="709" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/0ea18b90-df28-4ae8-a6d4-b1d6bde326d5.png" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px" title="referencedatarefreshdiagram6" width="1323"></a></p>


  <p align="justify">&nbsp;</p>


  <p align="justify">위에서 설명한 것처럼 복사 작업&nbsp;으로 데이터 팩터리 파이프라인을 만들 수 있습니다. 이 파이프라인은 날짜&nbsp; 및 시간 정보를 기반으로 Azure SQL 최신 버전의 customertable을 해당 경로의 Blob으로 복사합니다. Azure Stream Analytics 작업&nbsp; 영역은 고객이 참조 데이터 입력으로 사용할 수 있도록 구성되었으며 항상 사용할 수 있게 되면 항상 참조 데이터의 최신 복사본을 선택합니다.</p>


  <p align="justify">위의 샘플 설정 및 참조 데이터를 복사하는 데이터 팩터리를 설정하는 방법에 대한 단계&nbsp;별 단계별 설정에 대한 자세한 내용은 GitHub <a href="https://github.com/Azure/Azure-DataFactory/tree/master/Samples/ReferenceDataRefreshForASAJobs">Azure Stream Analytics 작업 샘플에 대한 참조 데이터 새로 고침</a>을 참조하세요.</p>
