### YamlMime:Yaml
ms.openlocfilehash: c5e6eb39e64ab02993cd6ce66f4b69a846619b7d
ms.sourcegitcommit: d03bdc7fe5447adb6530886aab848b75fe8fa8ee
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 03/11/2022
ms.locfileid: "139908801"
Slug: onnx-runtime-is-now-open-source
Title: 이제 ONNX 런타임이 오픈 소스입니다.
Summary: 오늘 GitHub 오픈 소스 ONNX(Open Neural Network Exchange) 런타임이 있다고 발표합니다. ONNX 런타임은 Linux, Windows 및 Mac에서 ONNX 형식의 기계 학습 모델을 위한 고성능 유추 엔진입니다.
Content: >-
  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/9f6e2845-0f22-47a1-a038-0fd3a97b1c2b.png"><img align="left" alt="ONNX Runtime Logo" border="0" height="139" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/738e9a46-5e21-4251-b8bf-992529c25468.png" style="border: 0px currentcolor; border-image: none; float: left; display: inline; background-image: none;" title="ONNX 런타임 LogoToday" width="247"></a>에서 오픈 소스 ONNX(Open Neural Network Exchange) 런타임이 켜진 것을 발표합니다 <a href="https://github.com/microsoft/onnxruntime" target="_blank">GitHub</a>. ONNX 런타임은 Linux, Windows 및 Mac에서 ONNX 형식의 기계 학습 모델을 위한 고성능 유추 엔진입니다.</p>


  <p><a href="https://onnx.ai/" target="_blank">ONNX</a>  는 Microsoft가 Facebook 및 AWS와 공동 개발한 딥 러닝 및 기존 기계 학습 모델을 위한 개방형 형식입니다. ONNX 형식은 AI를 모든 사용자에게 보다 접근 가능하고 가치 있게 만드는 개방형 에코시스템의 기초입니다. 개발자는 작업에 적합한 프레임워크를 선택할 수 있고, 프레임워크 작성자는 혁신적인 개선 사항에 집중할 수 있으며, 하드웨어 공급업체는 신경망 계산에 대한 최적화를 간소화할 수 있습니다. </p>


  <p>Microsoft는 2년 이상 AI 연구를 수행하고 기계 학습 및 심층 신경망을 다양한 제품 및 서비스에 통합해 왔습니다. 팀이 다양한 교육 프레임워크를 사용하고 다양한 배포 옵션을 대상으로 하는 경우 이러한 분산된 솔루션을 통합하여 모델을 빠르고 간단하게 운영할 수 있도록 해야 했습니다. ONNX 런타임은 해당 솔루션을 제공합니다. 데이터 과학자는 선택한 프레임워크에서 모델을 학습 및 조정하고 클라우드와 에지에 걸친 제품의 고성능을 통해 이러한 모델을 프로덕션화할 수 있는 유연성을 제공합니다.</p>


  <h2>ONNX 런타임을 사용하는 이유</h2>


  <p>ONNX 런타임은 ONNX-ML 프로필을 포함하여 ONNX 1.2 이상을 완전히 지원하는 최초의 공개적으로 사용 가능한 유추 엔진입니다. 즉, 진화하는 AI 모델 및 기술 혁신을 지원하기 위해 ONNX 표준과 함께 직접 발전하고 있습니다.</p>


  <p>Microsoft에서 팀은 ONNX 런타임을 사용하여 Bing Search, Bing Ads, Office 생산성 서비스 등의 핵심 시나리오에서 사용되는 많은 모델의 점수 매기기 대기 시간과 효율성을 개선하고 있습니다. &#39;ONNX로 변환한 모델의 경우 기존 솔루션의 점수 매기기에 비해 평균 성능이 2배 향상되는 것을 볼&#39;있습니다. ONNX 런타임은 Windows ML 및 ML.net을 비롯한 다른 Microsoft 제품에도 통합됩니다.</p>


  <p>ONNX 런타임은 가볍고 모듈식이며 CPU 빌드 크기는 몇 메가바이트에 불과합니다. <a href="https://github.com/Microsoft/onnxruntime/blob/master/docs/HighLevelDesign.md" target="_blank">확장 가능한 아키텍처</a>  최적화 프로그램 및 하드웨어 가속기는 실행 공급자로 &ldquo;등록하여 낮은 대기 시간과 높은 컴퓨팅 효율성을 제공합니다.&rdquo; 결과적으로 인식된 대기 시간이 낮아지고 머신 사용률이 감소하고 처리량이 증가하여 비용을 절감할 수 있는 보다 원활한 엔드투엔드 사용자 환경이 생성됩니다. </p>


  <h2>업계 파트너의 심층 지원</h2>


  <p>ONNX 커뮤니티의 주요 기업은 ONNX 런타임과 기술을 통합하기 위해 적극적으로 작업하거나 계획하고 있습니다. 이를 통해 최상의 성능을 달성하면서 전체 ONNX 사양을 지원할 수 있습니다.</p>


  <p>Microsoft와 Intel은 nGraph 컴파일러를 ONNX 런타임의 실행 공급자로 통합하기 위해 함께 노력하고 있습니다. nGraph 컴파일러는 비 디바이스별 최적화와 디바이스별 최적화를 모두 적용하여 기존 및 예정된 하드웨어 대상을 모두 가속화할 수 있습니다.CPU 유추에 nGraph 컴파일러를 사용하면 네이티브 프레임워크에 비해 최대 <a href="https://ai.intel.com/ngraph-compiler-stack-beta-release/" target="_blank">45배의 성능 향상</a> 을 얻을 수 있습니다.  </p>


  <p>NVIDIA는 TensorRT를 ONNX 런타임과 통합하여 NVIDIA GPU에서 빠르게 증가하는 모델 및 앱 집합을 배포하는 동시에 최상의 성능을 달성할 수 있는 쉬운 워크플로를 제공합니다.NVIDIA TensorRT에는 권장자, 자연어 처리 및 이미지/비디오 처리와 같은 애플리케이션에서 최소 대기 시간으로 처리량을 크게 높이는 고성능 유추 최적화 프로그램 및 런타임이 포함되어 있습니다. </p>


  <p>ONNX의 또 다른  <a href="https://developer.qualcomm.com/blog/run-your-onnx-ai-models-faster-snapdragon" target="_blank">옹호자</a> 인 퀄컴도 ONNX 런타임에 대한 지원을 표명했습니다.&ldquo;ONNX 런타임의 도입은 여러 디바이스 범주에서 프레임워크 상호 운용성, 표준화 및 성능 최적화를 더욱 발전시키는 긍정적인 다음 단계이며, 개발자는 Snapdragon 모바일 플랫폼에서 ONNX 런타임에 대한 지원을 환영할 것으로 기대한다고 Qualcomm Technologies&quot;, Inc.의 AI 제품 관리 수석 디렉터 게리 브로트먼은 말합니다.  </p>


  <p>최근 ONNX에 합류한 후 선도적인 IoT 칩 제조업체인 NXP도 ONNX 런타임에 대한 지원을 발표했습니다.&ldquo;많은 기계 학습 프레임워크 중에서 선택할 때, 우리는 고객이 최대의 유연성과 자유를&rdquo; 갖기를 원한다고 NXP의 AI 기술 센터 책임자인 Markus Levy는 말합니다.&quot;&rsquo;Microsoft가 플랫폼에서 릴리스한 ONNX 런타임을 지원하여 ML 개발자의 고객 커뮤니티에 ONNX 혜택을 제공합니다.&rdquo;</p>


  <p>하드웨어 파트너 외에도 프레임워크 공급자 Preferred Networks는 ONNX 런타임을 활용합니다. &quot;Preferred Networks는 딥 러닝 프레임워크 <a href="https://chainer.org/" target="_blank">Chainer</a> 개발 외에도 여러 프로그래밍 언어&quot; &quot; 를 위한 ONNX 유추 엔진 래퍼 라이브러리인 <a href="https://github.com/pfnet-research/menoh" target="_blank">Menoh</a>를 만들었습니다. Menoh는 ONNX 런타임을 기본 백 엔드로 사용하고 Chainer는 현재 ONNX 런타임을 사용하여 ONNX 내보내기 기능을 테스트합니다. Preferred Networks는 Microsoft가 ONNX 런타임을 만든 것을 기쁘게 생각하며 향후 Microsoft와 함께 ONNX에서 작업할 수 있기를 기대합니다.&quot;  </p>


  <h2>ONNX 런타임을 사용하는 방법</h2>


  <p>먼저 ONNX 모델이 필요할&#39;있습니다. ONNX 모델이&#39;없나요? 문제가 되지 않습니다. ONNX의 아름다움은  <a href="https://github.com/onnx/tutorials" target="_blank">다양한 도구를</a> 통해 사용할 수 있는 프레임워크 상호 운용성입니다.</p>


  <ul>
   <li><a href="https://github.com/onnx/models" target="_blank">ONNX 모델 동물원에서</a> 직접 ResNet 및 TinyYOLO와 같은 인기 모델의 미리 학습된 버전을 가져올 수 있습니다.</li>
   <li><a href="https://docs.microsoft.com/en-us/azure/cognitive-services/Custom-Vision-Service/home" target="_blank">Azure Custom Vision Cognitive Service</a>를 사용하여 사용자 지정 컴퓨터 비전 모델을 만들 수 있습니다.</li>
   <li>TensorFlow, Keras, Scikit-Learn 또는 CoreML 형식의 모델이 이미 있는 경우 오픈 소스 변환기(<a href="https://pypi.org/project/onnxmltools/" target="_blank">ONNXMLTools</a> 및 <a href="https://pypi.org/project/tf2onnx/" target="_blank">TF2ONNX</a>)를 사용하여 모델을 변환할 수 있습니다.</li>
   <li>Azure Machine Learning 서비스를 사용하여 새 모델을 학습시키고 <a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-build-deploy-onnx" target="_blank">ONNX 형식으로 저장할 수 있습니다</a>.  </li>
  </ul>


  <p>ONNX 런타임을 사용하려면 원하는 플랫폼 및 선택한 언어에 대한 패키지를 설치하거나 원본에서 빌드를 만듭니다. ONNX 런타임은 Linux, Windows 및 Mac에서 호환되는 CUDA <strong>( </strong> <strong>CUDA </strong>) 및  <strong>Python</strong>, <strong>C#</strong> 및 <strong>C</strong> 인터페이스를 모두 지원합니다.설치 지침은 <a href="https://github.com/microsoft/onnxruntime" target="_blank">GitHub</a> 확인합니다.     </p>


  <p>소스 또는 미리 컴파일된 이진 파일에서 직접 코드에 ONNX 런타임을 통합할 수 있지만 쉽게 작동할 수 있는 방법은 호출할 애플리케이션에 대한  <a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-build-deploy-onnx" target="_blank">서비스</a>  배포를 Azure Machine Learning 사용하는 것입니다.   </p>


  <h2>참여하기  </h2>


  <p>ONNX 런타임의 출시는 AI를 위한 개방적이고 상호 운용 가능한 에코시스템을 향한 노력의 중요한 단계이며, 지금까지 커뮤니티의 열정과 지원에 대해 매우 기쁘게 생각하고 있습니다. 이를 통해 AI에서 제품 혁신을 보다 쉽게 추진할 수 있고 개발 커뮤니티가 이를 시험해 보도록 강력히 장려할 수 있기를 바랍니다. ONNX 런타임은 지속적으로 발전하고 개선되고 있으며, 이 흥미로운 영역에 대한 여러분의 피드백과 기여를 기대합니다!  </p>


  <p>피드백 또는 질문이 있으신가요?  <a href="https://github.com/Microsoft/onnxruntime/issues" target="_blank">문제</a>  제출 <a href="https://github.com/microsoft/onnxruntime" target="_blank">Github</a>에서 <a href="https://twitter.com/onnxruntime">Twitter</a>에서 팔로우하세요.  </p>
