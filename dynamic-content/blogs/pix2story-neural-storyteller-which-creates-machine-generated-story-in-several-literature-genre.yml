### YamlMime:Yaml
ms.openlocfilehash: 1a1ddcb0cbad5e7d27d90321925d440d2f690b45
ms.sourcegitcommit: d03bdc7fe5447adb6530886aab848b75fe8fa8ee
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 03/11/2022
ms.locfileid: "139908181"
Slug: pix2story-neural-storyteller-which-creates-machine-generated-story-in-several-literature-genre
Title: 'Pix2Story: 여러 문학 장르에서 기계 생성 스토리를 만드는 신경망 스토리텔러'
Summary: '스토리텔링은 인간의 본성의 핵심입니다. 우리는 글을 쓰기 훨씬 전에 스토리텔러였고, 우리의 가치를 공유하고, 주로 구두 스토리텔링을 통해 사회를 만들었습니다. 그런 다음, 우리는 우리의 이야기를 기록하고 공유 할 수있는 방법을 찾을 수 있었고, 우리의 이야기를 광범위하게 공유 할 수있는 더 진보 된 방법을 찾았습니다. 구텐베르크의 인쇄기에서 텔레비전, 인터넷으로. '
Content: >-
  <p>스토리텔링은 인간의 본성의 핵심입니다. 우리는 글을 쓰기 훨씬 전에 스토리텔러였고, 우리의 가치를 공유하고, 주로 구두 스토리텔링을 통해 사회를 만들었습니다. 그런 다음, 우리는 우리의 이야기를 기록하고 공유 할 수있는 방법을 찾을 수 있었고, 우리의 이야기를 광범위하게 공유 할 수있는 더 진보 된 방법을 찾았습니다. 구텐베르크&rsquo; 인쇄에서 텔레비전, 인터넷으로. 특히 다른 문학 장르의 그림을 보는 것만으로는 스토리를 작성해야 하는 경우, 이야기를 쓰는 것은 쉽지 않습니다.</p>


  <p>NLP(자연어 처리)는 컴퓨터-인간 상호 작용에 혁명을 일으키고 있는 분야입니다. 우리는 컴퓨터 비전으로 오늘날 우리가 가지고있는 놀라운 정확성을 보았지만 NLP를 보여주는 더 자연스럽고 응집력있는 내러티브를 만들 수 있는지 보고 싶었습니다. Azure에서 사용자가 그림을 업로드하고 여러 문학 장르를 기반으로 기계에서 생성된 스토리를 가져올 수 있는 신경 스토리텔러 웹 애플리케이션인 <a href="https://www.ailab.microsoft.com/experiments/7feda326-186c-494b-9b12-fbb41f1dce0c" target="_blank">Pix2Story</a> 를 개발했습니다. 우리는 여러 논문 &ldquo; 에 우리의 작품을 기반으로 <a href="https://arxiv.org/abs/1506.06726" target="_blank">건너 뛰기 생각 벡터</a>,&rdquo; <a href="https://arxiv.org/abs/1502.03044" target="_blank">쇼, 참석 및 말하기: 시각 주의 신경 이미지 캡션 생성</a>,&rdquo; &ldquo;<a href="https://arxiv.org/abs/1506.06724" target="_blank">책과 영화 정렬: 영화와 책을 보고 이야기 같은 시각적 설명을 향해</a>,&rdquo; 그리고 일부 리포지토리 <a href="https://github.com/ryankiros/neural-storyteller" target="_blank">신경 스토리텔러</a>.&ldquo; 아이디어는 업로드 된 그림에서 캡션을 얻고 장르와 그림을 기반으로 내러티브를 생성하기 위해 되풀이 신경망 모델에 공급하는 것입니다.</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/93d4031b-3517-4b8d-bae5-43369cee7372.gif"><img alt="Pix2Story_Homepage-Carousel_580x326-v3" height="326" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/d32d5d20-914c-443f-97a3-ba2cae8df446.gif" style="margin-right: auto; margin-left: auto; float: none; display: block;" title="Pix2Story_Homepage-Carousel_580x326-v3" width="580"></a></p>


  <h2>솔루션</h2>


  <p>업로드된 이미지를 분석하고 캡션을 생성하여 시각적 입력을 이해하기 위해 300.000개 이미지의 MS COCO 캡션 데이터 세트에 시각적 의미 체계 포함 모델을 학습한 프로세스의 일부입니다. 또한 캡션을 변형하고 선택한 장르인 어드벤처, 공상 과학 또는 스릴러를 기반으로 내러티브를 생성합니다. 이를 위해 2,000개 이상의 소설에서 2주 동안 인코더 디코더 모델을 학습시켰습니다. 이 교육을 통해 소설의 각 구절을 벡터 공간에 생각을 포함하는 방법인 건너뛰기 생각 벡터에 매핑할 수 있습니다. 이를 통해 우리는 단어뿐만 아니라 문맥에서 그 단어의 의미를 이해하여 인코딩된 구절의 주변 문장을 재구성할 수 있게 했습니다. 새 Azure Machine Learning Service와 Python 3의 Azure 모델 관리 SDK를 사용하여 이러한 모델로 Docker 이미지를 만들고 GPU 기능과 함께 AKS(Azure Kubernetes Services)를 사용하여 배포하여 프로젝트를 프로덕션에 준비했습니다. 프로세스 흐름을 자세히 살펴보겠습니다&rsquo;.</p>


  <h2>시각적 의미 체계 포함</h2>


  <p>프로젝트의 첫 번째 부분은 입력 그림을 캡션으로 변환하는 부분입니다. 캡션은 아래 예제와 같이 그림을 간략하게 설명합니다.</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/fb636832-f7bc-427d-b151-ff2d49107984.jpg"><img alt="White dog sitting on green grass" border="0" height="461" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/72d075d0-d642-4e50-9433-6b9253b63dc7.jpg" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" title="녹색 잔디에 앉아 흰색 개" width="692"></a></p>


  <p><strong>검은 반점이있는 흰색 개에 녹색과 흰색 나비 넥타이를 가진 프리스비 작은 흰색 개를보고있는 흰색 개가 잔디에 앉아있다.</strong></p>


  <p>이 캡션을 생성하는 데 사용되는 모델은 서로 다른 두 네트워크로 구성됩니다. 첫째, 주석 벡터라고 하는 기능 벡터 집합을 추출하는 나선형 신경망입니다.</p>


  <p>모델의 두 번째 부분은 컨텍스트 벡터, 이전에 숨겨진 상태 및 이전에 생성된 단어를 조건화할 때마다 한 단어를 생성하여 캡션을 생성하는 LSTM(장기 메모리) 네트워크입니다.</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/d620ee10-ec6b-4f4f-8c4e-68393ba70562.jpg"><img alt="Feature map" border="0" height="272" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/8f3becff-13ad-4e11-91ce-a8239b525e25.jpg" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" title="기능 맵" width="613"></a></p>


  <h2>Skipthought 벡터</h2>


  <p>R. Kiros의 Skipthought Vectors는 다른 작업에서 사용할 수 있는 제네릭 문장 표현을 생성하는 모델입니다. 이 프로젝트의 경우 책의 텍스트 연속성을 사용하여 인코딩된 구절의 주변 문장을 재구성하려는 인코더 디코더 모델을 학습시키는 것이 좋습니다.</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/c097a677-970b-4c8d-b8ea-dcca66d266f9.jpg"><img alt="Skipthought vectors" border="0" height="152" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/52ac209e-0965-46df-ae7f-41a01247a7a9.jpg" style="margin: 0px auto; border: 0px currentcolor; border-image: none; float: none; display: block; background-image: none;" title="Skipthought 벡터" width="873"></a></p>


  <p>모델은 인코더 디코더 모델입니다. 인코더는 문장을 벡터에 매핑하는 데 사용됩니다. 디코더는 소스 문장에 대한 번역을 생성하기 위해 이 벡터의 조건을 지정합니다.</p>


  <p>사용된 어휘는 google 뉴스 미리 학습된 벡터를 사용하여 확장되었으며, 이 벡터의 단어에 있는 책 어휘에 있는 단어를 매핑하는 선형 회귀기를 생성합니다.</p>


  <h2>스타일 이동</h2>


  <p>VSE에서 인코더에 제공된 문장이 짧은 설명 문장인 경우 skipthoughts가 작동하기 위해 참석하면 최종 출력은 짧은 문장이 됩니다. 이러한 이유로, 원하는 출력이 더 문학적인 구절이라면, 우리는 스타일이 바뀌도록 해야 합니다. 즉, skipthought 벡터 표현을 사용하여 입력을 출력에서 유도하려는 특성으로 설정하는 것을 의미합니다. 작업은 다음과 같습니다.</p>


  <p>Skipthoughts 디코더 입력 = 그림의 인코딩된 캡션 - 인코딩된 평균 모든 캡션 + 예상 출력과 비슷한 길이 및 기능을 가진 인코딩된 통로.</p>


  <h2>배포</h2>


  <p>이 프로젝트는 Azure Machine Learning Services 작업 영역을 사용하여 파일 및 예측과 관련된 모든 모델을 사용하여 Docker 이미지를 생성하도록 배포되었습니다. AKS를 사용하여 솔루션의 크기를 자동으로 조정하는 데 사용할 배포가 이루어졌습니다.</p>


  <p><a href="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/091fbe04-5dc9-4b47-8674-fdda13b26fa6.png" target="_blank"><img alt="image" border="0" height="553" src="https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/5b2dafee-b54e-4785-829f-e9dc9bccce0c.png" style="border: 0px currentcolor; border-image: none; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;" title="이미지" width="1024"></a></p>


  <h2>사용자 고유의 모델 학습</h2>


  <h3>새 모델 학습의 경우:</h3>


  <ul>
   <li>conda 환경 만들기:</li>
  </ul>


  <pre>

  conda env create --file environment.yml</pre>


  <ul>
   <li>conda env 활성화:</li>
  </ul>


  <pre>

  activate storytelling</pre>


  <ul>
   <li>config.py 책이나 텍스트 및 학습 설정의 경로를 설정합니다.</li>
   <li>training.py 실행하여 인코더를 학습시키고, 필요한 파일을 생성하고, 텍스트에 따라 디코더를 학습시킵니다.</li>
   <li>바이어스 생성: 인코딩된 문장의 평균 및 인코딩된 캡션의 평균입니다.</li>
   <li>스토리를 생성하려면 Python 콘솔에서 다음을 실행합니다.</li>
  </ul>


  <pre>

  &gt;import generate

  &gt;story_generator = generate.StoryGenerator()

  &gt;story_generator.story(image_loc=&#39;path/to/your/image&#39;)</pre>


  <p>축하합니다! 이제 완전히 작동하는 애플리케이션을 시작해야 합니다. 프로젝트를 재미있게 테스트하고 기여해 주셔서 감사합니다!</p>


  <p>&quot;Pix2Story- 신경 스토리텔러&quot;-사용자가 그림을 업로드하고 여러 문학 장르를 기반으로 AI 생성 스토리를 가져올 수 있는 웹앱입니다.</p>


  <h2>추가 리소스</h2>


  <ul>
   <li><a href="https://aka.ms/pix2story" target="_blank">AI 랩</a></li>
   <li><a href="https://pix2story.azurewebsites.net" target="_blank">플레이그라운드</a></li>
  </ul>


  <p>코드, 솔루션 개발 프로세스 및 <a href="https://github.com/Microsoft/ailab/tree/master/Pix2Story" target="_blank">GitHub</a> 대한 다른 모든 세부 정보를 찾을 수 있습니다.</p>


  <p>이 게시물이 AI를 시작하는 데 도움이 되고 AI 개발자가 되도록 동기를 부여하는 데 도움이 되기를 바랍니다.</p>
