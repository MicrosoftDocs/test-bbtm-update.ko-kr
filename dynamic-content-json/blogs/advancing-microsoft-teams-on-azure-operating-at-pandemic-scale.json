{
    "Slug": "advancing-microsoft-teams-on-azure-operating-at-pandemic-scale",
    "Title": "전염병 규모로 작동하는 Azure에서 Microsoft Teams 진행",
    "Summary": "Scale, resiliency, and performance do not happen overnight—it takes sustained and deliberate investment, day over day, and a performance-first mindset to build products that delight our users.",
    "Content": "<p>&ldquo;COVID-19 전염병은 일하고, 공부하고, 사교하는 것이 무엇을 의미하는지 재설정했습니다. 우리 중 많은 사람들처럼, 나는 동료들과의 연결로 Microsoft Teams 의존하게되었습니다. 이 게시물에서는 Microsoft Teams 제품 그룹&mdash;<strong>Rish Tandon</strong>(기업 부사장), <strong>Aarthi Natarajan</strong>(그룹 엔지니어링 관리자),<strong> 마틴 테일레퍼</strong>(설계자)&mdash;의 친구들이 엔터프라이즈급 안전한 생산성 앱 관리 및 스케일링에 대한 몇 가지 학습을 공유합니다.&rdquo; - Mark Russinovich, CTO, Azure</p>\n\n<hr>\n<p>&nbsp;</p>\n\n<p>규모, 복원력 및 성능은 하룻밤 사이에&mdash; 발생하지 않습니다. 지속적이고 신중한 투자, 하루 종일, 그리고 사용자를 기쁘게하는 제품을 구축하기 위한 성능 우선 사고방식이 필요합니다. 출시 이후 Teams 2017년 출시부터 2019년 7월 일일 사용자 1,300만 명, 2019년 11월 2천만 명까지 강력한 성장을 경험했습니다. 4월에는 Teams 일일 활성 사용자 7,500만 명, 일일 모임 참가자 2억 명, 일일 모임 분 41억 명이 있다고 공유했습니다. 우리는 Teams 지금까지 경험한 급속한 성장을 감안할 때 이러한 속도로 서비스를 확장하는 데 필요한 지속적인 작업에 익숙하다고 생각했습니다. COVID-19는 이 가정에 도전했습니다. 이 경험을 통해 이전에는 상상할 수 없었던 성장 기간 동안 서비스를 계속 실행할 수 있을까요?</p>\n\n<h2>견고한 기초</h2>\n\n<p>Teams 마이크로 서비스 아키텍처를 기반으로 하며, 수백 개의 마이크로 서비스가 응집력 있게 작동하여 메시징, 모임, 파일, 일정 및 앱을 비롯한 다양한 기능을 제품에&rsquo; 제공합니다. 마이크로 서비스를 사용하면 각 구성 요소 팀이 독립적으로 변경 내용을 작업하고 릴리스할 수 있습니다.</p>\n\n<p>Azure는 Microsoft Teams 포함하여 모든 Microsofts&rsquo; 클라우드 서비스를 뒷받침하는 클라우드 플랫폼입니다. 워크로드는 Azure VM(가상 머신)에서 실행되며, 이전 서비스는 <a href=\"https://azure.microsoft.com/en-us/services/cloud-services/\" target=\"_blank\">Azure Cloud Services</a> 통해 배포되고 <a href=\"https://azure.microsoft.com/en-us/services/service-fabric/\" target=\"_blank\">Azure Service Fabric</a> 최신 서비스를 통해 배포됩니다. 기본 스토리지 스택은 <a href=\"https://azure.microsoft.com/en-us/services/cosmos-db/\" target=\"_blank\">Azure Cosmos DB</a>이며 일부 서비스는 <a href=\"https://azure.microsoft.com/en-us/services/storage/blobs/\" target=\"_blank\">Azure Blob Storage</a> 사용합니다. 향상된 처리량 및 복원력에 대한 <a href=\"https://azure.microsoft.com/en-us/services/cache/\" target=\"_blank\">Azure Cache for Redis</a> 의지합니다. <a href=\"https://azure.microsoft.com/en-us/services/traffic-manager/\" target=\"_blank\">Traffic Manager</a> 및 <a href=\"https://azure.microsoft.com/en-us/services/frontdoor/\" target=\"_blank\">Azure Front Door</a>를 활용하여 원하는 위치에 트래픽을 라우팅합니다. <a href=\"https://azure.microsoft.com/en-us/services/storage/queues/\" target=\"_blank\">Queue Storage</a> 및 <a href=\"https://azure.microsoft.com/en-us/services/event-hubs/\" target=\"_blank\">Event Hubs</a>를 사용하여 통신하고 테넌트 및 사용자를 관리하는 <a href=\"https://azure.microsoft.com/en-us/services/active-directory/\" target=\"_blank\">Azure Active Directory</a> 의존합니다.</p>\n\n<p>&nbsp;</p>\n\n<p><a href=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/18c7ac34-1b68-4073-b549-945fdda49e0e.png\"><img alt=\"    Diagram showing that Azure is the platform that underpins Teams Services and Office 365 Core Service\" border=\"0\" height=\"426\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/c229cba7-597c-46dc-8545-7f42706d8c55.png\" style=\"border: 0px currentcolor; border-image: none; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\" title=\"\" width=\"640\"></a></p>\n\n<p>&nbsp;</p>\n\n<p>이 게시물은 주로 클라우드 백 엔드에 초점을 맞추고 있지만,&rsquo; Teams 클라이언트 애플리케이션도 최신 디자인 패턴과 프레임워크를 사용하고 풍부한 사용자 환경을 제공하고 오프라인 또는 간헐적으로 연결된 환경을 지원한다는 점을 강조할 필요가 있습니다. 클라이언트를 신속하게 업데이트하고 서비스와 함께 업데이트하는 핵심 기능은 신속한 반복을 위한 핵심 기능입니다. &rsquo;아키텍처를 더 자세히 알아보려면 <a href=\"https://myignite.techcommunity.microsoft.com/sessions/83471\" target=\"_blank\">Microsoft Ignite 2019</a>에서 이 세션을 확인하세요.</p>\n\n<h2>기민한 개발</h2>\n\n<p>CI/CD 파이프라인은 <a href=\"https://docs.microsoft.com/en-us/azure/devops/pipelines/get-started/what-is-azure-pipelines?view=azure-devops\" target=\"_blank\">Azure Pipelines</a> 기반으로 빌드됩니다. 자동화된 엔드투엔드 테스트와 원격 분석 신호의 조합을 기반으로 게이트와 함께 링 기반 배포 전략을 사용합니다. 원격 분석 신호는 인시던트 관리 파이프라인과 통합되어 서비스 및 클라이언트 정의 메트릭 모두에 대한 경고를 제공합니다. 분석을 위해 <a href=\"https://azure.microsoft.com/en-us/services/data-explorer/\" target=\"_blank\">Azure Data Explorer</a> 많이 사용합니다.</p>\n\n<p>또한 크래시 속도, 메모리 사용량, 애플리케이션 응답성, 성능 및 사용자 참여와 같은 주요 제품 메트릭에 대한 기능 동작을 평가하는 성과 기록표가 있는 실험 파이프라인을 사용합니다. 이를 통해 새 기능이 원하는 방식으로 작동하는지 파악할 수 있습니다.</p>\n\n<p>모든 서비스와 클라이언트는 중앙 집중식 구성 관리 서비스를 사용합니다. 이 서비스는 제품 기능을 켜고 끄고, 캐시 시간-라이브 값을 조정하고, 네트워크 요청 빈도를 제어하고, API에 연결할 네트워크 엔드포인트를 설정하는 구성 상태를 제공합니다. 이렇게 하면 어둡게 시작하고 변경&rdquo; 내용의 영향을 정확하게 측정하여 모든 사용자에게 안전하고 효율적인지 확인할 수 있도록 A/B 테스트를 수행할 수 있는 유연한 프레임워크&ldquo;를 제공합니다.</p>\n\n<h2>주요 복원력 전략</h2>\n\n<p>다양한 서비스에서 다음과 같은 몇 가지 복원력 전략을 사용합니다.</p>\n\n<ul>\n    <li><strong>활성-활성 내결함성 시스템</strong>: 활성-활성 내결함성 시스템은 두 개 이상의 운영 독립적 이질 경로로 정의되며, 각 경로는 안정적인 상태에서 라이브 트래픽을 제공할 뿐만 아니라 원활한 장애 조치(failover)를 위해 클라이언트 및 프로토콜 경로 선택을 활용하면서 예상 트래픽의 100%를 제공할 수 있는 기능을 갖습니다. 이 전략은 장애 도메인이 매우 크거나 다른 유형의 시스템을 빌드하고 유지 관리하는 것을 정당화하기 위해 합리적인 비용으로 고객에게 영향을 미치는 경우를 위해 채택됩니다. 예를 들어 외부에 표시되는 모든 클라이언트 도메인에 Office 365 DNS 시스템을 사용합니다. 또한 정적 CDN 클래스 데이터는 Azure Front Door와 Akamai 모두에서 호스팅됩니다.</li>\n    <li><strong>복원력 최적화 캐시</strong>: 성능 및 복원력 모두를 위해 구성 요소 간의 캐시를 광범위하게 활용합니다. 캐시는 평균 대기 시간을 줄이고 다운스트림 서비스를 사용할 수 없는 경우 데이터 원본을 제공하는 데 도움이 됩니다. 데이터를 오랫동안 캐시에 보관하면 데이터 새로 고침 문제가 발생합니다. 그러나 데이터를 오랫동안 캐시에 유지하는 것이 다운스트림 오류에 대한 최상의 방어책입니다. TTL(Time to Live)뿐만 아니라 캐시 데이터에 대한 TTR(새로 고침 시간)에 초점을 맞춥니다. 긴 TTL 및 더 짧은 TTR 값을 설정하여 데이터를 유지하는 최신 방법과 다운스트림 종속성이 실패할 때마다 데이터를 유지할 기간을 미세 조정할 수 있습니다.</li>\n    <li><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/circuit-breaker\" target=\"_blank\"><strong>회로 차단기</strong></a>: 서비스가 실패할 가능성이 있는 작업을 수행하지 못하게 하는 일반적인 디자인 패턴입니다. 다시 시도 요청에 의해 압도되지 않고 다운스트림 서비스가 복구될 수 있는 기회를 제공합니다. 또한 종속성이 문제가 발생할 때 서비스의 응답을 개선하여 시스템이 오류 조건에 더 관대하도록 지원합니다.</li>\n    <li><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/bulkhead\" target=\"_blank\"><strong>격벽 격리</strong></a>: 중요한 서비스 중 일부를 완전히 격리된 배포로 분할합니다. 한 배포에서 문제가 발생하는 경우 격벽 격리는 다른 배포가 계속 작동하도록 설계되었습니다. 이 완화는 가능한 한 많은 고객에 대한 기능을 유지합니다.</li>\n    <li><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/throttling\" target=\"_blank\"><strong>API 수준 속도 제한</strong></a>: 중요한 서비스가 API 수준에서 요청을 제한할 수 있도록 합니다. 이러한 속도 제한은 위에서 설명한 중앙 집중식 구성 관리 시스템을 통해 관리됩니다. 이 기능을 사용하면 COVID-19 서지 동안 중요하지 않은 API 제한을 평가할 수 있었습니다.</li>\n    <li><a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/retry\" target=\"_blank\"><strong>효율적인 재시도 패턴</strong></a>: 모든 API 클라이언트가 네트워크 오류가 발생할 때 트래픽 폭풍을 방지하는 효율적인 재시도 논리를 구현하는지 확인하고 유효성을 검사합니다.</li>\n    <li><strong>시간 제한</strong>: 시간 제한 의미 체계를 일관되게 사용하면 다운스트림 종속성에서 문제가 발생할 때 작업이 중단되지 않습니다.</li>\n    <li><strong>네트워크 오류의 정상적인 처리</strong>: 오프라인 또는 연결이 좋지 않을 때 클라이언트 환경을 개선하기 위해 장기 투자를 했습니다. 이 영역의 주요 개선 사항은 COVID-19 서지가 시작된 것처럼 프로덕션으로 시작되어 클라이언트가 네트워크 품질에 관계없이 일관된 환경을 제공할 수 있도록 합니다.</li>\n</ul>\n\n<p>Azure <a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/\" target=\"_blank\">클라우드 디자인 패턴을</a> 본 적이 있다면 이러한 개념 중 상당수가 익숙할 수 있습니다.&nbsp; 또한 이러한 패턴 중 일부에 대한 구현을 제공하는 마이크로 서비스에서 <a href=\"https://github.com/App-vNext/Polly\" target=\"_blank\">Polly 라이브러리</a> 를 광범위하게 사용합니다.</p>\n\n<p>우리의 아키텍처는 우리를 위해 잘 작동했다, Teams 사용은 월별로 성장하고 플랫폼은 수요를 충족하기 위해 쉽게 확장했다. 그러나 확장성은 집합이 &ldquo;아니며 고려 사항을 잊어버리&rdquo; 기 때문에 복잡한 시스템에서 나타나는 새로운 동작을 해결하기 위해 지속적인 주의가 필요합니다.</p>\n\n<p><strong>COVID-19 재택 주문이 전 세계에 도입되기 시작했을 때, 우리는 시스템에 내장된 건축적 유연성을 활용하고 빠르게 증가하는 수요에 효과적으로 대응하기 위해 우리가 할 수 있는 모든 노브를 전환해야 했습니다.</strong></p>\n\n<h2>용량 예측</h2>\n\n<p>다른 제품과 마찬가지로 원시 사용자 및 사용 패턴 측면에서 성장이 발생할 위치를 예측하기 위해 모델을 빌드하고 지속적으로 반복합니다. 모델은 기록 데이터, 주기적 패턴, 신규 들어오는 대규모 고객 및 다양한 기타 신호를 기반으로 합니다.</p>\n\n<p>급증이 시작되면서 이전 예측 모델이 빠르게 사용되지 않는다는 것이 분명해졌기 때문에 글로벌 수요의 엄청난 성장을 고려한 새로운 모델을 구축해야 했습니다. 기존 사용자의 새로운 사용 패턴, 기존 유휴 사용자의 새로운 사용량, 제품에 온보딩하는 많은 신규 사용자가 동시에 확인되었습니다. 또한 잠재적인 컴퓨팅 및 네트워킹 병목 현상을 처리하기 위해 가속화된 소싱 결정을 내려야 했습니다. 여러 예측 모델링 기술(<a href=\"https://otexts.com/fpp2/arima.html\" target=\"_blank\">ARIMA</a>, 가산, 곱하기, 로그)을 사용합니다. 이를 위해 초과 예측을 방지하기 위해 국가별 기본 한도를 추가했습니다. 산업 및 지리적 영역별 사용량별 변곡점 및 성장 패턴을 이해하려고 노력하여 모델을 조정했습니다. 우리는 병목 현상 지역에 대한 최대 부하 예측을 보강하기 위해 국가별 COVID-19 영향 날짜에 대한 <a href=\"https://coronavirus.jhu.edu/\" target=\"_blank\">Johns Hopkins&rsquo;</a> 연구를 포함한 외부 데이터 원본을 통합했습니다.</p>\n\n<p>프로세스 전반에 걸쳐, 우리는 주의의 측면에 잘못과 사용 패턴이 안정화로 오버 프로비저닝&mdash;을 선호, 우리는 또한 필요에 따라 다시 확장.</p>\n\n<h2>컴퓨팅 리소스 크기 조정</h2>\n\n<p>일반적으로 자연 재해를 견딜 수 있도록 Teams 설계합니다. 여러 Azure 지역을 사용하면 데이터 센터 문제뿐만 아니라 주요 지리적 영역에 대한 중단으로 인한 위험을 완화하는 데 도움이 됩니다. 그러나 이는 이러한 사태 발생 시 영향을 받는 지역&rsquo; 부하에 대비할 수 있도록 추가 리소스를 프로비전한다는 것을 의미합니다. 규모를 확장하기 위해 모든 중요한 마이크로 서비스의 배포를 모든 주요 Azure 지역의 추가 지역으로 신속하게 확장했습니다. 지리당 총 지역 수를 늘려 각 지역이 비상 부하를 흡수하는 데 필요한 총 예비 용량을 줄여 총 용량 요구를 줄입니다. 이 새로운 규모로 부하를 처리하면 효율성을 개선할 수 있는 방법에 대한 몇 가지 인사이트를 얻을 수 있습니다.</p>\n\n<ul>\n    <li>더 많은 수의 더 작은 컴퓨팅 클러스터를 선호하도록 일부 마이크로 서비스를 다시 배포함으로써 클러스터별 크기 조정 고려 사항을 피할 수 있었고, 배포 속도를 높이는 데 도움이 되었으며, 보다 세분화된 부하 분산을 제공했습니다.</li>\n    <li>이전에는 다양한 마이크로 서비스에 사용하는 특정 VM(가상 머신) 형식에 의존했습니다. VM 유형 또는 CPU 측면에서 더 유연하고 전반적인 컴퓨팅 능력 또는 메모리에 집중함으로써 각 지역에서 Azure 리소스를 보다 효율적으로 사용할 수 있었습니다.</li>\n    <li>서비스 코드 자체에서 최적화할 기회를 찾았습니다. 예를 들어 몇 가지 간단한 개선으로 인해 아바타를 생성하는 데 소요되는 CPU 시간이 크게 감소했습니다(이니셜이 있는 작은 거품은 사용자 사진을 사용할 수 없을 때 사용됨).</li>\n</ul>\n\n<h2>네트워킹 및 라우팅 최적화</h2>\n\n<p>대부분의 Teams&rsquo; 용량 사용량은 지정된 Azure 지리에 대해 낮 시간 내에 발생하며, 이로 인해 야간에 유휴 리소스가 발생합니다. 이 유휴 용량을 활용하기 위한 라우팅 전략을 구현했습니다(규정 준수 및 데이터 보존 요구 사항을 항상 준수).</p>\n\n<ul>\n    <li>비대화형 백그라운드 작업은 현재 유휴 용량으로 동적으로 마이그레이션됩니다. 이 작업은 Azure Front Door에서 API 관련 경로를 프로그래밍하여 트래픽이 올바른 위치에 있는지 확인합니다.</li>\n    <li>서지를 처리하기 위해 여러 지역에 걸쳐 통화 및 모임 트래픽이 라우팅되었습니다. Azure Traffic Manager 사용하여 관찰된 사용 패턴을 활용하여 부하를 효과적으로 분산했습니다. 또한 WAN(광역 네트워크) 제한을 방지하기 위해 하루 중 시간 부하 분산을 수행한 Runbook을 만들기 위해 노력했습니다.</li>\n</ul>\n\n<p>일부 Teams&rsquo; 클라이언트 트래픽은 Azure Front Door에서 종료됩니다. 그러나 더 많은 지역에 더 많은 클러스터를 배포하면서 새 클러스터가 트래픽을 충분히 얻지 못하고 있는 것을 발견했습니다. 사용자의 위치와 Azure Front Door 노드의 위치를 배포한 아티팩트입니다. 이 고르지 않은 트래픽 분포를 해결하기 위해 Azure Front Doors&rsquo; 기능을 사용하여 국가 수준에서 트래픽을 라우팅했습니다. 이 예제에서는 서비스 중 하나에 대해 추가 프랑스 트래픽을 영국 서부 지역으로 라우팅한 후 향상된 트래픽 분포를 얻을 수 있습니다.</p>\n\n<p align=\"center\"><a href=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/8125fe92-6b30-4d30-846c-fb22fa82d630.png\"><img alt=\" Graph showing improved traffic distribution after routing additional France traffic to the UK West region\" border=\"0\" height=\"284\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/588d8f8c-4b7d-4e63-8af5-2989021fbcab.png\" style=\"border: 0px currentcolor; border-image: none; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\" title=\"\" width=\"1024\"></a><em>&nbsp;<br>\n그림 1: 지역 간 트래픽을 라우팅한 후 트래픽 분포가 향상되었습니다.</em></p>\n\n<h2>캐시 및 스토리지 개선 사항</h2>\n\n<p>많은 분산 캐시를 사용합니다. 대규모 분산 캐시가 많이 있습니다. 트래픽이 증가함에 따라 개별 캐시의 크기가 조정되지 않는 지점까지 캐시의 부하도 증가했습니다. 캐시 사용에 상당한 영향을 미치는 몇 가지 간단한 변경 내용을 배포했습니다.</p>\n\n<ul>\n    <li>캐시 상태를 원시 JSON이 아닌 이진 형식으로 저장하기 시작했습니다. 이를 위해 프로토콜 버퍼 형식을 사용했습니다.</li>\n    <li>캐시로 보내기 전에 데이터를 압축하기 시작했습니다. 뛰어난 속도와 압축 비율로 인해 LZ4 압축을 사용했습니다.</li>\n</ul>\n\n<p>페이로드 크기 65% 감소, 역직렬화 시간 40% 감소, 직렬화 시간 20% 감소를 달성할 수 있었습니다. 모든 주위에 승리.</p>\n\n<p>조사에 따르면 여러 캐시에 지나치게 공격적인 TTL 설정이 있었기 때문에 불필요한 즉시 데이터가 제거되었습니다. 이러한 TTL을 늘리면 평균 대기 시간과 다운스트림 시스템의 부하를 줄일 수 있습니다.</p>\n\n<h2>의도적인 성능 저하(기능 브라운아웃)</h2>\n\n<p>우리는 실제로 잡초&rsquo;가 물건을 밀어 얼마나 멀리 알고 있었다&rsquo;으로, 우리는 우리가 온라인으로 추가 Teams 용량을 가지고 우리에게 시간을 구입하기 위해 예기치 않은 수요 급증에 신속하게 반응 할 수있는 장소 메커니즘을 배치하는 것이 신중하다고 결정했다.</p>\n\n<p>모든 기능이 고객에게 똑같이 중요한 것은 아닙니다. 예를 들어 메시지를 보내고 받는 것은 다른 사람이 현재 메시지를 입력하고 있음을 확인하는 기능보다 더 중요합니다. 이 때문에 서비스를 확장하는 동안 2주 동안 입력 표시기를 해제했습니다. 이로 인해 인프라의 일부 부분으로 최대 트래픽이 30% 감소했습니다.</p>\n\n<p>일반적으로 필요한 데이터가 가까이 있으므로 평균 엔드투엔드 대기 시간을 줄일 수 있도록 아키텍처의 여러 계층에서 적극적인 프리페치를 사용합니다. 그러나 프리페치하면 사용되지 않는 데이터를 가져올 때 일정량의 작업이 낭비되고 스토리지 리소스가 프리페치된 데이터를 보유해야 하므로 비용이 많이 들 수 있습니다. 일부 시나리오에서는 프리페치를 사용하지 않도록 설정하여 대기 시간이 더 긴 일부 서비스의 용량을 확보했습니다. 다른 경우에는 프리페치 동기화 간격의 기간을 늘렸습니다. 이러한 예 중 하나는 모바일에서 일정 프리페치를 억제하여 요청 볼륨을 80% 줄이는 것이었습니다.<br>\n&nbsp;</p>\n\n<p align=\"center\"><a href=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/6529abae-ee4b-469a-80f8-cb8d7c8c224d.png\"><img alt=\" Graph showing that suppressing calendar prefetch on mobile reduced request volume by 80%\" border=\"0\" height=\"589\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/4f02c6cf-a077-4d97-9620-1b410bbfdd1e.png\" style=\"border: 0px currentcolor; border-image: none; display: inline; background-image: none;\" title=\"\" width=\"970\"></a><br>\n<em>그림 2: 모바일에서 일정 이벤트 세부 정보의 프리페치를 사용하지 않도록 설정합니다.</em></p>\n\n<h2>인시던트 관리</h2>\n\n<p>시스템의 상태를 추적하고 유지하는 데 사용하는 성숙한 인시던트 관리 프로세스가 있지만 이 환경은 달랐습니다. 교통량이 급증했을 뿐만 아니라 엔지니어와 동료들은 집에서 일하는 데 적응하면서 개인적, 정서적 어려움을 겪고 있었습니다.</p>\n\n<p>고객뿐만 아니라 엔지니어도 지원하도록 하기 위해 다음과 같은 몇 가지 변경 사항을 적용했습니다.</p>\n\n<ul>\n    <li>인시던트 관리 회전을 주별 주기에서 일일 주기로 전환했습니다.</li>\n    <li>모든 대기 엔지니어는 교대 근무 사이에 최소 12시간의 휴가를 가졌습니다.</li>\n    <li>우리는 회사 전역에서 더 많은 인시던트 관리자를 데려왔습니다.</li>\n    <li>서비스 전체에서 중요하지 않은 모든 변경 내용을 연기했습니다.</li>\n</ul>\n\n<p>이러한 변경은 모든 인시던트 관리자와 온콜 엔지니어가 고객의 요구를 충족하는 동안 집에서 자신의 요구에 집중할 수 있는 충분한 시간을 확보하는 데 도움이 됩니다.</p>\n\n<h2>Teams 미래</h2>\n\n<p>몇 년 전에도 이런 일이 일어났다면 이 상황이 어땠을지 되돌아보고 궁금해하는 것은 흥미롭습니다. 클라우드 컴퓨팅 없이는 그랬던 것처럼 크기를 조정하는 것이 불가능했을 것입니다. 구성 파일을 단순히 변경하여 오늘 수행할 수 있는 작업은 이전에 새 장비 또는 새 건물을 구입해야 할 수도 있습니다. 현재의 크기 조정 상황이 안정화됨에 따라 우리는 미래에 주목하고 있습니다. 인프라를 개선할 기회가 많다고 생각합니다.</p>\n\n<ul>\n    <li>Azure Kubernetes Service 사용하여 VM 기반 배포에서 컨테이너 기반 배포로 전환할 계획이며, 운영 비용을 절감하고 민첩성을 개선하며 업계에 맞게 조정될 것으로 예상됩니다.</li>\n    <li>REST 사용을 최소화하고 gRPC와 같은 보다 효율적인 이진 프로토콜을 선호할 것으로 예상됩니다. 시스템 전체의 여러 폴링 인스턴스를 보다 효율적인 이벤트 기반 모델로 대체할 예정입니다.</li>\n    <li>우리는 시스템을 안정적으로 만들기 위해 마련한 모든 메커니즘이 항상 완벽하게 작동하고 작동할 준비가 되도록 카오스 엔지니어링 관행을 체계적으로 수용하고 있습니다.</li>\n</ul>\n\n<p>아키텍처를 업계 접근 방식에 맞게 유지하고 Azure 팀의 모범 사례를 활용하여 도움을 요청해야 할 때 전문가들은 데이터 분석, 모니터링, 성능 최적화 및 인시던트 관리에 이르는 문제를 신속하게 해결할 수 있도록 도와줄 수 있습니다. Microsoft 및 광범위한 소프트웨어 개발 커뮤니티에서 동료의 개방성에 감사드립니다. 아키텍처와 기술은 중요하지만 시스템을 정상 상태로 유지하는 것은 사용자의 팀입니다.</p>\n\n<p>&nbsp;</p>\n\n<hr>\n<p><em>관련 게시물: </em><a href=\"https://aka.ms/AdvancingReliability/6\" target=\"_blank\"><em>Azure는 COVID-19</em></a><em>에 응답합니다.<br>\n관련 문서: </em><a href=\"https://aka.ms/AA8ad9b\" target=\"_blank\"><em>COVID-19 전염병 기간 동안 고객, Microsoft를 돕기 위한 Azures&rsquo; 용량 증가</em></a><em>.</em></p>\n"
}